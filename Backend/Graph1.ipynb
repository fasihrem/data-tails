{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-igraph in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.6)\n",
      "Requirement already satisfied: igraph==0.11.6 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-igraph) (0.11.6)\n",
      "Requirement already satisfied: texttable>=1.6.2 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from igraph==0.11.6->python-igraph) (1.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvis in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (8.9.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.3.0)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install python-igraph\n",
    "!pip install pyvis\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import rdflib\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  type  \\\n",
      "0  66e965e698330736e0d693d5  None   \n",
      "1  66e965e698330736e0d693d6  None   \n",
      "2  66e965e698330736e0d693d7  None   \n",
      "3  66e965e698330736e0d693d8  None   \n",
      "4  66e965e698330736e0d693d9  None   \n",
      "\n",
      "                                           postTitle postDesc  \\\n",
      "0  Adults(especially those over 30), how young do...      NaN   \n",
      "1  What is a thing that your parents consider nor...      NaN   \n",
      "2                 What is a smell that comforts you?      NaN   \n",
      "3  When in history was it the best time to be a w...      NaN   \n",
      "4    What's the worst way someone broke up with you?      NaN   \n",
      "\n",
      "              postTime            authorName noOfUpvotes isNSFW  \\\n",
      "0  2024-08-06 01:02:35  Excellent-Studio7257        4068  False   \n",
      "1  2024-08-06 01:47:22        Bigbumoffhappy        2073  False   \n",
      "2  2024-08-05 22:21:53         bloomsmittenn        2188  False   \n",
      "3  2024-08-06 03:32:59   More_food_please_77         778  False   \n",
      "4  2024-08-05 21:01:39    ImpressiveWrap7363        1989  False   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  [It's weird. I can have an adult/intellectual ...        5590.0   \n",
      "1  [Thinking you're supposed to raise your kids t...        1684.0   \n",
      "2  [The weather in the woods after a heavy rain, ...        4958.0   \n",
      "3  [It might depend on where you are but now with...        1046.0   \n",
      "4  [I have 2 bad ones.  First one, he had his mom...        1749.0   \n",
      "\n",
      "                                            imageUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                             postUrl  subReddit  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['DataTails']\n",
    "collection = db['Data']  \n",
    "data_cursor = collection.find({})\n",
    "DF = pd.DataFrame(list(data_cursor))\n",
    "print(DF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "StopWords = set(stopwords.words('english'))\n",
    "def Preprocessing(text):\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in StopWords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "Cols = ['subReddit', 'postTitle', 'postDesc', 'postTime', 'authorName', 'noOfUpvotes', 'comments', 'noOfComments', 'postUrl']\n",
    "DF = DF[Cols]\n",
    "print(DF.isnull().sum())\n",
    "DF['postDesc'].fillna('', inplace=True)\n",
    "DF['noOfUpvotes'].fillna(0, inplace=True)\n",
    "DF['postTime'] = pd.to_datetime(DF['postTime'], errors='coerce')\n",
    "DF['comments'] = DF['comments'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "DF['postTitle'] = DF['postTitle'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['postDesc'] = DF['postDesc'].apply(lambda x: Preprocessing(str(x)))\n",
    "print(DF.head())\n",
    "print(DF.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedData = DF.groupby('subReddit')\n",
    "SubReddits = defaultdict(dict)\n",
    "for subreddit, group in GroupedData:\n",
    "    print(f\"Processing Subreddit: {subreddit}\")\n",
    "    group['Combined'] = group['postTitle'] + \" \" + group['postDesc']\n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: word_tokenize(x))\n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "    LDA = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "    SubReddits[subreddit]['lda_model'] = LDA\n",
    "    SubReddits[subreddit]['dictionary'] = dictionary\n",
    "    SubReddits[subreddit]['corpus'] = corpus\n",
    "    group['topics'] = [LDA.get_document_topics(bow) for bow in corpus]\n",
    "    DF.loc[group.index, 'topics'] = group['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF.shape)\n",
    "print(DF.describe())\n",
    "print(DF.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subReddit           2\n",
      "postTitle           0\n",
      "postDesc        15607\n",
      "postTime            0\n",
      "authorName        587\n",
      "noOfUpvotes         0\n",
      "comments            0\n",
      "noOfComments        2\n",
      "postUrl             2\n",
      "imageUrl            3\n",
      "isNSFW              1\n",
      "dtype: int64\n",
      "   subReddit                                          postTitle postDesc  \\\n",
      "0  AskReddit                   [adult, especially, young, seem]       []   \n",
      "1  AskReddit  [parent, consider, normal, consider, normal, a...       []   \n",
      "2  AskReddit                                   [smell, comfort]       []   \n",
      "3  AskReddit                       [history, best, time, woman]       []   \n",
      "4  AskReddit                       [worst, way, someone, broke]       []   \n",
      "\n",
      "             postTime            authorName noOfUpvotes  \\\n",
      "0 2024-08-06 01:02:35  Excellent-Studio7257        4068   \n",
      "1 2024-08-06 01:47:22        Bigbumoffhappy        2073   \n",
      "2 2024-08-05 22:21:53         bloomsmittenn        2188   \n",
      "3 2024-08-06 03:32:59   More_food_please_77         778   \n",
      "4 2024-08-05 21:01:39    ImpressiveWrap7363        1989   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  It's weird. I can have an adult/intellectual c...        5590.0   \n",
      "1  Thinking you're supposed to raise your kids th...        1684.0   \n",
      "2  The weather in the woods after a heavy rain Ne...        4958.0   \n",
      "3  It might depend on where you are but now with ...        1046.0   \n",
      "4  I have 2 bad ones.  First one, he had his mom ...        1749.0   \n",
      "\n",
      "                                             postUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                            imageUrl  isNSFW  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "subReddit       0\n",
      "postTitle       0\n",
      "postDesc        0\n",
      "postTime        1\n",
      "authorName      0\n",
      "noOfUpvotes     0\n",
      "comments        0\n",
      "noOfComments    0\n",
      "postUrl         0\n",
      "imageUrl        0\n",
      "isNSFW          0\n",
      "dtype: int64\n",
      "Processing Subreddit: Advice\n",
      "Processing Subreddit: AmItheAsshole\n",
      "Processing Subreddit: AskReddit\n",
      "Processing Subreddit: Damnthatsinteresting\n",
      "Processing Subreddit: Filmmakers\n",
      "Processing Subreddit: Jokes\n",
      "Processing Subreddit: Music\n",
      "Processing Subreddit: NoStupidQuestions\n",
      "Processing Subreddit: Showerthoughts\n",
      "Processing Subreddit: Unknown_SubReddit\n",
      "Processing Subreddit: askscience\n",
      "Processing Subreddit: aww\n",
      "Processing Subreddit: books\n",
      "Processing Subreddit: funny\n",
      "Processing Subreddit: gadgets\n",
      "Processing Subreddit: gaming\n",
      "Processing Subreddit: help\n",
      "Processing Subreddit: islamabad\n",
      "Processing Subreddit: memes\n",
      "Processing Subreddit: mildlyinteresting\n",
      "Processing Subreddit: movies\n",
      "Processing Subreddit: news\n",
      "Processing Subreddit: olympics\n",
      "Processing Subreddit: pakistan\n",
      "Processing Subreddit: pics\n",
      "Processing Subreddit: politics\n",
      "Processing Subreddit: programming\n",
      "Processing Subreddit: science\n",
      "Processing Subreddit: showerthoughts\n",
      "Processing Subreddit: socialmedia\n",
      "Processing Subreddit: sports\n",
      "Processing Subreddit: technology\n",
      "Processing Subreddit: todayilearned\n",
      "Processing Subreddit: videos\n",
      "Processing Subreddit: wallstreetbets\n",
      "Processing Subreddit: worldnews\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "StopWords = set(stopwords.words('english'))\n",
    "custom_stopwords = StopWords | {\"make\", \"thing\", \"know\", \"http\", \"get\", \"want\", \"like\", \"would\", \"could\", \"you\", \"say\",\"also\",\"aita\",\"com\",\"www\",\"made\",\"ago\",\"day\",\"000\"}\n",
    "def Preprocessing(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in custom_stopwords and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "Cols = ['subReddit', 'postTitle', 'postDesc', 'postTime', 'authorName', 'noOfUpvotes', 'comments', 'noOfComments', 'postUrl','imageUrl','isNSFW']\n",
    "DF = DF[Cols]\n",
    "print(DF.isnull().sum())\n",
    "\n",
    "\n",
    "DF['subReddit'] = DF['subReddit'].fillna('Unknown_SubReddit')\n",
    "DF['authorName'] = DF['authorName'].fillna('Unknown_Author')\n",
    "DF['postTitle'] = DF['postTitle'].fillna('Untitled')\n",
    "DF['postUrl'] = DF['postUrl'].fillna('http://example.com/NOPOST.png')\n",
    "DF['imageUrl'] = DF['imageUrl'].fillna('http://example.com/NOImage.png')\n",
    "DF['isNSFW'] = DF['isNSFW'].fillna(False)\n",
    "DF['postDesc'].fillna('', inplace=True)\n",
    "DF['noOfUpvotes'].fillna(0, inplace=True)\n",
    "DF['noOfComments'].fillna(0, inplace=True)\n",
    "DF['postTime'] = pd.to_datetime(DF['postTime'], errors='coerce')\n",
    "DF['comments'] = DF['comments'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "DF['postTitle'] = DF['postTitle'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['postDesc'] = DF['postDesc'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['isNSFW'] = DF['isNSFW'].astype(bool)\n",
    "print(DF.head())\n",
    "print(DF.isnull().sum())\n",
    "\n",
    "# Initialize graph and namespaces\n",
    "g = rdflib.Graph()\n",
    "SIOC = rdflib.Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = rdflib.Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = rdflib.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "g.bind(\"sioc\", SIOC)\n",
    "g.bind(\"dc\", DCMI)\n",
    "g.bind(\"foaf\", FOAF)\n",
    "\n",
    "# Function to add posts to the graph\n",
    "def add_post_to_graph(row, index, topic_uri):\n",
    "    post_uri = rdflib.URIRef(f\"http://reddit.com/post{index}\")\n",
    "    subreddit_uri = rdflib.URIRef(f\"http://reddit.com/subreddit/{row['subReddit']}\")\n",
    "    author_uri = rdflib.URIRef(f\"http://reddit.com/user/{row['authorName']}\")\n",
    "    \n",
    "    g.add((post_uri, rdflib.RDF.type, SIOC.Post))\n",
    "    g.add((post_uri, DCMI.title, rdflib.Literal(row['postTitle'])))\n",
    "    g.add((post_uri, DCMI.description, rdflib.Literal(row['postDesc'])))\n",
    "    g.add((post_uri, DCMI.date, rdflib.Literal(row['postTime'])))\n",
    "    g.add((post_uri, SIOC.num_replies, rdflib.Literal(row['noOfUpvotes'])))\n",
    "    g.add((post_uri, SIOC.link, rdflib.URIRef(row['postUrl'])))\n",
    "    \n",
    "    g.add((subreddit_uri, rdflib.RDF.type, SIOC.Container))\n",
    "    g.add((subreddit_uri, SIOC.has_post, post_uri))\n",
    "    g.add((post_uri, SIOC.Container, subreddit_uri))\n",
    "\n",
    "    g.add((author_uri, rdflib.RDF.type, FOAF.Person))\n",
    "    g.add((author_uri, FOAF.name, rdflib.Literal(row['authorName'])))\n",
    "    g.add((post_uri, SIOC.has_creator, author_uri))\n",
    "\n",
    "    comment_uri = rdflib.URIRef(f\"http://reddit.com/comment/{index}\")\n",
    "    g.add((comment_uri, rdflib.RDF.type, SIOC.Comment))\n",
    "    g.add((comment_uri, DCMI.title, rdflib.Literal(row['comments'])))\n",
    "    g.add((post_uri, SIOC.has_reply, comment_uri))\n",
    "\n",
    "    g.add((topic_uri, SIOC.has_post, post_uri))\n",
    "\n",
    "# Group by subreddit and process each one for topic modeling\n",
    "GroupedData = DF.groupby('subReddit')\n",
    "all_topics = defaultdict(set)\n",
    "for subreddit, group in GroupedData:\n",
    "    print(f\"Processing Subreddit: {subreddit}\")\n",
    "    group['Combined'] = group['postTitle'] + group['postDesc']\n",
    "    \n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in custom_stopwords])\n",
    "    \n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "\n",
    "    LDA = gensim.models.LdaModel(\n",
    "        corpus,\n",
    "        num_topics=5,\n",
    "        id2word=dictionary,\n",
    "        passes=15,\n",
    "        iterations=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    for idx, topic in LDA.print_topics(num_words=5):\n",
    "        topic_words = topic.split(\" + \")\n",
    "        unique_words = {word.split(\"*\")[1].strip('\"') for word in topic_words}\n",
    "        all_topics[subreddit].update(unique_words)\n",
    "\n",
    "# Save RDF graph in Turtle and JSON-LD formats\n",
    "g.serialize('D:/FYP/Github/data-tails/Backend/Ontologies/KG1.ttl', format='turtle')\n",
    "json_ld = g.serialize(format='json-ld', indent=4)\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/KG1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_ld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddits and topics have been saved to Topics.csv.\n",
      "Filtered topics saved to D:/FYP/Github/data-tails/Backend/Ontologies/FTopics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save subreddits and topics to Topics.csv\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/Topics.csv\", mode='w', newline='', encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"subreddit\", \"topics\"])\n",
    "    for subreddit, topics in all_topics.items():\n",
    "        writer.writerow([subreddit, \", \".join(sorted(topics))])\n",
    "\n",
    "print(\"Subreddits and topics have been saved to Topics.csv.\")\n",
    "\n",
    "\n",
    "\n",
    "# List of subreddits to include in the new CSV\n",
    "selected_subreddits = [\n",
    "    \"todayilearned\", \"news\", \"mildlyinteresting\", \"AskReddit\", \"Damnthatsinteresting\", \n",
    "    \"showerthoughts\", \"wallstreetbets\", \"NatureIsFuckingLit\", \"LifeProTips\", \"space\", \n",
    "    \"MovieDetails\", \"tifu\", \"futurology\", \"Music\", \"europe\", \"unpopularopinion\", \n",
    "    \"pakistan\", \"islamabad\", \"lahore\", \"karachi\"\n",
    "]\n",
    "\n",
    "# Read the existing topics CSV\n",
    "topics_df = pd.read_csv(\"D:/FYP/Github/data-tails/Backend/Ontologies/Topics.csv\")\n",
    "\n",
    "# Filter rows where 'subreddit' is in the selected subreddits list\n",
    "filtered_topics_df = topics_df[topics_df['subreddit'].isin(selected_subreddits)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_topics_csv_path = \"D:/FYP/Github/data-tails/Backend/Ontologies/FTopics.csv\"\n",
    "filtered_topics_df.to_csv(filtered_topics_csv_path, index=False)\n",
    "\n",
    "print(f\"Filtered topics saved to {filtered_topics_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x000001F35584A9E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\isodate\\isodatetime.py\", line 55, in parse_datetime\n",
      "    tmpdate = parse_date(datestring)\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\isodate\\isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: 'Na'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Context:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, Namespace, Literal\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import subprocess\n",
    "import re\n",
    "from isodate import parse_datetime, ISO8601Error\n",
    "\n",
    "# Namespaces\n",
    "SIOC = Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "reddit = Namespace(\"http://reddit.com/\")\n",
    "\n",
    "# Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "def clean_dates(graph):\n",
    "    for s, p, o in graph.triples((None, DCMI.date, None)):\n",
    "        if isinstance(o, Literal):\n",
    "            try:\n",
    "                parse_datetime(str(o))  # Validate\n",
    "            except ISO8601Error:\n",
    "                graph.remove((s, p, o))\n",
    "                graph.add((s, p, Literal(\"1970-01-01T00:00:00\", datatype=SIOC.date)))\n",
    "\n",
    "def retrieve_expanded_context(graph, user_query, depth=2):\n",
    "    sparql_query = f\"\"\"\n",
    "    SELECT ?subject ?predicate ?object\n",
    "    WHERE {{\n",
    "        ?subject ?predicate ?object .\n",
    "        OPTIONAL {{ ?subject <{DCMI.title}> ?title }}\n",
    "        OPTIONAL {{ ?subject <{DCMI.description}> ?description }}\n",
    "        FILTER(\n",
    "            CONTAINS(LCASE(STR(?title)), LCASE(\"{user_query}\")) || \n",
    "            CONTAINS(LCASE(STR(?description)), LCASE(\"{user_query}\"))\n",
    "        )\n",
    "    }}\n",
    "    LIMIT {depth * 10}\n",
    "    \"\"\"\n",
    "    results = graph.query(sparql_query)\n",
    "    return \"\\n\".join([f\"{s} --[{p}]--> {o}\" for s, p, o in results])\n",
    "\n",
    "def query_wizardlm2(context, user_query):\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{user_query}\\n\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"wizardlm2\"],\n",
    "            input=prompt.encode(\"utf-8\"),  # Encode input\n",
    "            capture_output=True\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(result.stderr.decode(\"utf-8\"))\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error querying WizardLM2: {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kg_file = \"./KG.ttl\"\n",
    "    kg = Graph()\n",
    "    kg.parse(kg_file, format=\"turtle\")\n",
    "    clean_dates(kg)\n",
    "\n",
    "    user_query = \"I have OCD, neeed help?\"\n",
    "    context = retrieve_expanded_context(kg, user_query, depth=3)\n",
    "    print(f\"Retrieved Context:\\n{context}\")\n",
    "\n",
    "    response = query_wizardlm2(context, user_query)\n",
    "    print(f\"\\nLLM Response:\\n{response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
