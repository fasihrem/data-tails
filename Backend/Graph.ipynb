{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontologies to use\n",
    "- **Post-Subreddit:** A post belongs to a specific subreddit.\n",
    "- **Post-Author:** A post is created by an author.\n",
    "- **Post-Topics:** A post is related to one or more topics based on the topic modeling results.\n",
    "- **Post-Comments:** A post has a set of comments.\n",
    "- **Post-Keywords:** A post is associated with specific keywords derived from the topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvis in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (8.9.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.3.0)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "!pip install pyvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import rdflib\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from gensim import corpora\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from *MongoDB*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to MongoDB\n",
    "- Read the data from the collection\n",
    "- Convert the data into a pandas DataFrame\n",
    "- Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  type  \\\n",
      "0  66e965e698330736e0d693d5  None   \n",
      "1  66e965e698330736e0d693d6  None   \n",
      "2  66e965e698330736e0d693d7  None   \n",
      "3  66e965e698330736e0d693d8  None   \n",
      "4  66e965e698330736e0d693d9  None   \n",
      "\n",
      "                                           postTitle postDesc  \\\n",
      "0  Adults(especially those over 30), how young do...      NaN   \n",
      "1  What is a thing that your parents consider nor...      NaN   \n",
      "2                 What is a smell that comforts you?      NaN   \n",
      "3  When in history was it the best time to be a w...      NaN   \n",
      "4    What's the worst way someone broke up with you?      NaN   \n",
      "\n",
      "              postTime            authorName noOfUpvotes isNSFW  \\\n",
      "0  2024-08-06 01:02:35  Excellent-Studio7257        4068  False   \n",
      "1  2024-08-06 01:47:22        Bigbumoffhappy        2073  False   \n",
      "2  2024-08-05 22:21:53         bloomsmittenn        2188  False   \n",
      "3  2024-08-06 03:32:59   More_food_please_77         778  False   \n",
      "4  2024-08-05 21:01:39    ImpressiveWrap7363        1989  False   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  [It's weird. I can have an adult/intellectual ...        5590.0   \n",
      "1  [Thinking you're supposed to raise your kids t...        1684.0   \n",
      "2  [The weather in the woods after a heavy rain, ...        4958.0   \n",
      "3  [It might depend on where you are but now with...        1046.0   \n",
      "4  [I have 2 bad ones.  First one, he had his mom...        1749.0   \n",
      "\n",
      "                                            imageUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                             postUrl  subReddit  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['DataTails']\n",
    "collection = db['Data']  \n",
    "data_cursor = collection.find({})\n",
    "DF = pd.DataFrame(list(data_cursor))\n",
    "print(DF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stopword Removal & Lemmatization:** \n",
    "    - Preprocessing() uses NLTK to remove stopwords and lemmatize the text.\n",
    "- **Handling Missing Data:**\n",
    "    - Missing postDesc fields are filled with an empty string.\n",
    "    - Missing noOfUpvotes is filled with 0.\n",
    "- **Datetime Conversion:** \n",
    "    - postTime is converted to a datetime object, and any errors are coerced.\n",
    "- **Handling Comments:** \n",
    "    - Comments are converted from list format to a string of concatenated comments.\n",
    "- **Final Clean Text:** \n",
    "    - Both postTitle and postDesc are cleaned using regular expressions and tokenization, and then passed through the NLTK-based text preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subReddit           2\n",
      "postTitle           0\n",
      "postDesc        15607\n",
      "postTime            0\n",
      "authorName        587\n",
      "noOfUpvotes         0\n",
      "comments            0\n",
      "noOfComments        2\n",
      "postUrl             2\n",
      "imageUrl            3\n",
      "isNSFW              1\n",
      "dtype: int64\n",
      "   subReddit                                          postTitle postDesc  \\\n",
      "0  AskReddit                   [adult, especially, young, seem]       []   \n",
      "1  AskReddit  [parent, consider, normal, consider, normal, a...       []   \n",
      "2  AskReddit                                   [smell, comfort]       []   \n",
      "3  AskReddit                       [history, best, time, woman]       []   \n",
      "4  AskReddit                       [worst, way, someone, broke]       []   \n",
      "\n",
      "             postTime            authorName noOfUpvotes  \\\n",
      "0 2024-08-06 01:02:35  Excellent-Studio7257        4068   \n",
      "1 2024-08-06 01:47:22        Bigbumoffhappy        2073   \n",
      "2 2024-08-05 22:21:53         bloomsmittenn        2188   \n",
      "3 2024-08-06 03:32:59   More_food_please_77         778   \n",
      "4 2024-08-05 21:01:39    ImpressiveWrap7363        1989   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  It's weird. I can have an adult/intellectual c...        5590.0   \n",
      "1  Thinking you're supposed to raise your kids th...        1684.0   \n",
      "2  The weather in the woods after a heavy rain Ne...        4958.0   \n",
      "3  It might depend on where you are but now with ...        1046.0   \n",
      "4  I have 2 bad ones.  First one, he had his mom ...        1749.0   \n",
      "\n",
      "                                             postUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                            imageUrl  isNSFW  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "subReddit       0\n",
      "postTitle       0\n",
      "postDesc        0\n",
      "postTime        1\n",
      "authorName      0\n",
      "noOfUpvotes     0\n",
      "comments        0\n",
      "noOfComments    0\n",
      "postUrl         0\n",
      "imageUrl        0\n",
      "isNSFW          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "StopWords = set(stopwords.words('english'))\n",
    "custom_stopwords = StopWords | {\"make\", \"thing\", \"know\", \"get\", \"want\", \"like\", \"would\", \"could\", \"you\", \"say\",\"also\",\"aita\",\"com\",\"www\",\"made\",\"ago\",\"day\",\"000\"}\n",
    "def Preprocessing(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in custom_stopwords and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "Cols = ['subReddit', 'postTitle', 'postDesc', 'postTime', 'authorName', 'noOfUpvotes', 'comments', 'noOfComments', 'postUrl','imageUrl','isNSFW']\n",
    "DF = DF[Cols]\n",
    "print(DF.isnull().sum())\n",
    "\n",
    "\n",
    "DF['subReddit'] = DF['subReddit'].fillna('Unknown_SubReddit')\n",
    "DF['authorName'] = DF['authorName'].fillna('Unknown_Author')\n",
    "DF['postTitle'] = DF['postTitle'].fillna('Untitled')\n",
    "DF['postUrl'] = DF['postUrl'].fillna('http://example.com/NOPOST.png')\n",
    "DF['imageUrl'] = DF['imageUrl'].fillna('http://example.com/NOImage.png')\n",
    "DF['isNSFW'] = DF['isNSFW'].fillna(False)\n",
    "DF['postDesc'].fillna('', inplace=True)\n",
    "DF['noOfUpvotes'].fillna(0, inplace=True)\n",
    "DF['noOfComments'].fillna(0, inplace=True)\n",
    "DF['postTime'] = pd.to_datetime(DF['postTime'], errors='coerce')\n",
    "DF['comments'] = DF['comments'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "DF['postTitle'] = DF['postTitle'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['postDesc'] = DF['postDesc'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['isNSFW'] = DF['isNSFW'].astype(bool)\n",
    "print(DF.head())\n",
    "print(DF.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import json\n",
    "\n",
    "# RDF graph initialization\n",
    "g = rdflib.Graph()\n",
    "SIOC = rdflib.Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = rdflib.Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = rdflib.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "REDDIT = rdflib.Namespace(\"http://reddit.com/ns#\")  # Custom namespace for Reddit-specific relationships\n",
    "g.bind(\"sioc\", SIOC)\n",
    "g.bind(\"dc\", DCMI)\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"reddit\", REDDIT)\n",
    "\n",
    "\n",
    "# Function to add post data to RDF graph\n",
    "def add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri):\n",
    "    post_uri = URIRef(f\"http://reddit.com/post{index}\")\n",
    "    subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{row['subReddit']}\")\n",
    "    author_uri = URIRef(f\"http://reddit.com/user/{row['authorName']}\")\n",
    "    \n",
    "    # Add post properties and link to topic\n",
    "    g.add((post_uri, rdflib.RDF.type, SIOC.Post))\n",
    "    g.add((post_uri, DCMI.title, Literal(' '.join(row['postTitle']))))\n",
    "    g.add((post_uri, DCMI.description, Literal(' '.join(row['postDesc']))))\n",
    "    g.add((post_uri, DCMI.date, Literal(row['postTime'])))\n",
    "    g.add((post_uri, SIOC.num_replies, Literal(row['noOfUpvotes'])))\n",
    "    g.add((post_uri, SIOC.link, URIRef(row['postUrl'])))\n",
    "    g.add((post_uri, SIOC.NSFW, Literal(row['isNSFW'])))\n",
    "    g.add((post_uri, SIOC.has_type, post_type_uri))\n",
    "    g.add((post_uri, SIOC.topic, topic_uri))\n",
    "\n",
    "    # Create the relationships based on the list of relationship types\n",
    "    # CreatedBy: Indicates that a post is created by an author.\n",
    "    g.add((post_uri, REDDIT.CreatedBy, author_uri))\n",
    "\n",
    "    # BelongsTo: Links a post to a subreddit.\n",
    "    g.add((post_uri, REDDIT.BelongsTo, subreddit_uri))\n",
    "\n",
    "    # PartOfPost: Indicates that a comment is part of a post (the comment belongs to the post).\n",
    "    comment_uri = URIRef(f\"http://reddit.com/comment/{index}\")\n",
    "    g.add((comment_uri, REDDIT.PartOfPost, post_uri))\n",
    "\n",
    "    # HasType: Links an entity (post, comment) to its specific type (text post, link post, etc.).\n",
    "    g.add((post_uri, REDDIT.HasType, post_type_uri))\n",
    "    g.add((comment_uri, REDDIT.HasType, comment_type_uri))\n",
    "\n",
    "    # TaggedIn: Indicates that a post or comment is tagged with a specific topic or keyword.\n",
    "    for tag in row.get('tags', []):  # Assuming tags are available as a list\n",
    "        tag_uri = URIRef(f\"http://reddit.com/tag/{tag}\")\n",
    "        g.add((post_uri, REDDIT.TaggedIn, tag_uri))\n",
    "        g.add((comment_uri, REDDIT.TaggedIn, tag_uri))\n",
    "\n",
    "    # UpvotedBy: Indicates that a user has upvoted a post or comment.\n",
    "    if row.get('upvotedBy'):\n",
    "        for upvoter in row['upvotedBy']:\n",
    "            upvoter_uri = URIRef(f\"http://reddit.com/user/{upvoter}\")\n",
    "            g.add((post_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "\n",
    "    # DownvotedBy: Indicates that a user has downvoted a post or comment.\n",
    "    if row.get('downvotedBy'):\n",
    "        for downvoter in row['downvotedBy']:\n",
    "            downvoter_uri = URIRef(f\"http://reddit.com/user/{downvoter}\")\n",
    "            g.add((post_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "\n",
    "    # Moderates: Indicates that a user is a moderator of a subreddit.\n",
    "    if row.get('moderators'):\n",
    "        for moderator in row['moderators']:\n",
    "            moderator_uri = URIRef(f\"http://reddit.com/user/{moderator}\")\n",
    "            g.add((subreddit_uri, REDDIT.Moderates, moderator_uri))\n",
    "\n",
    "    # HasFlair: Links a post or comment to a specific flair (e.g., tags like \"Important\", \"Question\", etc.).\n",
    "    if row.get('flair'):\n",
    "        flair_uri = URIRef(f\"http://reddit.com/flair/{row['flair']}\")\n",
    "        g.add((post_uri, REDDIT.HasFlair, flair_uri))\n",
    "        g.add((comment_uri, REDDIT.HasFlair, flair_uri))\n",
    "\n",
    "    # Link post to subreddit and type\n",
    "    g.add((subreddit_uri, rdflib.RDF.type, SIOC.Container))\n",
    "    g.add((subreddit_uri, SIOC.has_post, post_uri))\n",
    "    g.add((subreddit_uri, SIOC.has_type, subreddit_type_uri))\n",
    "    g.add((post_uri, SIOC.Container, subreddit_uri))\n",
    "\n",
    "    # Link to author and author type\n",
    "    g.add((author_uri, rdflib.RDF.type, FOAF.Person))\n",
    "    g.add((author_uri, FOAF.name, Literal(row['authorName'])))\n",
    "    g.add((post_uri, SIOC.has_creator, author_uri))\n",
    "    g.add((author_uri, SIOC.has_type, author_type_uri))\n",
    "\n",
    "    # Add comments and link to comment type\n",
    "    g.add((comment_uri, rdflib.RDF.type, SIOC.Comment))\n",
    "    g.add((comment_uri, DCMI.title, Literal(row['comments'])))\n",
    "    g.add((post_uri, SIOC.has_reply, comment_uri))\n",
    "    g.add((comment_uri, SIOC.has_type, comment_type_uri))\n",
    "\n",
    "# Process each subreddit and create LDA models\n",
    "GroupedData = DF.groupby('subReddit')\n",
    "all_topics = defaultdict(set)\n",
    "\n",
    "for subreddit, group in GroupedData:\n",
    "    group['Combined'] = group['postTitle'] + group['postDesc']\n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in custom_stopwords])\n",
    "\n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "    LDA = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, iterations=100, random_state=42)\n",
    "    \n",
    "    for idx, topic in LDA.print_topics(num_words=5):\n",
    "        topic_uri = URIRef(f\"http://reddit.com/topic/{subreddit}_{idx}\")\n",
    "        topic_words = topic.split(\" + \")\n",
    "        unique_words = {word.split(\"*\")[1].strip('\"') for word in topic_words}\n",
    "        all_topics[subreddit].update(unique_words)\n",
    "\n",
    "        # Define URIs for types\n",
    "        subreddit_type_uri = URIRef(f\"http://reddit.com/type/subreddit/{subreddit}\")\n",
    "        post_type_uri = URIRef(f\"http://reddit.com/type/post/{subreddit}\")\n",
    "        comment_type_uri = URIRef(f\"http://reddit.com/type/comment/{subreddit}\")\n",
    "        author_type_uri = URIRef(f\"http://reddit.com/type/author/{subreddit}\")\n",
    "\n",
    "        # Link subreddit to topic\n",
    "        subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{subreddit}\")\n",
    "        g.add((subreddit_uri, SIOC.has_topic, topic_uri))\n",
    "        g.add((topic_uri, rdflib.RDF.type, SIOC.Topic))\n",
    "\n",
    "        # Iterate through each row in the group\n",
    "        for index, row in group.iterrows():\n",
    "            add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri)\n",
    "\n",
    "# Save graph\n",
    "g.serialize('D:/FYP/Github/data-tails/Backend/Ontologies/KG.ttl', format='turtle')\n",
    "json_ld = g.serialize(format='json-ld', indent=4)\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/KG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting *KG.json, KG.ttl* to format of D3 input file to view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import rdflib\n",
    "# from rdflib.namespace import RDF, SIOC, REDDIT\n",
    "\n",
    "# Initialize D3-compatible JSON structure\n",
    "d3_data = {\n",
    "    \"nodes\": [],\n",
    "    \"links\": []\n",
    "}\n",
    "\n",
    "def parse_label(url):\n",
    "    if \"subreddit\" in url:\n",
    "        # Extract subreddit name after /subreddit/\n",
    "        name = url.split(\"/subreddit/\")[1]\n",
    "        return f\"Subreddit({name})\"\n",
    "    elif \"topic\" in url:\n",
    "        # Extract topic name after /topic/\n",
    "        name = url.split(\"/topic/\")[1]\n",
    "        return f\"Topic({name})\"\n",
    "    elif \"post\" in url:\n",
    "        # Extract post ID after /post\n",
    "        name = url.split(\"/reddit.com/\")[1]\n",
    "        return f\"Post({name})\"\n",
    "    elif \"user\" in url:\n",
    "        # Extract username after /user/\n",
    "        name = url.split(\"/user/\")[1]\n",
    "        return f\"Author({name})\"\n",
    "    elif \"comment\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/comment/\")[1]\n",
    "        return f\"Comment({name})\"\n",
    "    elif \"tag\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/tag/\")[1]\n",
    "        return f\"Tag({name})\"\n",
    "    elif \"upvoter\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/upvoter/\")[1]\n",
    "        return f\"UpVote({name})\"\n",
    "    elif \"downvoter\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/downvoter/\")[1]\n",
    "        return f\"DownVote({name})\"\n",
    "    return url  # In case no specific type is found, return the URL itself\n",
    "\n",
    "# Create a mapping for nodes to avoid duplicates\n",
    "node_map = {}\n",
    "\n",
    "# Function to add a node if it doesn't already exist\n",
    "def add_node(label, node_type, group, description=\"\"):\n",
    "    if label not in node_map:\n",
    "        node_id = f\"{node_type}_{len(d3_data['nodes']) + 1}\"  # Create a unique node ID\n",
    "        node_map[label] = node_id\n",
    "        d3_data[\"nodes\"].append({\n",
    "            \"id\": node_id,\n",
    "            \"label\": label,\n",
    "            \"type\": node_type,\n",
    "            \"group\": group  # Group for categorization\n",
    "        })\n",
    "    return node_map[label]\n",
    "\n",
    "# Populate nodes and links based on the RDF graph structure\n",
    "for subreddit in g.subjects(rdflib.RDF.type, SIOC.Container):\n",
    "    subreddit_label = str(subreddit)\n",
    "    subreddit_label= parse_label(subreddit_label)\n",
    "    subreddit_id = add_node(subreddit_label, \"Subreddit\", group=0, description=\"Community for specific topics\")\n",
    "\n",
    "    # For each post in the subreddit\n",
    "    for post in g.objects(subreddit, SIOC.has_post):\n",
    "        post_label = str(post)\n",
    "        post_label= parse_label(post_label)\n",
    "        post_id = add_node(post_label, \"Post\", group=1, description=\"Individual posts in the subreddit\")\n",
    "\n",
    "        # Create a link from subreddit to post\n",
    "        d3_data[\"links\"].append({\n",
    "            \"source\": subreddit_id,\n",
    "            \"target\": post_id,\n",
    "            \"type\": \"Contains\",\n",
    "            \"weight\": 1  # Link weight can be adjusted based on relevance or count\n",
    "        })\n",
    "\n",
    "        # Add authors\n",
    "        author = g.value(post, SIOC.has_creator)\n",
    "        if author:\n",
    "            author_label = str(author)\n",
    "            author_label= parse_label(author_label)\n",
    "            author_id = add_node(author_label, \"Author\", group=2, description=\"User who created the post\")\n",
    "\n",
    "            # Create a link from post to author\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": author_id,\n",
    "                \"type\": \"CreatedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        # For each comment on the post\n",
    "        for comment in g.objects(post, SIOC.has_reply):\n",
    "            comment_label = str(comment)\n",
    "            comment_label= parse_label(comment_label)\n",
    "            comment_id = add_node(comment_label, \"Comment\", group=3, description=\"User comments on the post\")\n",
    "\n",
    "            # Create a link from post to comment\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": comment_id,\n",
    "                \"type\": \"HasReply\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "            # Link comment to author if available\n",
    "            if author:\n",
    "                d3_data[\"links\"].append({\n",
    "                    \"source\": author_id,\n",
    "                    \"target\": comment_id,\n",
    "                    \"type\": \"CommentedBy\",\n",
    "                    \"weight\": 1\n",
    "                })\n",
    "\n",
    "            # Add tags to comment if available\n",
    "            for tag in g.objects(post, REDDIT.TaggedIn):\n",
    "                tag_label = str(tag)\n",
    "                tag_label= parse_label(tag_label)\n",
    "                tag_id = add_node(tag_label, \"Tag\", group=4, description=\"Tags related to the post or comment\")\n",
    "                d3_data[\"links\"].append({\n",
    "                    \"source\": comment_id,\n",
    "                    \"target\": tag_id,\n",
    "                    \"type\": \"TaggedWith\",\n",
    "                    \"weight\": 1\n",
    "                })\n",
    "\n",
    "        # Add topics to post\n",
    "        for topic in g.objects(post, SIOC.topic):\n",
    "            topic_label = str(topic)\n",
    "            topic_label= parse_label(topic_label)\n",
    "            topic_id = add_node(topic_label, \"Topic\", group=5, description=\"Topic related to the post\")\n",
    "\n",
    "            # Create a link from post to topic\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": topic_id,\n",
    "                \"type\": \"RelatedTo\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        # Add votes (upvotes and downvotes)\n",
    "        for upvoter in g.objects(post, REDDIT.UpvotedBy):\n",
    "            upvoter_label = str(upvoter)\n",
    "            upvoter_label= parse_label(upvoter_label)\n",
    "            upvoter_id = add_node(upvoter_label, \"Upvoter\", group=6, description=\"User who upvoted the post\")\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": upvoter_id,\n",
    "                \"type\": \"UpvotedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        for downvoter in g.objects(post, REDDIT.DownvotedBy):\n",
    "            downvoter_label = str(downvoter)\n",
    "            downvoter_label= parse_label(downvoter_label)\n",
    "            downvoter_id = add_node(downvoter_label, \"Downvoter\", group=7, description=\"User who downvoted the post\")\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": downvoter_id,\n",
    "                \"type\": \"DownvotedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "# Write the D3-compatible JSON data to a file\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/D3KG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(d3_data, f, indent=4, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import rdflib\n",
    "from collections import deque\n",
    "\n",
    "# Initialize D3-compatible JSON structure\n",
    "d3_data = {\n",
    "    \"nodes\": [],\n",
    "    \"links\": []\n",
    "}\n",
    "\n",
    "# Create a mapping for nodes to avoid duplicates\n",
    "node_map = {}\n",
    "\n",
    "# Function to add a node if it doesn't already exist\n",
    "def add_node(label, node_type, group, description):\n",
    "    if label not in node_map:\n",
    "        node_id = f\"{node_type}_{len(d3_data['nodes']) + 1}\"  # Create a unique node ID\n",
    "        node_map[label] = node_id\n",
    "        d3_data[\"nodes\"].append({\n",
    "            \"id\": node_id,\n",
    "            \"label\": label,\n",
    "            \"type\": node_type,\n",
    "            \"group\": group,\n",
    "            \"description\": description\n",
    "        })\n",
    "    return node_map[label]\n",
    "\n",
    "# Function to perform BFS and retrieve a subgraph\n",
    "def bfs(start_node_id, max_depth):\n",
    "    visited = set()\n",
    "    queue = deque([(start_node_id, 0)])  # (node_id, depth)\n",
    "    nodes_to_display = set([start_node_id])\n",
    "    links_to_display = []\n",
    "\n",
    "    # Mark the starting node to highlight it\n",
    "    start_node_found = False\n",
    "\n",
    "    while queue:\n",
    "        current_node_id, current_depth = queue.popleft()\n",
    "        if current_depth < max_depth:\n",
    "            # Add links of current node to the links_to_display\n",
    "            for link in d3_data[\"links\"]:\n",
    "                if link[\"source\"] == current_node_id and link[\"target\"] not in visited:\n",
    "                    links_to_display.append(link)\n",
    "                    queue.append((link[\"target\"], current_depth + 1))\n",
    "                elif link[\"target\"] == current_node_id and link[\"source\"] not in visited:\n",
    "                    links_to_display.append(link)\n",
    "                    queue.append((link[\"source\"], current_depth + 1))\n",
    "\n",
    "        visited.add(current_node_id)\n",
    "\n",
    "        # Highlight the start node\n",
    "        if current_node_id == start_node_id and not start_node_found:\n",
    "            start_node_found = True\n",
    "            for node in d3_data[\"nodes\"]:\n",
    "                if node[\"id\"] == start_node_id:\n",
    "                    node[\"highlighted\"] = \"Head\"\n",
    "\n",
    "    # Filter the nodes that are part of the display\n",
    "    for link in links_to_display:\n",
    "        nodes_to_display.add(link[\"source\"])\n",
    "        nodes_to_display.add(link[\"target\"])\n",
    "\n",
    "    # Filter out the nodes and links that should be displayed\n",
    "    filtered_nodes = [node for node in d3_data[\"nodes\"] if node[\"id\"] in nodes_to_display]\n",
    "    filtered_links = [link for link in links_to_display]\n",
    "\n",
    "    return filtered_nodes, filtered_links\n",
    "\n",
    "# Load the RDF data from the D3KG.json file\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/D3KG.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    d3_data = json.load(f)\n",
    "\n",
    "# Example of how to call BFS with a starting node and max depth\n",
    "start_node = \"Comment_4\"\n",
    "max_depth = 2\n",
    "filtered_nodes, filtered_links = bfs(start_node, max_depth)\n",
    "\n",
    "# Write the filtered subgraph to a file\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/BFS.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"nodes\": filtered_nodes, \"links\": filtered_links}, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T08:04:13.944503Z",
     "start_time": "2025-03-07T08:04:08.585281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Loading Knowledge Graphs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [2], line 171\u001B[0m\n\u001B[1;32m    168\u001B[0m kg_ttl_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./KG.ttl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m🔍 Loading Knowledge Graphs...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 171\u001B[0m kg_json \u001B[38;5;241m=\u001B[39m load_kg_json(kg_json_path)\n\u001B[1;32m    172\u001B[0m kg_ttl, adjacency_list \u001B[38;5;241m=\u001B[39m load_kg_ttl(kg_ttl_path)\n\u001B[1;32m    174\u001B[0m user_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEnter your query: \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn [2], line 45\u001B[0m, in \u001B[0;36mload_kg_json\u001B[0;34m(file_path)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 45\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ Loaded KG.json with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m entities.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {entity[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m@id\u001B[39m\u001B[38;5;124m\"\u001B[39m]: entity \u001B[38;5;28;01mfor\u001B[39;00m entity \u001B[38;5;129;01min\u001B[39;00m data}\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py:293\u001B[0m, in \u001B[0;36mload\u001B[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(fp, \u001B[38;5;241m*\u001B[39m, \u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, object_hook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, parse_float\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    275\u001B[0m         parse_int\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, parse_constant\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, object_pairs_hook\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw):\n\u001B[1;32m    276\u001B[0m     \u001B[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001B[39;00m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;124;03m    a JSON document) to a Python object.\u001B[39;00m\n\u001B[1;32m    278\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001B[39;00m\n\u001B[1;32m    292\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobject_hook\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobject_hook\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    295\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_float\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_float\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparse_int\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_int\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparse_constant\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparse_constant\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mobject_pairs_hook\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobject_pairs_hook\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[1;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[1;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[0;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    348\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONDecoder\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[0;34m(self, s, _w)\u001B[0m\n\u001B[1;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, s, _w\u001B[38;5;241m=\u001B[39mWHITESPACE\u001B[38;5;241m.\u001B[39mmatch):\n\u001B[1;32m    333\u001B[0m     \u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[1;32m    334\u001B[0m \u001B[38;5;124;03m    containing a JSON document).\u001B[39;00m\n\u001B[1;32m    335\u001B[0m \n\u001B[1;32m    336\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 337\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    338\u001B[0m     end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n\u001B[1;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:353\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[0;34m(self, s, idx)\u001B[0m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001B[39;00m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    350\u001B[0m \n\u001B[1;32m    351\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 353\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscan_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import rdflib\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "from collections import deque, defaultdict\n",
    "from rdflib import Graph, Namespace, Literal, URIRef\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from groq import Groq\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ✅ Initialize Groq Client\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# ✅ CSV File for Conversation History\n",
    "csv_file_path = \"conversation_history.csv\"\n",
    "conversation_history = []\n",
    "\n",
    "# ✅ Define RDF Namespaces\n",
    "SIOC = Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "REDDIT = Namespace(\"http://reddit.com/ns#\")\n",
    "\n",
    "# ✅ NLP Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Extracts meaningful words from text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "# ✅ Load KG.json for Fast Lookups\n",
    "def load_kg_json(file_path):\n",
    "    \"\"\"Loads KG.json and converts it into a dictionary for fast lookup.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"✅ Loaded KG.json with {len(data)} entities.\")\n",
    "        return {entity[\"@id\"]: entity for entity in data}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading KG.json: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ✅ Load KG.ttl & Build Adjacency List\n",
    "def load_kg_ttl(file_path):\n",
    "    \"\"\"Loads KG.ttl and builds an adjacency list for fast graph traversal.\"\"\"\n",
    "    try:\n",
    "        g = Graph()\n",
    "        g.parse(file_path, format=\"turtle\")\n",
    "        adjacency_list = defaultdict(list)\n",
    "        for s, p, o in g:\n",
    "            adjacency_list[str(s)].append((s, p, o))\n",
    "            adjacency_list[str(o)].append((s, p, o))  \n",
    "        print(f\"✅ Loaded KG.ttl with {len(g)} triples.\")\n",
    "        return g, adjacency_list\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading KG.ttl: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ✅ Multi-threaded BFS Traversal\n",
    "def bfs_traverse_parallel(adjacency_list, start_nodes, max_depth=5):\n",
    "    \"\"\"Performs parallel BFS traversal for faster subgraph extraction.\"\"\"\n",
    "    if not adjacency_list:\n",
    "        return \"❌ Knowledge Graph not loaded.\"\n",
    "\n",
    "    visited = set()\n",
    "    queue = deque([(node, 0) for node in start_nodes])\n",
    "    nodes_to_display, links_to_display = set(start_nodes), set()\n",
    "    results = []\n",
    "\n",
    "    def process_node(current_node, depth):\n",
    "        if depth < max_depth:\n",
    "            for s, p, o in adjacency_list.get(current_node, []):\n",
    "                if isinstance(o, Literal):  \n",
    "                    results.append(str(o))  \n",
    "                else:\n",
    "                    links_to_display.add((s, p, o))\n",
    "                    queue.append((str(o), depth + 1))\n",
    "                    queue.append((str(s), depth + 1))\n",
    "\n",
    "            visited.add(current_node)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        while queue:\n",
    "            current_node, depth = queue.popleft()\n",
    "            executor.submit(process_node, current_node, depth)\n",
    "\n",
    "    return {\"context\": results[:10]} if results else \"❌ Data not found.\"\n",
    "\n",
    "# ✅ Fast Context Retrieval\n",
    "def retrieve_context_fast(kg_json, adjacency_list, user_query):\n",
    "    \"\"\"Finds relevant nodes based on user query and retrieves subgraph using BFS.\"\"\"\n",
    "    if not kg_json:\n",
    "        return \"❌ KG.json not loaded.\"\n",
    "\n",
    "    query_keywords = preprocess_text(user_query)\n",
    "\n",
    "    # **Step 1: Fast Lookup in KG.json**\n",
    "    matched_entities = set()\n",
    "    for entity_id, entity in kg_json.items():\n",
    "        for key, value in entity.items():\n",
    "            if isinstance(value, str) and any(keyword in value.lower() for keyword in query_keywords):\n",
    "                matched_entities.add(entity_id)\n",
    "\n",
    "    if not matched_entities:\n",
    "        return \"❌ Data not found.\"\n",
    "\n",
    "    # **Step 2: BFS traversal in KG.ttl for details**\n",
    "    context_results = []\n",
    "    for entity in matched_entities:\n",
    "        subgraph = bfs_traverse_parallel(adjacency_list, [entity], max_depth=5)\n",
    "        if subgraph != \"❌ Data not found.\":\n",
    "            context_results.extend(subgraph[\"context\"])\n",
    "\n",
    "    return {\"context\": context_results[:10]} if context_results else \"❌ Data not found.\"\n",
    "\n",
    "# ✅ Groq Chat API\n",
    "def chat_with_groq(context, user_query):\n",
    "    \"\"\"Interacts with Groq model using retrieved KG context.\"\"\"\n",
    "    global conversation_history\n",
    "\n",
    "    # Add user message to conversation history\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "    # Create chat prompt\n",
    "    prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {user_query}\n",
    "\n",
    "    Provide a detailed answer based on the context.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call Groq API\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=conversation_history,  \n",
    "        model=\"llama3-8b-8192\"\n",
    "    )\n",
    "\n",
    "    # Extract response\n",
    "    response = chat_completion.choices[0].message.content\n",
    "\n",
    "    # Add response to history\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    # Update conversation history in CSV\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Role\", \"Content\"])\n",
    "        for entry in conversation_history:\n",
    "            writer.writerow([entry[\"role\"], entry[\"content\"]])\n",
    "\n",
    "    return response\n",
    "\n",
    "# ✅ Run Main Program\n",
    "if __name__ == \"__main__\":\n",
    "    kg_json_path = \"./KG.json\"\n",
    "    kg_ttl_path = \"./KG.ttl\"\n",
    "\n",
    "    print(\"\\n🔍 Loading Knowledge Graphs...\")\n",
    "    kg_json = load_kg_json(kg_json_path)\n",
    "    kg_ttl, adjacency_list = load_kg_ttl(kg_ttl_path)\n",
    "\n",
    "    user_query = input(\"Enter your query: \")\n",
    "\n",
    "    print(\"\\n🔍 Retrieving Context...\")\n",
    "    context = retrieve_context_fast(kg_json, adjacency_list, user_query)\n",
    "\n",
    "    print(\"\\n🤖 Querying Groq...\")\n",
    "    response = chat_with_groq(context, user_query)\n",
    "\n",
    "    print(\"\\n💡 Groq Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T07:59:02.778605Z",
     "start_time": "2025-03-07T07:58:58.509410Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/fasihrem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scatterplot', 'candlestick_chart', 'line_chart']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Download required dataset\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Define visualization categories\n",
    "chart_types = {\n",
    "    \"bar_chart\": [\"bar chart\", \"comparison\", \"categories\", \"grouped\", \"stacked\"],\n",
    "    \"line_chart\": [\"line chart\", \"trend\", \"time series\", \"growth\"],\n",
    "    \"area_chart\": [\"area chart\", \"filled line\", \"distribution\"],\n",
    "    \"scatterplot\": [\"scatterplot\", \"correlation\", \"relationship\", \"dot plot\"],\n",
    "    \"density_facet\": [\"density faceted\", \"density plot\", \"distribution\"],\n",
    "    \"gradient_encoding\": [\"gradient encoding\", \"color scale\", \"intensity\"],\n",
    "    \"candlestick_chart\": [\"candlestick\", \"stock\", \"market trends\"],\n",
    "    \"stacked_normalized_area_chart\": [\"normalized area chart\", \"stacked\", \"part-to-whole\"],\n",
    "\n",
    "    # Complex Visualizations\n",
    "    \"circle_packing\": [\"circle packing\", \"hierarchy\", \"nested\"],\n",
    "    \"dendrogram\": [\"dendrogram\", \"tree structure\", \"clustering\"],\n",
    "    \"DAG\": [\"directed acyclic graph\", \"dag\", \"flow\"],\n",
    "    \"treemap\": [\"treemap\", \"hierarchy\", \"proportion\"],\n",
    "    \"chord_diagram\": [\"chord diagram\", \"relationships\", \"connections\"],\n",
    "    \"heatmap\": [\"heatmap\", \"matrix\", \"intensity\"],\n",
    "    \"earthquake_globe\": [\"earthquake globe\", \"geospatial\", \"earthquake\"],\n",
    "    \"maps\": [\"map\", \"geographical\", \"location\"],\n",
    "    \"map_small_multiples\": [\"small multiples\", \"map comparison\"],\n",
    "    \"hexbin_map\": [\"hexbin map\", \"spatial aggregation\"],\n",
    "    \"centerline_labelling\": [\"centerline labeling\", \"map text\"],\n",
    "    \"voronoi_map\": [\"voronoi map\", \"spatial segmentation\"],\n",
    "    \"sorted_heatmap\": [\"sorted heatmap\", \"ranked matrix\"],\n",
    "    \n",
    "}\n",
    "\n",
    "# Flatten words for clustering\n",
    "all_terms = []\n",
    "term_to_category = {}\n",
    "for category, words in chart_types.items():\n",
    "    for word in words:\n",
    "        term_to_category[word] = category\n",
    "        all_terms.append(word)\n",
    "\n",
    "# Vectorize words\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(all_terms)\n",
    "\n",
    "# Cluster words dynamically\n",
    "kmeans = KMeans(n_clusters=len(chart_types), random_state=42, n_init=10)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Assign cluster labels\n",
    "word_clusters = {word: kmeans.labels_[i] for i, word in enumerate(all_terms)}\n",
    "\n",
    "# Function to recommend visualizations based on user query\n",
    "def recommend_visualizations(user_query):\n",
    "    recommended_categories = set()\n",
    "    \n",
    "    # Extract words from query\n",
    "    query_words = re.findall(r\"\\b\\w+\\b\", user_query.lower())\n",
    "\n",
    "    # Check for synonyms and assign categories\n",
    "    for word in query_words:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.name() in term_to_category:\n",
    "                    recommended_categories.add(term_to_category[lemma.name()])\n",
    "    \n",
    "    # Rank and return top 3 visualizations\n",
    "    recommendations = list(recommended_categories)\n",
    "    return recommendations[:3] if recommendations else [\"No matching visualizations found.\"]\n",
    "\n",
    "# Example Query\n",
    "query = \"I want to analyze stock trends and see relationships\"\n",
    "print(recommend_visualizations(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
