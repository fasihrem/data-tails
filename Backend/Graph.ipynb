{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontologies to use\n",
    "- **Post-Subreddit:** A post belongs to a specific subreddit.\n",
    "- **Post-Author:** A post is created by an author.\n",
    "- **Post-Topics:** A post is related to one or more topics based on the topic modeling results.\n",
    "- **Post-Comments:** A post has a set of comments.\n",
    "- **Post-Keywords:** A post is associated with specific keywords derived from the topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvis in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (8.9.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.3.0)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "!pip install pyvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import rdflib\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from gensim import corpora\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from *MongoDB*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to MongoDB\n",
    "- Read the data from the collection\n",
    "- Convert the data into a pandas DataFrame\n",
    "- Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  type  \\\n",
      "0  66e965e698330736e0d693d5  None   \n",
      "1  66e965e698330736e0d693d6  None   \n",
      "2  66e965e698330736e0d693d7  None   \n",
      "3  66e965e698330736e0d693d8  None   \n",
      "4  66e965e698330736e0d693d9  None   \n",
      "\n",
      "                                           postTitle postDesc  \\\n",
      "0  Adults(especially those over 30), how young do...      NaN   \n",
      "1  What is a thing that your parents consider nor...      NaN   \n",
      "2                 What is a smell that comforts you?      NaN   \n",
      "3  When in history was it the best time to be a w...      NaN   \n",
      "4    What's the worst way someone broke up with you?      NaN   \n",
      "\n",
      "              postTime            authorName noOfUpvotes isNSFW  \\\n",
      "0  2024-08-06 01:02:35  Excellent-Studio7257        4068  False   \n",
      "1  2024-08-06 01:47:22        Bigbumoffhappy        2073  False   \n",
      "2  2024-08-05 22:21:53         bloomsmittenn        2188  False   \n",
      "3  2024-08-06 03:32:59   More_food_please_77         778  False   \n",
      "4  2024-08-05 21:01:39    ImpressiveWrap7363        1989  False   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  [It's weird. I can have an adult/intellectual ...        5590.0   \n",
      "1  [Thinking you're supposed to raise your kids t...        1684.0   \n",
      "2  [The weather in the woods after a heavy rain, ...        4958.0   \n",
      "3  [It might depend on where you are but now with...        1046.0   \n",
      "4  [I have 2 bad ones.  First one, he had his mom...        1749.0   \n",
      "\n",
      "                                            imageUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                             postUrl  subReddit  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['DataTails']\n",
    "collection = db['Data']  \n",
    "data_cursor = collection.find({})\n",
    "DF = pd.DataFrame(list(data_cursor))\n",
    "print(DF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stopword Removal & Lemmatization:** \n",
    "    - Preprocessing() uses NLTK to remove stopwords and lemmatize the text.\n",
    "- **Handling Missing Data:**\n",
    "    - Missing postDesc fields are filled with an empty string.\n",
    "    - Missing noOfUpvotes is filled with 0.\n",
    "- **Datetime Conversion:** \n",
    "    - postTime is converted to a datetime object, and any errors are coerced.\n",
    "- **Handling Comments:** \n",
    "    - Comments are converted from list format to a string of concatenated comments.\n",
    "- **Final Clean Text:** \n",
    "    - Both postTitle and postDesc are cleaned using regular expressions and tokenization, and then passed through the NLTK-based text preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subReddit           2\n",
      "postTitle           0\n",
      "postDesc        15607\n",
      "postTime            0\n",
      "authorName        587\n",
      "noOfUpvotes         0\n",
      "comments            0\n",
      "noOfComments        2\n",
      "postUrl             2\n",
      "imageUrl            3\n",
      "isNSFW              1\n",
      "dtype: int64\n",
      "   subReddit                                          postTitle postDesc  \\\n",
      "0  AskReddit                   [adult, especially, young, seem]       []   \n",
      "1  AskReddit  [parent, consider, normal, consider, normal, a...       []   \n",
      "2  AskReddit                                   [smell, comfort]       []   \n",
      "3  AskReddit                       [history, best, time, woman]       []   \n",
      "4  AskReddit                       [worst, way, someone, broke]       []   \n",
      "\n",
      "             postTime            authorName noOfUpvotes  \\\n",
      "0 2024-08-06 01:02:35  Excellent-Studio7257        4068   \n",
      "1 2024-08-06 01:47:22        Bigbumoffhappy        2073   \n",
      "2 2024-08-05 22:21:53         bloomsmittenn        2188   \n",
      "3 2024-08-06 03:32:59   More_food_please_77         778   \n",
      "4 2024-08-05 21:01:39    ImpressiveWrap7363        1989   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  It's weird. I can have an adult/intellectual c...        5590.0   \n",
      "1  Thinking you're supposed to raise your kids th...        1684.0   \n",
      "2  The weather in the woods after a heavy rain Ne...        4958.0   \n",
      "3  It might depend on where you are but now with ...        1046.0   \n",
      "4  I have 2 bad ones.  First one, he had his mom ...        1749.0   \n",
      "\n",
      "                                             postUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                            imageUrl  isNSFW  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "subReddit       0\n",
      "postTitle       0\n",
      "postDesc        0\n",
      "postTime        1\n",
      "authorName      0\n",
      "noOfUpvotes     0\n",
      "comments        0\n",
      "noOfComments    0\n",
      "postUrl         0\n",
      "imageUrl        0\n",
      "isNSFW          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "StopWords = set(stopwords.words('english'))\n",
    "custom_stopwords = StopWords | {\"make\", \"thing\", \"know\", \"get\", \"want\", \"like\", \"would\", \"could\", \"you\", \"say\",\"also\",\"aita\",\"com\",\"www\",\"made\",\"ago\",\"day\",\"000\"}\n",
    "def Preprocessing(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in custom_stopwords and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "Cols = ['subReddit', 'postTitle', 'postDesc', 'postTime', 'authorName', 'noOfUpvotes', 'comments', 'noOfComments', 'postUrl','imageUrl','isNSFW']\n",
    "DF = DF[Cols]\n",
    "print(DF.isnull().sum())\n",
    "\n",
    "\n",
    "DF['subReddit'] = DF['subReddit'].fillna('Unknown_SubReddit')\n",
    "DF['authorName'] = DF['authorName'].fillna('Unknown_Author')\n",
    "DF['postTitle'] = DF['postTitle'].fillna('Untitled')\n",
    "DF['postUrl'] = DF['postUrl'].fillna('http://example.com/NOPOST.png')\n",
    "DF['imageUrl'] = DF['imageUrl'].fillna('http://example.com/NOImage.png')\n",
    "DF['isNSFW'] = DF['isNSFW'].fillna(False)\n",
    "DF['postDesc'].fillna('', inplace=True)\n",
    "DF['noOfUpvotes'].fillna(0, inplace=True)\n",
    "DF['noOfComments'].fillna(0, inplace=True)\n",
    "DF['postTime'] = pd.to_datetime(DF['postTime'], errors='coerce')\n",
    "DF['comments'] = DF['comments'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "DF['postTitle'] = DF['postTitle'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['postDesc'] = DF['postDesc'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['isNSFW'] = DF['isNSFW'].astype(bool)\n",
    "print(DF.head())\n",
    "print(DF.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import json\n",
    "\n",
    "# RDF graph initialization\n",
    "g = rdflib.Graph()\n",
    "SIOC = rdflib.Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = rdflib.Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = rdflib.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "REDDIT = rdflib.Namespace(\"http://reddit.com/ns#\")  # Custom namespace for Reddit-specific relationships\n",
    "g.bind(\"sioc\", SIOC)\n",
    "g.bind(\"dc\", DCMI)\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"reddit\", REDDIT)\n",
    "\n",
    "\n",
    "# Function to add post data to RDF graph\n",
    "def add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri):\n",
    "    post_uri = URIRef(f\"http://reddit.com/post{index}\")\n",
    "    subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{row['subReddit']}\")\n",
    "    author_uri = URIRef(f\"http://reddit.com/user/{row['authorName']}\")\n",
    "    \n",
    "    # Add post properties and link to topic\n",
    "    g.add((post_uri, rdflib.RDF.type, SIOC.Post))\n",
    "    g.add((post_uri, DCMI.title, Literal(' '.join(row['postTitle']))))\n",
    "    g.add((post_uri, DCMI.description, Literal(' '.join(row['postDesc']))))\n",
    "    g.add((post_uri, DCMI.date, Literal(row['postTime'])))\n",
    "    g.add((post_uri, SIOC.num_replies, Literal(row['noOfUpvotes'])))\n",
    "    g.add((post_uri, SIOC.link, URIRef(row['postUrl'])))\n",
    "    g.add((post_uri, SIOC.NSFW, Literal(row['isNSFW'])))\n",
    "    g.add((post_uri, SIOC.has_type, post_type_uri))\n",
    "    g.add((post_uri, SIOC.topic, topic_uri))\n",
    "\n",
    "    # Create the relationships based on the list of relationship types\n",
    "    # CreatedBy: Indicates that a post is created by an author.\n",
    "    g.add((post_uri, REDDIT.CreatedBy, author_uri))\n",
    "\n",
    "    # BelongsTo: Links a post to a subreddit.\n",
    "    g.add((post_uri, REDDIT.BelongsTo, subreddit_uri))\n",
    "\n",
    "    # PartOfPost: Indicates that a comment is part of a post (the comment belongs to the post).\n",
    "    comment_uri = URIRef(f\"http://reddit.com/comment/{index}\")\n",
    "    g.add((comment_uri, REDDIT.PartOfPost, post_uri))\n",
    "\n",
    "    # HasType: Links an entity (post, comment) to its specific type (text post, link post, etc.).\n",
    "    g.add((post_uri, REDDIT.HasType, post_type_uri))\n",
    "    g.add((comment_uri, REDDIT.HasType, comment_type_uri))\n",
    "\n",
    "    # TaggedIn: Indicates that a post or comment is tagged with a specific topic or keyword.\n",
    "    for tag in row.get('tags', []):  # Assuming tags are available as a list\n",
    "        tag_uri = URIRef(f\"http://reddit.com/tag/{tag}\")\n",
    "        g.add((post_uri, REDDIT.TaggedIn, tag_uri))\n",
    "        g.add((comment_uri, REDDIT.TaggedIn, tag_uri))\n",
    "\n",
    "    # UpvotedBy: Indicates that a user has upvoted a post or comment.\n",
    "    if row.get('upvotedBy'):\n",
    "        for upvoter in row['upvotedBy']:\n",
    "            upvoter_uri = URIRef(f\"http://reddit.com/user/{upvoter}\")\n",
    "            g.add((post_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "\n",
    "    # DownvotedBy: Indicates that a user has downvoted a post or comment.\n",
    "    if row.get('downvotedBy'):\n",
    "        for downvoter in row['downvotedBy']:\n",
    "            downvoter_uri = URIRef(f\"http://reddit.com/user/{downvoter}\")\n",
    "            g.add((post_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "\n",
    "    # Moderates: Indicates that a user is a moderator of a subreddit.\n",
    "    if row.get('moderators'):\n",
    "        for moderator in row['moderators']:\n",
    "            moderator_uri = URIRef(f\"http://reddit.com/user/{moderator}\")\n",
    "            g.add((subreddit_uri, REDDIT.Moderates, moderator_uri))\n",
    "\n",
    "    # HasFlair: Links a post or comment to a specific flair (e.g., tags like \"Important\", \"Question\", etc.).\n",
    "    if row.get('flair'):\n",
    "        flair_uri = URIRef(f\"http://reddit.com/flair/{row['flair']}\")\n",
    "        g.add((post_uri, REDDIT.HasFlair, flair_uri))\n",
    "        g.add((comment_uri, REDDIT.HasFlair, flair_uri))\n",
    "\n",
    "    # Link post to subreddit and type\n",
    "    g.add((subreddit_uri, rdflib.RDF.type, SIOC.Container))\n",
    "    g.add((subreddit_uri, SIOC.has_post, post_uri))\n",
    "    g.add((subreddit_uri, SIOC.has_type, subreddit_type_uri))\n",
    "    g.add((post_uri, SIOC.Container, subreddit_uri))\n",
    "\n",
    "    # Link to author and author type\n",
    "    g.add((author_uri, rdflib.RDF.type, FOAF.Person))\n",
    "    g.add((author_uri, FOAF.name, Literal(row['authorName'])))\n",
    "    g.add((post_uri, SIOC.has_creator, author_uri))\n",
    "    g.add((author_uri, SIOC.has_type, author_type_uri))\n",
    "\n",
    "    # Add comments and link to comment type\n",
    "    g.add((comment_uri, rdflib.RDF.type, SIOC.Comment))\n",
    "    g.add((comment_uri, DCMI.title, Literal(row['comments'])))\n",
    "    g.add((post_uri, SIOC.has_reply, comment_uri))\n",
    "    g.add((comment_uri, SIOC.has_type, comment_type_uri))\n",
    "\n",
    "# Process each subreddit and create LDA models\n",
    "GroupedData = DF.groupby('subReddit')\n",
    "all_topics = defaultdict(set)\n",
    "\n",
    "for subreddit, group in GroupedData:\n",
    "    group['Combined'] = group['postTitle'] + group['postDesc']\n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in custom_stopwords])\n",
    "\n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "    LDA = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, iterations=100, random_state=42)\n",
    "    \n",
    "    for idx, topic in LDA.print_topics(num_words=5):\n",
    "        topic_uri = URIRef(f\"http://reddit.com/topic/{subreddit}_{idx}\")\n",
    "        topic_words = topic.split(\" + \")\n",
    "        unique_words = {word.split(\"*\")[1].strip('\"') for word in topic_words}\n",
    "        all_topics[subreddit].update(unique_words)\n",
    "\n",
    "        # Define URIs for types\n",
    "        subreddit_type_uri = URIRef(f\"http://reddit.com/type/subreddit/{subreddit}\")\n",
    "        post_type_uri = URIRef(f\"http://reddit.com/type/post/{subreddit}\")\n",
    "        comment_type_uri = URIRef(f\"http://reddit.com/type/comment/{subreddit}\")\n",
    "        author_type_uri = URIRef(f\"http://reddit.com/type/author/{subreddit}\")\n",
    "\n",
    "        # Link subreddit to topic\n",
    "        subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{subreddit}\")\n",
    "        g.add((subreddit_uri, SIOC.has_topic, topic_uri))\n",
    "        g.add((topic_uri, rdflib.RDF.type, SIOC.Topic))\n",
    "\n",
    "        # Iterate through each row in the group\n",
    "        for index, row in group.iterrows():\n",
    "            add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri)\n",
    "\n",
    "# Save graph\n",
    "g.serialize('D:/FYP/Github/data-tails/Backend/Ontologies/KG.ttl', format='turtle')\n",
    "json_ld = g.serialize(format='json-ld', indent=4)\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/KG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting *KG.json, KG.ttl* to format of D3 input file to view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import rdflib\n",
    "# from rdflib.namespace import RDF, SIOC, REDDIT\n",
    "\n",
    "# Initialize D3-compatible JSON structure\n",
    "d3_data = {\n",
    "    \"nodes\": [],\n",
    "    \"links\": []\n",
    "}\n",
    "\n",
    "def parse_label(url):\n",
    "    if \"subreddit\" in url:\n",
    "        # Extract subreddit name after /subreddit/\n",
    "        name = url.split(\"/subreddit/\")[1]\n",
    "        return f\"Subreddit({name})\"\n",
    "    elif \"topic\" in url:\n",
    "        # Extract topic name after /topic/\n",
    "        name = url.split(\"/topic/\")[1]\n",
    "        return f\"Topic({name})\"\n",
    "    elif \"post\" in url:\n",
    "        # Extract post ID after /post\n",
    "        name = url.split(\"/reddit.com/\")[1]\n",
    "        return f\"Post({name})\"\n",
    "    elif \"user\" in url:\n",
    "        # Extract username after /user/\n",
    "        name = url.split(\"/user/\")[1]\n",
    "        return f\"Author({name})\"\n",
    "    elif \"comment\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/comment/\")[1]\n",
    "        return f\"Comment({name})\"\n",
    "    elif \"tag\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/tag/\")[1]\n",
    "        return f\"Tag({name})\"\n",
    "    elif \"upvoter\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/upvoter/\")[1]\n",
    "        return f\"UpVote({name})\"\n",
    "    elif \"downvoter\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/downvoter/\")[1]\n",
    "        return f\"DownVote({name})\"\n",
    "    return url  # In case no specific type is found, return the URL itself\n",
    "\n",
    "# Create a mapping for nodes to avoid duplicates\n",
    "node_map = {}\n",
    "\n",
    "# Function to add a node if it doesn't already exist\n",
    "def add_node(label, node_type, group, description=\"\"):\n",
    "    if label not in node_map:\n",
    "        node_id = f\"{node_type}_{len(d3_data['nodes']) + 1}\"  # Create a unique node ID\n",
    "        node_map[label] = node_id\n",
    "        d3_data[\"nodes\"].append({\n",
    "            \"id\": node_id,\n",
    "            \"label\": label,\n",
    "            \"type\": node_type,\n",
    "            \"group\": group  # Group for categorization\n",
    "        })\n",
    "    return node_map[label]\n",
    "\n",
    "# Populate nodes and links based on the RDF graph structure\n",
    "for subreddit in g.subjects(rdflib.RDF.type, SIOC.Container):\n",
    "    subreddit_label = str(subreddit)\n",
    "    subreddit_label= parse_label(subreddit_label)\n",
    "    subreddit_id = add_node(subreddit_label, \"Subreddit\", group=0, description=\"Community for specific topics\")\n",
    "\n",
    "    # For each post in the subreddit\n",
    "    for post in g.objects(subreddit, SIOC.has_post):\n",
    "        post_label = str(post)\n",
    "        post_label= parse_label(post_label)\n",
    "        post_id = add_node(post_label, \"Post\", group=1, description=\"Individual posts in the subreddit\")\n",
    "\n",
    "        # Create a link from subreddit to post\n",
    "        d3_data[\"links\"].append({\n",
    "            \"source\": subreddit_id,\n",
    "            \"target\": post_id,\n",
    "            \"type\": \"Contains\",\n",
    "            \"weight\": 1  # Link weight can be adjusted based on relevance or count\n",
    "        })\n",
    "\n",
    "        # Add authors\n",
    "        author = g.value(post, SIOC.has_creator)\n",
    "        if author:\n",
    "            author_label = str(author)\n",
    "            author_label= parse_label(author_label)\n",
    "            author_id = add_node(author_label, \"Author\", group=2, description=\"User who created the post\")\n",
    "\n",
    "            # Create a link from post to author\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": author_id,\n",
    "                \"type\": \"CreatedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        # For each comment on the post\n",
    "        for comment in g.objects(post, SIOC.has_reply):\n",
    "            comment_label = str(comment)\n",
    "            comment_label= parse_label(comment_label)\n",
    "            comment_id = add_node(comment_label, \"Comment\", group=3, description=\"User comments on the post\")\n",
    "\n",
    "            # Create a link from post to comment\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": comment_id,\n",
    "                \"type\": \"HasReply\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "            # Link comment to author if available\n",
    "            if author:\n",
    "                d3_data[\"links\"].append({\n",
    "                    \"source\": author_id,\n",
    "                    \"target\": comment_id,\n",
    "                    \"type\": \"CommentedBy\",\n",
    "                    \"weight\": 1\n",
    "                })\n",
    "\n",
    "            # Add tags to comment if available\n",
    "            for tag in g.objects(post, REDDIT.TaggedIn):\n",
    "                tag_label = str(tag)\n",
    "                tag_label= parse_label(tag_label)\n",
    "                tag_id = add_node(tag_label, \"Tag\", group=4, description=\"Tags related to the post or comment\")\n",
    "                d3_data[\"links\"].append({\n",
    "                    \"source\": comment_id,\n",
    "                    \"target\": tag_id,\n",
    "                    \"type\": \"TaggedWith\",\n",
    "                    \"weight\": 1\n",
    "                })\n",
    "\n",
    "        # Add topics to post\n",
    "        for topic in g.objects(post, SIOC.topic):\n",
    "            topic_label = str(topic)\n",
    "            topic_label= parse_label(topic_label)\n",
    "            topic_id = add_node(topic_label, \"Topic\", group=5, description=\"Topic related to the post\")\n",
    "\n",
    "            # Create a link from post to topic\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": topic_id,\n",
    "                \"type\": \"RelatedTo\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        # Add votes (upvotes and downvotes)\n",
    "        for upvoter in g.objects(post, REDDIT.UpvotedBy):\n",
    "            upvoter_label = str(upvoter)\n",
    "            upvoter_label= parse_label(upvoter_label)\n",
    "            upvoter_id = add_node(upvoter_label, \"Upvoter\", group=6, description=\"User who upvoted the post\")\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": upvoter_id,\n",
    "                \"type\": \"UpvotedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        for downvoter in g.objects(post, REDDIT.DownvotedBy):\n",
    "            downvoter_label = str(downvoter)\n",
    "            downvoter_label= parse_label(downvoter_label)\n",
    "            downvoter_id = add_node(downvoter_label, \"Downvoter\", group=7, description=\"User who downvoted the post\")\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": downvoter_id,\n",
    "                \"type\": \"DownvotedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "# Write the D3-compatible JSON data to a file\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/D3KG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(d3_data, f, indent=4, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import rdflib\n",
    "from collections import deque\n",
    "\n",
    "# Initialize D3-compatible JSON structure\n",
    "d3_data = {\n",
    "    \"nodes\": [],\n",
    "    \"links\": []\n",
    "}\n",
    "\n",
    "# Create a mapping for nodes to avoid duplicates\n",
    "node_map = {}\n",
    "\n",
    "# Function to add a node if it doesn't already exist\n",
    "def add_node(label, node_type, group, description):\n",
    "    if label not in node_map:\n",
    "        node_id = f\"{node_type}_{len(d3_data['nodes']) + 1}\"  # Create a unique node ID\n",
    "        node_map[label] = node_id\n",
    "        d3_data[\"nodes\"].append({\n",
    "            \"id\": node_id,\n",
    "            \"label\": label,\n",
    "            \"type\": node_type,\n",
    "            \"group\": group,\n",
    "            \"description\": description\n",
    "        })\n",
    "    return node_map[label]\n",
    "\n",
    "# Function to perform BFS and retrieve a subgraph\n",
    "def bfs(start_node_id, max_depth):\n",
    "    visited = set()\n",
    "    queue = deque([(start_node_id, 0)])  # (node_id, depth)\n",
    "    nodes_to_display = set([start_node_id])\n",
    "    links_to_display = []\n",
    "\n",
    "    # Mark the starting node to highlight it\n",
    "    start_node_found = False\n",
    "\n",
    "    while queue:\n",
    "        current_node_id, current_depth = queue.popleft()\n",
    "        if current_depth < max_depth:\n",
    "            # Add links of current node to the links_to_display\n",
    "            for link in d3_data[\"links\"]:\n",
    "                if link[\"source\"] == current_node_id and link[\"target\"] not in visited:\n",
    "                    links_to_display.append(link)\n",
    "                    queue.append((link[\"target\"], current_depth + 1))\n",
    "                elif link[\"target\"] == current_node_id and link[\"source\"] not in visited:\n",
    "                    links_to_display.append(link)\n",
    "                    queue.append((link[\"source\"], current_depth + 1))\n",
    "\n",
    "        visited.add(current_node_id)\n",
    "\n",
    "        # Highlight the start node\n",
    "        if current_node_id == start_node_id and not start_node_found:\n",
    "            start_node_found = True\n",
    "            for node in d3_data[\"nodes\"]:\n",
    "                if node[\"id\"] == start_node_id:\n",
    "                    node[\"highlighted\"] = \"Head\"\n",
    "\n",
    "    # Filter the nodes that are part of the display\n",
    "    for link in links_to_display:\n",
    "        nodes_to_display.add(link[\"source\"])\n",
    "        nodes_to_display.add(link[\"target\"])\n",
    "\n",
    "    # Filter out the nodes and links that should be displayed\n",
    "    filtered_nodes = [node for node in d3_data[\"nodes\"] if node[\"id\"] in nodes_to_display]\n",
    "    filtered_links = [link for link in links_to_display]\n",
    "\n",
    "    return filtered_nodes, filtered_links\n",
    "\n",
    "# Load the RDF data from the D3KG.json file\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/D3KG.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    d3_data = json.load(f)\n",
    "\n",
    "# Example of how to call BFS with a starting node and max depth\n",
    "start_node = \"Comment_4\"\n",
    "max_depth = 2\n",
    "filtered_nodes, filtered_links = bfs(start_node, max_depth)\n",
    "\n",
    "# Write the filtered subgraph to a file\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/BFS.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"nodes\": filtered_nodes, \"links\": filtered_links}, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Loading Knowledge Graphs...\n",
      "✅ Loaded KG.json with 63763 entities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000022975AB81F0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\isodate\\isodatetime.py\", line 55, in parse_datetime\n",
      "    tmpdate = parse_date(datestring)\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\isodate\\isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: 'Na'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded KG.ttl with 648909 triples.\n",
      "\n",
      "🔍 Retrieving Relevant Comments...\n",
      "\n",
      "🤖 Querying Groq...\n",
      "\n",
      "💡 Groq Response: I'm glad you're seeking help and information about overcoming OCD! While it's not possible to completely \"get rid\" of OCD, it is treatable, and many people with OCD are able to live fulfilling lives with the right treatment and support. Here are some steps you can take to overcome OCD:\n",
      "\n",
      "1. **Seek professional help**: Work with a mental health professional, such as a psychologist or psychiatrist, who has experience in treating OCD. They can help you develop a personalized treatment plan, which may include:\n",
      "\t* Cognitive-behavioral therapy (CBT): A type of therapy that focuses on changing negative thought patterns and behaviors.\n",
      "\t* Exposure and response prevention (ERP): A technique that involves gradually exposing yourself to situations or objects that trigger anxiety, while resisting the urge to perform compulsive behaviors.\n",
      "\t* Medication: Antidepressants, such as selective serotonin reuptake inhibitors (SSRIs), can help reduce OCD symptoms.\n",
      "2. **Learn about OCD**: Educate yourself about OCD, including its signs, symptoms, and treatment options. Understanding your condition can help you feel more empowered and motivated to seek help.\n",
      "3. **Practice self-care**: Engage in activities that help you relax and reduce stress, such as:\n",
      "\t* Exercise: Regular physical activity can help reduce anxiety and improve mood.\n",
      "\t* Meditation and mindfulness: Practices that focus on being present and non-judgmental can help you cope with anxiety and OCD symptoms.\n",
      "\t* Journaling: Writing down your thoughts and feelings can help you process and release emotions.\n",
      "4. **Challenge negative thoughts**: OCD often involves intrusive thoughts and negative self-talk. Try to recognize and challenge these thoughts by:\n",
      "\t* Identifying and reframing negative thoughts\n",
      "\t* Practicing self-compassion and self-kindness\n",
      "\t* Focusing on the present moment, rather than catastrophizing or worrying about the future\n",
      "5. **Build a support network**: Surround yourself with people who are supportive and understanding. Sharing your experiences with others who have OCD can also help you feel less isolated and more motivated to seek help.\n",
      "6. **Join a support group**: Look for online or in-person support groups, such as the OCD Foundation or the International OCD Foundation. These groups can provide a safe and supportive environment to share your experiences and connect with others who are going through similar challenges.\n",
      "7. **Practice self-compassion**: Be kind and understanding with yourself as you work through the process of overcoming OCD. Remember that recovery is a journey, and setbacks are a normal part of the process.\n",
      "8. **Seek support for loved ones**: OCD can affect not just the individual with the condition, but also their loved ones. Encourage family and friends to seek support and education to better understand and support you.\n",
      "9. **Stay patient and persistent**: Recovery from OCD takes time and effort. Be patient with yourself and stay committed to your treatment plan, even when it feels challenging or frustrating.\n",
      "10. **Consider alternative approaches**: Some people find that alternative approaches, such as cognitive behavioral therapy for adults (CBT-A), dialectical behavior therapy (DBT), or acceptance and commitment therapy (ACT), can be helpful in managing OCD symptoms.\n",
      "\n",
      "Remember, overcoming OCD is a process, and it's okay to take it one step at a time. With the right treatment, support, and mindset, you can learn to manage your OCD symptoms and live a fulfilling life.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import rdflib\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "from collections import deque, defaultdict\n",
    "from rdflib import Graph, Namespace, Literal, URIRef\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from groq import Groq\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ✅ Initialize Groq Client\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# ✅ CSV File for Conversation History\n",
    "csv_file_path = \"conversation_history.csv\"\n",
    "conversation_history = []\n",
    "\n",
    "# ✅ Define RDF Namespaces\n",
    "SIOC = Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "REDDIT = Namespace(\"http://reddit.com/ns#\")\n",
    "\n",
    "# ✅ NLP Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Extracts meaningful words from text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    tokens = word_tokenize(text.lower())  \n",
    "    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "# ✅ Load KG.json for Fast Lookups\n",
    "def load_kg_json(file_path):\n",
    "    \"\"\"Loads KG.json and converts it into a dictionary for fast lookup.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"✅ Loaded KG.json with {len(data)} entities.\")\n",
    "        return {entity[\"@id\"]: entity for entity in data}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading KG.json: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ✅ Load KG.ttl & Build Adjacency List\n",
    "def load_kg_ttl(file_path):\n",
    "    \"\"\"Loads KG.ttl and builds an adjacency list for fast graph traversal.\"\"\"\n",
    "    try:\n",
    "        g = Graph()\n",
    "        g.parse(file_path, format=\"turtle\")\n",
    "        adjacency_list = defaultdict(list)\n",
    "\n",
    "        for s, p, o in g:\n",
    "            adjacency_list[str(s)].append((s, p, o))\n",
    "            adjacency_list[str(o)].append((s, p, o))  \n",
    "        \n",
    "        print(f\"✅ Loaded KG.ttl with {len(g)} triples.\")\n",
    "        return g, adjacency_list\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading KG.ttl: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ✅ **Optimized BFS Retrieval with Subreddit & Topic Filtering**\n",
    "def retrieve_relevant_comments(kg_json, adjacency_list, subreddit, topic):\n",
    "    \"\"\"Retrieves only comments relevant to the given subreddit and topic.\"\"\"\n",
    "    if not kg_json:\n",
    "        return \"❌ KG.json not loaded.\"\n",
    "\n",
    "    subreddit_uri = f\"http://reddit.com/subreddit/{subreddit}\"\n",
    "    topic_uri = f\"http://reddit.com/topic/{topic}\"\n",
    "\n",
    "    matched_comments = set()\n",
    "\n",
    "    # **Step 1: Find Posts Related to Subreddit & Topic**\n",
    "    relevant_posts = set()\n",
    "    for entity_id, entity in kg_json.items():\n",
    "        if \"sioc:Container\" in entity and entity[\"sioc:Container\"] == subreddit_uri:\n",
    "            # Check if topic is mentioned\n",
    "            if \"sioc:topic\" in entity and topic_uri in entity[\"sioc:topic\"]:\n",
    "                relevant_posts.add(entity_id)\n",
    "\n",
    "    if not relevant_posts:\n",
    "        return \"❌ No posts found for the given subreddit & topic.\"\n",
    "\n",
    "    # **Step 2: Retrieve Only Comments from Relevant Posts**\n",
    "    for post_uri in relevant_posts:\n",
    "        for comment_uri, p, o in adjacency_list.get(post_uri, []):\n",
    "            if \"sioc:Comment\" in str(o):  \n",
    "                matched_comments.add(comment_uri)\n",
    "\n",
    "    if not matched_comments:\n",
    "        return \"❌ No relevant comments found.\"\n",
    "\n",
    "    # **Step 3: Retrieve Context of Matched Comments**\n",
    "    context_results = []\n",
    "    for comment in matched_comments:\n",
    "        comment_text = kg_json.get(comment, {}).get(\"dc:title\", \"\")\n",
    "        if comment_text:\n",
    "            context_results.append(comment_text)\n",
    "\n",
    "    return {\"context\": context_results[:10]} if context_results else \"❌ Data not found.\"\n",
    "\n",
    "# ✅ Groq Chat API\n",
    "def chat_with_groq(context, user_query):\n",
    "    \"\"\"Interacts with Groq model using retrieved KG context.\"\"\"\n",
    "    global conversation_history\n",
    "\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {user_query}\n",
    "\n",
    "    Provide a detailed answer based on the context.\n",
    "    \"\"\"\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=conversation_history,  \n",
    "        model=\"llama3-8b-8192\"\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Role\", \"Content\"])\n",
    "        for entry in conversation_history:\n",
    "            writer.writerow([entry[\"role\"], entry[\"content\"]])\n",
    "\n",
    "    return response\n",
    "\n",
    "# ✅ **Run Main Program**\n",
    "if __name__ == \"__main__\":\n",
    "    kg_json_path = \"./KG.json\"\n",
    "    kg_ttl_path = \"./KG.ttl\"\n",
    "\n",
    "    print(\"\\n🔍 Loading Knowledge Graphs...\")\n",
    "    kg_json = load_kg_json(kg_json_path)\n",
    "    kg_ttl, adjacency_list = load_kg_ttl(kg_ttl_path)\n",
    "\n",
    "    subreddit = \"Advice\"\n",
    "    topic = \"Human Health\"\n",
    "    user_query = \"How can I get rid of OCD?\"\n",
    "\n",
    "    print(\"\\n🔍 Retrieving Relevant Comments...\")\n",
    "    context = retrieve_relevant_comments(kg_json, adjacency_list, subreddit, topic)\n",
    "\n",
    "    print(\"\\n🤖 Querying Groq...\")\n",
    "    response = chat_with_groq(context, user_query)\n",
    "\n",
    "    print(\"\\n💡 Groq Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Top 3 Recommended Visualizations: [('line_chart', 1.39), ('area_chart', 0.78), ('bar_chart', 0.76)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load NLP models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define visualization categories conceptually\n",
    "chart_types = {\n",
    "    \"bar_chart\": \"Used for comparing categories or ranking values.\",\n",
    "    \"line_chart\": \"Best for showing trends over time or sequential data.\",\n",
    "    \"area_chart\": \"Shows cumulative data trends with a filled area.\",\n",
    "    \"scatterplot\": \"Visualizes relationships or correlations between two variables.\",\n",
    "    \"density_facet\": \"Represents distribution patterns across multiple facets.\",\n",
    "    \"gradient_encoding\": \"Indicates intensity or magnitude with color gradients.\",\n",
    "    \"candlestick_chart\": \"Commonly used for stock market trends and price movements.\",\n",
    "    \"stacked_normalized_area_chart\": \"Shows part-to-whole relationships over time.\",\n",
    "    \"circle_packing\": \"Represents hierarchical relationships in a compact form.\",\n",
    "    \"dendrogram\": \"Illustrates hierarchical clustering or tree structures.\",\n",
    "    \"DAG\": \"Shows directed relationships, commonly used for processes or networks.\",\n",
    "    \"treemap\": \"Depicts hierarchical structures using nested rectangles.\",\n",
    "    \"chord_diagram\": \"Best for visualizing relationships and interactions.\",\n",
    "    \"heatmap\": \"Displays intensity values in a matrix format.\",\n",
    "    \"connection_map\": \"Visualizes spatial relationships and geographic data.\",\n",
    "    \"maps\": \"Represents geographic data with location-based insights.\",\n",
    "    \"map_small_multiples\": \"Shows multiple maps for comparison.\",\n",
    "    \"hexbin_map\": \"Aggregates spatial data into hexagonal bins.\",\n",
    "    \"centerline_labelling\": \"Enhances map readability with labeled centerlines.\",\n",
    "    \"voronoi_map\": \"Divides spatial regions based on distance.\",\n",
    "    \"sorted_heatmap\": \"Ranks and sorts data in a matrix format.\"\n",
    "}\n",
    "\n",
    "# Precompute chart category embeddings\n",
    "category_embeddings = {\n",
    "    chart: model.encode(description, normalize_embeddings=True)\n",
    "    for chart, description in chart_types.items()\n",
    "}\n",
    "\n",
    "# Function to extract features from both the query and response\n",
    "def extract_features(query, response):\n",
    "    \"\"\"Extracts numerical data, locations, trends, and relationships from text.\"\"\"\n",
    "    combined_text = query + \" \" + response\n",
    "    doc = nlp(combined_text)\n",
    "\n",
    "    numbers = [token.text for token in doc if token.like_num]\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\"}]\n",
    "\n",
    "    trend_keywords = {\"increase\", \"decline\", \"growth\", \"rise\", \"drop\", \"trend\", \"fall\", \"reduce\", \"expansion\"}\n",
    "    trends = [token.lemma_ for token in doc if token.lemma_ in trend_keywords]\n",
    "\n",
    "    relationship_keywords = {\"prefer\", \"dominate\", \"compared\", \"versus\", \"majority\", \"minority\"}\n",
    "    relationships = [token.lemma_ for token in doc if token.lemma_ in relationship_keywords]\n",
    "\n",
    "    ranking_keywords = {\"ranking\", \"top\", \"best\", \"percent\", \"favorite\", \"most popular\"}\n",
    "    rankings = [token.lemma_ for token in doc if token.lemma_ in ranking_keywords]\n",
    "\n",
    "    geo_terms = {\"earthquake\", \"magnitude\", \"epicenter\", \"seismic\", \"hurricane\", \"flood\", \"storm\"}\n",
    "    geo_features = [token.lemma_ for token in doc if token.lemma_ in geo_terms]\n",
    "\n",
    "    keywords = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    return {\n",
    "        \"numbers\": numbers,\n",
    "        \"locations\": locations,\n",
    "        \"trends\": trends,\n",
    "        \"relationships\": relationships,\n",
    "        \"geo_features\": geo_features,\n",
    "        \"rankings\": rankings,\n",
    "        \"keywords\": keywords\n",
    "    }\n",
    "\n",
    "# Adjust similarity scores based on extracted features\n",
    "def boost_scores(scores, response_type, features):\n",
    "    \"\"\"Dynamically boosts scores based on response type and detected keywords.\"\"\"\n",
    "    boosts = {\n",
    "        \"numeric\": {\"bar_chart\": 0.6, \"line_chart\": 0.5, \"heatmap\": 0.4},\n",
    "        \"geographic\": {\"maps\": 0.7, \"connection_map\": 0.6, \"hexbin_map\": 0.5, \"voronoi_map\": 0.5},\n",
    "        \"textual\": {\"scatterplot\": 0.2, \"DAG\": 0.2, \"chord_diagram\": 0.2}\n",
    "    }\n",
    "\n",
    "    if response_type in boosts:\n",
    "        for chart, boost in boosts[response_type].items():\n",
    "            scores[chart] += boost\n",
    "\n",
    "    if features[\"trends\"]:\n",
    "        scores[\"line_chart\"] += 0.6\n",
    "        scores[\"area_chart\"] += 0.5\n",
    "\n",
    "    if features[\"relationships\"]:\n",
    "        scores[\"bar_chart\"] += 0.6\n",
    "        scores[\"chord_diagram\"] += 0.5\n",
    "\n",
    "    if features[\"geo_features\"]:\n",
    "        scores[\"maps\"] += 0.7\n",
    "        scores[\"connection_map\"] += 0.6\n",
    "        scores[\"hexbin_map\"] += 0.5\n",
    "\n",
    "    if features[\"rankings\"]:\n",
    "        scores[\"bar_chart\"] += 0.7\n",
    "        scores[\"sorted_heatmap\"] += 0.5\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Function to recommend visualizations\n",
    "def recommend_visualizations(query, response):\n",
    "    \"\"\"Recommends the most relevant visualizations based on query and response context.\"\"\"\n",
    "    features = extract_features(query, response)\n",
    "    response_embedding = model.encode(query + \" \" + response, normalize_embeddings=True)\n",
    "\n",
    "    similarity_scores = {\n",
    "        chart: cosine_similarity([response_embedding], [embedding]).flatten()[0]\n",
    "        for chart, embedding in category_embeddings.items()\n",
    "    }\n",
    "\n",
    "    similarity_scores = boost_scores(similarity_scores, \"numeric\", features)\n",
    "\n",
    "    top_recommendations = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    return [(chart, round(score, 2)) for chart, score in top_recommendations]\n",
    "# Example Usage\n",
    "user_query = \"Show iphone vs Samsung smartphone market share\"\n",
    "groq_response = \"A crucial topic in the tech world! Let's create a visualization of the iPhone vs Samsung smartphone market share. Market Share Trends:  2011: iPhone held around 25% of the global smartphone market share, while Samsung had around 10%.  2014: iPhone's market share peaked at around 46%, while Samsung's share reached 31%.  2018: Samsung surpassed Apple, with a market share of around 21.5%, compared to iPhone's 17.9%.  2020: The competition remained intense, with Samsung holding 22.1% of the market share, and iPhone holding 17.4%. Visual Representation: Here's a simplified visualization of the iPhone vs Samsung smartphone market share: mermaid graph LR subgraph 'Global Smartphone Market Share' iPhone -->|2011| 25% Samsung -->|2011| 10% iPhone -->|2014| 46% Samsung -->|2014| 31% iPhone -->|2018| 17.9% Samsung -->|2018| 21.5% iPhone -->|2020| 17.4% Samsung -->|2020| 22.1% end  This diagram shows the changing market share of iPhone and Samsung over the years, with arrows indicating the direction of change. Note that this is a simplified representation and is not meant to be a comprehensive or scientifically accurate diagram. Some interesting statistics:  As of 2020, the global smartphone market size was estimated to be around 1.5 billion devices.  Samsung's share of the global smartphone market has been steadily increasing since 2011, with a growth rate of around 2% per annum.  Apple's share of the global smartphone market has been relatively flat since 2018, with a growth rate of around 0.2% per annum.\"\n",
    "\n",
    "recommended_charts = recommend_visualizations(user_query, groq_response)\n",
    "\n",
    "print(\"\\n✅ Top 3 Recommended Visualizations:\", recommended_charts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
