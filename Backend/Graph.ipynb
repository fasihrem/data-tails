{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontologies to use\n",
    "- **Post-Subreddit:** A post belongs to a specific subreddit.\n",
    "- **Post-Author:** A post is created by an author.\n",
    "- **Post-Topics:** A post is related to one or more topics based on the topic modeling results.\n",
    "- **Post-Comments:** A post has a set of comments.\n",
    "- **Post-Keywords:** A post is associated with specific keywords derived from the topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvis in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (8.9.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.3.0)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "!pip install pyvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import rdflib\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from gensim import corpora\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from *MongoDB*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to MongoDB\n",
    "- Read the data from the collection\n",
    "- Convert the data into a pandas DataFrame\n",
    "- Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  type  \\\n",
      "0  66e965e698330736e0d693d5  None   \n",
      "1  66e965e698330736e0d693d6  None   \n",
      "2  66e965e698330736e0d693d7  None   \n",
      "3  66e965e698330736e0d693d8  None   \n",
      "4  66e965e698330736e0d693d9  None   \n",
      "\n",
      "                                           postTitle postDesc  \\\n",
      "0  Adults(especially those over 30), how young do...      NaN   \n",
      "1  What is a thing that your parents consider nor...      NaN   \n",
      "2                 What is a smell that comforts you?      NaN   \n",
      "3  When in history was it the best time to be a w...      NaN   \n",
      "4    What's the worst way someone broke up with you?      NaN   \n",
      "\n",
      "              postTime            authorName noOfUpvotes isNSFW  \\\n",
      "0  2024-08-06 01:02:35  Excellent-Studio7257        4068  False   \n",
      "1  2024-08-06 01:47:22        Bigbumoffhappy        2073  False   \n",
      "2  2024-08-05 22:21:53         bloomsmittenn        2188  False   \n",
      "3  2024-08-06 03:32:59   More_food_please_77         778  False   \n",
      "4  2024-08-05 21:01:39    ImpressiveWrap7363        1989  False   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  [It's weird. I can have an adult/intellectual ...        5590.0   \n",
      "1  [Thinking you're supposed to raise your kids t...        1684.0   \n",
      "2  [The weather in the woods after a heavy rain, ...        4958.0   \n",
      "3  [It might depend on where you are but now with...        1046.0   \n",
      "4  [I have 2 bad ones.  First one, he had his mom...        1749.0   \n",
      "\n",
      "                                            imageUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                             postUrl  subReddit  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['DataTails']\n",
    "collection = db['Data']  \n",
    "data_cursor = collection.find({})\n",
    "DF = pd.DataFrame(list(data_cursor))\n",
    "print(DF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stopword Removal & Lemmatization:** \n",
    "    - Preprocessing() uses NLTK to remove stopwords and lemmatize the text.\n",
    "- **Handling Missing Data:**\n",
    "    - Missing postDesc fields are filled with an empty string.\n",
    "    - Missing noOfUpvotes is filled with 0.\n",
    "- **Datetime Conversion:** \n",
    "    - postTime is converted to a datetime object, and any errors are coerced.\n",
    "- **Handling Comments:** \n",
    "    - Comments are converted from list format to a string of concatenated comments.\n",
    "- **Final Clean Text:** \n",
    "    - Both postTitle and postDesc are cleaned using regular expressions and tokenization, and then passed through the NLTK-based text preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subReddit           2\n",
      "postTitle           0\n",
      "postDesc        15607\n",
      "postTime            0\n",
      "authorName        587\n",
      "noOfUpvotes         0\n",
      "comments            0\n",
      "noOfComments        2\n",
      "postUrl             2\n",
      "imageUrl            3\n",
      "isNSFW              1\n",
      "dtype: int64\n",
      "   subReddit                                          postTitle postDesc  \\\n",
      "0  AskReddit                   [adult, especially, young, seem]       []   \n",
      "1  AskReddit  [parent, consider, normal, consider, normal, a...       []   \n",
      "2  AskReddit                                   [smell, comfort]       []   \n",
      "3  AskReddit                       [history, best, time, woman]       []   \n",
      "4  AskReddit                       [worst, way, someone, broke]       []   \n",
      "\n",
      "             postTime            authorName noOfUpvotes  \\\n",
      "0 2024-08-06 01:02:35  Excellent-Studio7257        4068   \n",
      "1 2024-08-06 01:47:22        Bigbumoffhappy        2073   \n",
      "2 2024-08-05 22:21:53         bloomsmittenn        2188   \n",
      "3 2024-08-06 03:32:59   More_food_please_77         778   \n",
      "4 2024-08-05 21:01:39    ImpressiveWrap7363        1989   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  It's weird. I can have an adult/intellectual c...        5590.0   \n",
      "1  Thinking you're supposed to raise your kids th...        1684.0   \n",
      "2  The weather in the woods after a heavy rain Ne...        4958.0   \n",
      "3  It might depend on where you are but now with ...        1046.0   \n",
      "4  I have 2 bad ones.  First one, he had his mom ...        1749.0   \n",
      "\n",
      "                                             postUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                            imageUrl  isNSFW  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "subReddit       0\n",
      "postTitle       0\n",
      "postDesc        0\n",
      "postTime        1\n",
      "authorName      0\n",
      "noOfUpvotes     0\n",
      "comments        0\n",
      "noOfComments    0\n",
      "postUrl         0\n",
      "imageUrl        0\n",
      "isNSFW          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "StopWords = set(stopwords.words('english'))\n",
    "custom_stopwords = StopWords | {\"make\", \"thing\", \"know\", \"get\", \"want\", \"like\", \"would\", \"could\", \"you\", \"say\",\"also\",\"aita\",\"com\",\"www\",\"made\",\"ago\",\"day\",\"000\"}\n",
    "def Preprocessing(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in custom_stopwords and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "Cols = ['subReddit', 'postTitle', 'postDesc', 'postTime', 'authorName', 'noOfUpvotes', 'comments', 'noOfComments', 'postUrl','imageUrl','isNSFW']\n",
    "DF = DF[Cols]\n",
    "print(DF.isnull().sum())\n",
    "\n",
    "\n",
    "DF['subReddit'] = DF['subReddit'].fillna('Unknown_SubReddit')\n",
    "DF['authorName'] = DF['authorName'].fillna('Unknown_Author')\n",
    "DF['postTitle'] = DF['postTitle'].fillna('Untitled')\n",
    "DF['postUrl'] = DF['postUrl'].fillna('http://example.com/NOPOST.png')\n",
    "DF['imageUrl'] = DF['imageUrl'].fillna('http://example.com/NOImage.png')\n",
    "DF['isNSFW'] = DF['isNSFW'].fillna(False)\n",
    "DF['postDesc'].fillna('', inplace=True)\n",
    "DF['noOfUpvotes'].fillna(0, inplace=True)\n",
    "DF['noOfComments'].fillna(0, inplace=True)\n",
    "DF['postTime'] = pd.to_datetime(DF['postTime'], errors='coerce')\n",
    "DF['comments'] = DF['comments'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "DF['postTitle'] = DF['postTitle'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['postDesc'] = DF['postDesc'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['isNSFW'] = DF['isNSFW'].astype(bool)\n",
    "print(DF.head())\n",
    "print(DF.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import json\n",
    "\n",
    "# RDF graph initialization\n",
    "g = rdflib.Graph()\n",
    "SIOC = rdflib.Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = rdflib.Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = rdflib.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "REDDIT = rdflib.Namespace(\"http://reddit.com/ns#\")  # Custom namespace for Reddit-specific relationships\n",
    "g.bind(\"sioc\", SIOC)\n",
    "g.bind(\"dc\", DCMI)\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"reddit\", REDDIT)\n",
    "\n",
    "\n",
    "# Function to add post data to RDF graph\n",
    "def add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri):\n",
    "    post_uri = URIRef(f\"http://reddit.com/post{index}\")\n",
    "    subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{row['subReddit']}\")\n",
    "    author_uri = URIRef(f\"http://reddit.com/user/{row['authorName']}\")\n",
    "    \n",
    "    # Add post properties and link to topic\n",
    "    g.add((post_uri, rdflib.RDF.type, SIOC.Post))\n",
    "    g.add((post_uri, DCMI.title, Literal(' '.join(row['postTitle']))))\n",
    "    g.add((post_uri, DCMI.description, Literal(' '.join(row['postDesc']))))\n",
    "    g.add((post_uri, DCMI.date, Literal(row['postTime'])))\n",
    "    g.add((post_uri, SIOC.num_replies, Literal(row['noOfUpvotes'])))\n",
    "    g.add((post_uri, SIOC.link, URIRef(row['postUrl'])))\n",
    "    g.add((post_uri, SIOC.NSFW, Literal(row['isNSFW'])))\n",
    "    g.add((post_uri, SIOC.has_type, post_type_uri))\n",
    "    g.add((post_uri, SIOC.topic, topic_uri))\n",
    "\n",
    "    # Create the relationships based on the list of relationship types\n",
    "    # CreatedBy: Indicates that a post is created by an author.\n",
    "    g.add((post_uri, REDDIT.CreatedBy, author_uri))\n",
    "\n",
    "    # BelongsTo: Links a post to a subreddit.\n",
    "    g.add((post_uri, REDDIT.BelongsTo, subreddit_uri))\n",
    "\n",
    "    # PartOfPost: Indicates that a comment is part of a post (the comment belongs to the post).\n",
    "    comment_uri = URIRef(f\"http://reddit.com/comment/{index}\")\n",
    "    g.add((comment_uri, REDDIT.PartOfPost, post_uri))\n",
    "\n",
    "    # HasType: Links an entity (post, comment) to its specific type (text post, link post, etc.).\n",
    "    g.add((post_uri, REDDIT.HasType, post_type_uri))\n",
    "    g.add((comment_uri, REDDIT.HasType, comment_type_uri))\n",
    "\n",
    "    # TaggedIn: Indicates that a post or comment is tagged with a specific topic or keyword.\n",
    "    for tag in row.get('tags', []):  # Assuming tags are available as a list\n",
    "        tag_uri = URIRef(f\"http://reddit.com/tag/{tag}\")\n",
    "        g.add((post_uri, REDDIT.TaggedIn, tag_uri))\n",
    "        g.add((comment_uri, REDDIT.TaggedIn, tag_uri))\n",
    "\n",
    "    # UpvotedBy: Indicates that a user has upvoted a post or comment.\n",
    "    if row.get('upvotedBy'):\n",
    "        for upvoter in row['upvotedBy']:\n",
    "            upvoter_uri = URIRef(f\"http://reddit.com/user/{upvoter}\")\n",
    "            g.add((post_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "\n",
    "    # DownvotedBy: Indicates that a user has downvoted a post or comment.\n",
    "    if row.get('downvotedBy'):\n",
    "        for downvoter in row['downvotedBy']:\n",
    "            downvoter_uri = URIRef(f\"http://reddit.com/user/{downvoter}\")\n",
    "            g.add((post_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "\n",
    "    # Moderates: Indicates that a user is a moderator of a subreddit.\n",
    "    if row.get('moderators'):\n",
    "        for moderator in row['moderators']:\n",
    "            moderator_uri = URIRef(f\"http://reddit.com/user/{moderator}\")\n",
    "            g.add((subreddit_uri, REDDIT.Moderates, moderator_uri))\n",
    "\n",
    "    # HasFlair: Links a post or comment to a specific flair (e.g., tags like \"Important\", \"Question\", etc.).\n",
    "    if row.get('flair'):\n",
    "        flair_uri = URIRef(f\"http://reddit.com/flair/{row['flair']}\")\n",
    "        g.add((post_uri, REDDIT.HasFlair, flair_uri))\n",
    "        g.add((comment_uri, REDDIT.HasFlair, flair_uri))\n",
    "\n",
    "    # Link post to subreddit and type\n",
    "    g.add((subreddit_uri, rdflib.RDF.type, SIOC.Container))\n",
    "    g.add((subreddit_uri, SIOC.has_post, post_uri))\n",
    "    g.add((subreddit_uri, SIOC.has_type, subreddit_type_uri))\n",
    "    g.add((post_uri, SIOC.Container, subreddit_uri))\n",
    "\n",
    "    # Link to author and author type\n",
    "    g.add((author_uri, rdflib.RDF.type, FOAF.Person))\n",
    "    g.add((author_uri, FOAF.name, Literal(row['authorName'])))\n",
    "    g.add((post_uri, SIOC.has_creator, author_uri))\n",
    "    g.add((author_uri, SIOC.has_type, author_type_uri))\n",
    "\n",
    "    # Add comments and link to comment type\n",
    "    g.add((comment_uri, rdflib.RDF.type, SIOC.Comment))\n",
    "    g.add((comment_uri, DCMI.title, Literal(row['comments'])))\n",
    "    g.add((post_uri, SIOC.has_reply, comment_uri))\n",
    "    g.add((comment_uri, SIOC.has_type, comment_type_uri))\n",
    "\n",
    "# Process each subreddit and create LDA models\n",
    "GroupedData = DF.groupby('subReddit')\n",
    "all_topics = defaultdict(set)\n",
    "\n",
    "for subreddit, group in GroupedData:\n",
    "    group['Combined'] = group['postTitle'] + group['postDesc']\n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in custom_stopwords])\n",
    "\n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "    LDA = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, iterations=100, random_state=42)\n",
    "    \n",
    "    for idx, topic in LDA.print_topics(num_words=5):\n",
    "        topic_uri = URIRef(f\"http://reddit.com/topic/{subreddit}_{idx}\")\n",
    "        topic_words = topic.split(\" + \")\n",
    "        unique_words = {word.split(\"*\")[1].strip('\"') for word in topic_words}\n",
    "        all_topics[subreddit].update(unique_words)\n",
    "\n",
    "        # Define URIs for types\n",
    "        subreddit_type_uri = URIRef(f\"http://reddit.com/type/subreddit/{subreddit}\")\n",
    "        post_type_uri = URIRef(f\"http://reddit.com/type/post/{subreddit}\")\n",
    "        comment_type_uri = URIRef(f\"http://reddit.com/type/comment/{subreddit}\")\n",
    "        author_type_uri = URIRef(f\"http://reddit.com/type/author/{subreddit}\")\n",
    "\n",
    "        # Link subreddit to topic\n",
    "        subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{subreddit}\")\n",
    "        g.add((subreddit_uri, SIOC.has_topic, topic_uri))\n",
    "        g.add((topic_uri, rdflib.RDF.type, SIOC.Topic))\n",
    "\n",
    "        # Iterate through each row in the group\n",
    "        for index, row in group.iterrows():\n",
    "            add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri)\n",
    "\n",
    "# Save graph\n",
    "g.serialize('D:/FYP/Github/data-tails/Backend/Ontologies/KG.ttl', format='turtle')\n",
    "json_ld = g.serialize(format='json-ld', indent=4)\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/KG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting *KG.json, KG.ttl* to format of D3 input file to view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import rdflib\n",
    "# from rdflib.namespace import RDF, SIOC, REDDIT\n",
    "\n",
    "# Initialize D3-compatible JSON structure\n",
    "d3_data = {\n",
    "    \"nodes\": [],\n",
    "    \"links\": []\n",
    "}\n",
    "\n",
    "def parse_label(url):\n",
    "    if \"subreddit\" in url:\n",
    "        # Extract subreddit name after /subreddit/\n",
    "        name = url.split(\"/subreddit/\")[1]\n",
    "        return f\"Subreddit({name})\"\n",
    "    elif \"topic\" in url:\n",
    "        # Extract topic name after /topic/\n",
    "        name = url.split(\"/topic/\")[1]\n",
    "        return f\"Topic({name})\"\n",
    "    elif \"post\" in url:\n",
    "        # Extract post ID after /post\n",
    "        name = url.split(\"/reddit.com/\")[1]\n",
    "        return f\"Post({name})\"\n",
    "    elif \"user\" in url:\n",
    "        # Extract username after /user/\n",
    "        name = url.split(\"/user/\")[1]\n",
    "        return f\"Author({name})\"\n",
    "    elif \"comment\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/comment/\")[1]\n",
    "        return f\"Comment({name})\"\n",
    "    elif \"tag\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/tag/\")[1]\n",
    "        return f\"Tag({name})\"\n",
    "    elif \"upvoter\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/upvoter/\")[1]\n",
    "        return f\"UpVote({name})\"\n",
    "    elif \"downvoter\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/downvoter/\")[1]\n",
    "        return f\"DownVote({name})\"\n",
    "    return url  # In case no specific type is found, return the URL itself\n",
    "\n",
    "# Create a mapping for nodes to avoid duplicates\n",
    "node_map = {}\n",
    "\n",
    "# Function to add a node if it doesn't already exist\n",
    "def add_node(label, node_type, group, description=\"\"):\n",
    "    if label not in node_map:\n",
    "        node_id = f\"{node_type}_{len(d3_data['nodes']) + 1}\"  # Create a unique node ID\n",
    "        node_map[label] = node_id\n",
    "        d3_data[\"nodes\"].append({\n",
    "            \"id\": node_id,\n",
    "            \"label\": label,\n",
    "            \"type\": node_type,\n",
    "            \"group\": group  # Group for categorization\n",
    "        })\n",
    "    return node_map[label]\n",
    "\n",
    "# Populate nodes and links based on the RDF graph structure\n",
    "for subreddit in g.subjects(rdflib.RDF.type, SIOC.Container):\n",
    "    subreddit_label = str(subreddit)\n",
    "    subreddit_label= parse_label(subreddit_label)\n",
    "    subreddit_id = add_node(subreddit_label, \"Subreddit\", group=0, description=\"Community for specific topics\")\n",
    "\n",
    "    # For each post in the subreddit\n",
    "    for post in g.objects(subreddit, SIOC.has_post):\n",
    "        post_label = str(post)\n",
    "        post_label= parse_label(post_label)\n",
    "        post_id = add_node(post_label, \"Post\", group=1, description=\"Individual posts in the subreddit\")\n",
    "\n",
    "        # Create a link from subreddit to post\n",
    "        d3_data[\"links\"].append({\n",
    "            \"source\": subreddit_id,\n",
    "            \"target\": post_id,\n",
    "            \"type\": \"Contains\",\n",
    "            \"weight\": 1  # Link weight can be adjusted based on relevance or count\n",
    "        })\n",
    "\n",
    "        # Add authors\n",
    "        author = g.value(post, SIOC.has_creator)\n",
    "        if author:\n",
    "            author_label = str(author)\n",
    "            author_label= parse_label(author_label)\n",
    "            author_id = add_node(author_label, \"Author\", group=2, description=\"User who created the post\")\n",
    "\n",
    "            # Create a link from post to author\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": author_id,\n",
    "                \"type\": \"CreatedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        # For each comment on the post\n",
    "        for comment in g.objects(post, SIOC.has_reply):\n",
    "            comment_label = str(comment)\n",
    "            comment_label= parse_label(comment_label)\n",
    "            comment_id = add_node(comment_label, \"Comment\", group=3, description=\"User comments on the post\")\n",
    "\n",
    "            # Create a link from post to comment\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": comment_id,\n",
    "                \"type\": \"HasReply\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "            # Link comment to author if available\n",
    "            if author:\n",
    "                d3_data[\"links\"].append({\n",
    "                    \"source\": author_id,\n",
    "                    \"target\": comment_id,\n",
    "                    \"type\": \"CommentedBy\",\n",
    "                    \"weight\": 1\n",
    "                })\n",
    "\n",
    "            # Add tags to comment if available\n",
    "            for tag in g.objects(post, REDDIT.TaggedIn):\n",
    "                tag_label = str(tag)\n",
    "                tag_label= parse_label(tag_label)\n",
    "                tag_id = add_node(tag_label, \"Tag\", group=4, description=\"Tags related to the post or comment\")\n",
    "                d3_data[\"links\"].append({\n",
    "                    \"source\": comment_id,\n",
    "                    \"target\": tag_id,\n",
    "                    \"type\": \"TaggedWith\",\n",
    "                    \"weight\": 1\n",
    "                })\n",
    "\n",
    "        # Add topics to post\n",
    "        for topic in g.objects(post, SIOC.topic):\n",
    "            topic_label = str(topic)\n",
    "            topic_label= parse_label(topic_label)\n",
    "            topic_id = add_node(topic_label, \"Topic\", group=5, description=\"Topic related to the post\")\n",
    "\n",
    "            # Create a link from post to topic\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": topic_id,\n",
    "                \"type\": \"RelatedTo\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        # Add votes (upvotes and downvotes)\n",
    "        for upvoter in g.objects(post, REDDIT.UpvotedBy):\n",
    "            upvoter_label = str(upvoter)\n",
    "            upvoter_label= parse_label(upvoter_label)\n",
    "            upvoter_id = add_node(upvoter_label, \"Upvoter\", group=6, description=\"User who upvoted the post\")\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": upvoter_id,\n",
    "                \"type\": \"UpvotedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        for downvoter in g.objects(post, REDDIT.DownvotedBy):\n",
    "            downvoter_label = str(downvoter)\n",
    "            downvoter_label= parse_label(downvoter_label)\n",
    "            downvoter_id = add_node(downvoter_label, \"Downvoter\", group=7, description=\"User who downvoted the post\")\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": downvoter_id,\n",
    "                \"type\": \"DownvotedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "# Write the D3-compatible JSON data to a file\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/D3KG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(d3_data, f, indent=4, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import rdflib\n",
    "from collections import deque\n",
    "\n",
    "# Initialize D3-compatible JSON structure\n",
    "d3_data = {\n",
    "    \"nodes\": [],\n",
    "    \"links\": []\n",
    "}\n",
    "\n",
    "# Create a mapping for nodes to avoid duplicates\n",
    "node_map = {}\n",
    "\n",
    "# Function to add a node if it doesn't already exist\n",
    "def add_node(label, node_type, group, description):\n",
    "    if label not in node_map:\n",
    "        node_id = f\"{node_type}_{len(d3_data['nodes']) + 1}\"  # Create a unique node ID\n",
    "        node_map[label] = node_id\n",
    "        d3_data[\"nodes\"].append({\n",
    "            \"id\": node_id,\n",
    "            \"label\": label,\n",
    "            \"type\": node_type,\n",
    "            \"group\": group,\n",
    "            \"description\": description\n",
    "        })\n",
    "    return node_map[label]\n",
    "\n",
    "# Function to perform BFS and retrieve a subgraph\n",
    "def bfs(start_node_id, max_depth):\n",
    "    visited = set()\n",
    "    queue = deque([(start_node_id, 0)])  # (node_id, depth)\n",
    "    nodes_to_display = set([start_node_id])\n",
    "    links_to_display = []\n",
    "\n",
    "    # Mark the starting node to highlight it\n",
    "    start_node_found = False\n",
    "\n",
    "    while queue:\n",
    "        current_node_id, current_depth = queue.popleft()\n",
    "        if current_depth < max_depth:\n",
    "            # Add links of current node to the links_to_display\n",
    "            for link in d3_data[\"links\"]:\n",
    "                if link[\"source\"] == current_node_id and link[\"target\"] not in visited:\n",
    "                    links_to_display.append(link)\n",
    "                    queue.append((link[\"target\"], current_depth + 1))\n",
    "                elif link[\"target\"] == current_node_id and link[\"source\"] not in visited:\n",
    "                    links_to_display.append(link)\n",
    "                    queue.append((link[\"source\"], current_depth + 1))\n",
    "\n",
    "        visited.add(current_node_id)\n",
    "\n",
    "        # Highlight the start node\n",
    "        if current_node_id == start_node_id and not start_node_found:\n",
    "            start_node_found = True\n",
    "            for node in d3_data[\"nodes\"]:\n",
    "                if node[\"id\"] == start_node_id:\n",
    "                    node[\"highlighted\"] = \"Head\"\n",
    "\n",
    "    # Filter the nodes that are part of the display\n",
    "    for link in links_to_display:\n",
    "        nodes_to_display.add(link[\"source\"])\n",
    "        nodes_to_display.add(link[\"target\"])\n",
    "\n",
    "    # Filter out the nodes and links that should be displayed\n",
    "    filtered_nodes = [node for node in d3_data[\"nodes\"] if node[\"id\"] in nodes_to_display]\n",
    "    filtered_links = [link for link in links_to_display]\n",
    "\n",
    "    return filtered_nodes, filtered_links\n",
    "\n",
    "# Load the RDF data from the D3KG.json file\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/D3KG.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    d3_data = json.load(f)\n",
    "\n",
    "# Example of how to call BFS with a starting node and max depth\n",
    "start_node = \"Comment_4\"\n",
    "max_depth = 2\n",
    "filtered_nodes, filtered_links = bfs(start_node, max_depth)\n",
    "\n",
    "# Write the filtered subgraph to a file\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/BFS.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"nodes\": filtered_nodes, \"links\": filtered_links}, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Loading Knowledge Graphs...\n",
      "‚úÖ Loaded KG.json with 63763 entities.\n",
      "‚ùå Error loading KG.json: 'list' object has no attribute 'items'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x00000294440372E0>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\isodate\\isodatetime.py\", line 55, in parse_datetime\n",
      "    tmpdate = parse_date(datestring)\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\isodate\\isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: 'Na'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded KG.ttl with 648909 triples.\n",
      "\n",
      "üîç Retrieving Relevant Comments...\n",
      "‚úÖ Retrieval Time: 0.0 sec\n",
      "\n",
      "ü§ñ Querying LLM...\n",
      "\n",
      "üí° LLM Response: The Middle District of Louisiana is a federal judicial district that includes the eastern and central parts of the state. The court has jurisdiction over civil and criminal cases that arise within the district.\n",
      "\n",
      "It's important to note that court decisions and results are public records, but they may not be readily available online. In general, you can find court decisions and results through online legal databases, such as:\n",
      "\n",
      "1. Westlaw or LexisNexis: These are fee-based legal databases that provide access to court decisions, statutes, and other legal materials.\n",
      "2. Federal Judicial Center's database: The Federal Judicial Center is the research arm of the federal judiciary. Their database provides access to opinions, orders, and other documents from federal courts, including the Middle District of Louisiana.\n",
      "3. The court's website: The Middle District of Louisiana's website may provide access to opinions, orders, and other court documents.\n",
      "4. PACER: The Public Access to Court Electronic Records (PACER) system is a fee-based service that provides access to federal court documents.\n",
      "\n",
      "If you are looking for a specific case or incident, you may want to try searching on one of these databases using the case number, parties' names, or other relevant information.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import rdflib\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from rdflib import Graph, Namespace, Literal, URIRef\n",
    "from groq import Groq\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ‚úÖ Initialize Groq Client\n",
    "client = Groq(api_key=\"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\")\n",
    "\n",
    "# ‚úÖ CSV File for Conversation History\n",
    "csv_file_path = \"conversation_history.csv\"\n",
    "conversation_history = []\n",
    "\n",
    "# ‚úÖ Define RDF Namespaces\n",
    "SIOC = Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "REDDIT = Namespace(\"http://reddit.com/ns#\")\n",
    "\n",
    "# ‚úÖ Load KG.json for Fast Lookups\n",
    "def load_kg_json(file_path):\n",
    "    \"\"\"Loads KG.json and creates an index for fast lookup.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"‚úÖ Loaded KG.json with {len(data)} entities.\")\n",
    "\n",
    "        # **Pre-index data for O(1) lookup by subreddit & topic**\n",
    "        index = defaultdict(lambda: defaultdict(list))\n",
    "        for entity_id, entity in data.items():\n",
    "            subreddit = entity.get(\"sioc:Container\", \"\").replace(\"http://reddit.com/subreddit/\", \"\")\n",
    "            topics = entity.get(\"sioc:topic\", [])\n",
    "            if isinstance(topics, str):\n",
    "                topics = [topics]\n",
    "\n",
    "            for topic in topics:\n",
    "                topic = topic.replace(\"http://reddit.com/topic/\", \"\")\n",
    "                index[subreddit][topic].append(entity_id)\n",
    "\n",
    "        return index, data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading KG.json: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ‚úÖ Load KG.ttl & Build Adjacency List\n",
    "def load_kg_ttl(file_path):\n",
    "    \"\"\"Loads KG.ttl and builds an adjacency list for fast traversal.\"\"\"\n",
    "    try:\n",
    "        g = Graph()\n",
    "        g.parse(file_path, format=\"turtle\")\n",
    "        adjacency_list = defaultdict(list)\n",
    "\n",
    "        for s, p, o in g:\n",
    "            adjacency_list[str(s)].append((s, p, o))\n",
    "\n",
    "        print(f\"‚úÖ Loaded KG.ttl with {len(g)} triples.\")\n",
    "        return g, adjacency_list\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading KG.ttl: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ‚úÖ **Faster Comment Retrieval**\n",
    "def retrieve_relevant_comments(index, kg_json, subreddit, topic):\n",
    "    \"\"\"Retrieves only relevant comments based on subreddit & topic index.\"\"\"\n",
    "    if not index:\n",
    "        return \"‚ùå KG.json not loaded.\"\n",
    "\n",
    "    # **Step 1: Get relevant posts from pre-indexed data**\n",
    "    relevant_posts = index.get(subreddit, {}).get(topic, [])\n",
    "    if not relevant_posts:\n",
    "        return \"‚ùå No relevant posts found for the subreddit & topic.\"\n",
    "\n",
    "    # **Step 2: Fetch only comments from those posts**\n",
    "    matched_comments = []\n",
    "    for post in relevant_posts:\n",
    "        post_data = kg_json.get(post, {})\n",
    "        if \"sioc:has_reply\" in post_data:\n",
    "            replies = post_data[\"sioc:has_reply\"]\n",
    "            if isinstance(replies, str):\n",
    "                replies = [replies]\n",
    "            for comment in replies:\n",
    "                comment_text = kg_json.get(comment, {}).get(\"dc:title\", \"\")\n",
    "                if comment_text:\n",
    "                    matched_comments.append(comment_text)\n",
    "\n",
    "    return {\"context\": matched_comments[:10]} if matched_comments else \"‚ùå No relevant comments found.\"\n",
    "\n",
    "# ‚úÖ Groq Chat API\n",
    "def chat_with_groq(context, user_query):\n",
    "    \"\"\"Interacts with Groq model using retrieved KG context.\"\"\"\n",
    "    global conversation_history\n",
    "\n",
    "    conversation_history.append({\"role\": \"user\", \"content\": user_query})\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {user_query}\n",
    "\n",
    "    Provide a detailed answer based on the context.\n",
    "    \"\"\"\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=conversation_history,  \n",
    "        model=\"llama3-8b-8192\"\n",
    "    )\n",
    "\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    conversation_history.append({\"role\": \"Reddit based assistant\", \"content\": response})\n",
    "\n",
    "    with open(csv_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Role\", \"Content\"])\n",
    "        for entry in conversation_history:\n",
    "            writer.writerow([entry[\"role\"], entry[\"content\"]])\n",
    "\n",
    "    return response\n",
    "\n",
    "# ‚úÖ **Run Main Program**\n",
    "if __name__ == \"__main__\":\n",
    "    kg_json_path = \"./KG.json\"\n",
    "    kg_ttl_path = \"./KG.ttl\"\n",
    "\n",
    "    print(\"\\nüîç Loading Knowledge Graphs...\")\n",
    "    index, kg_json = load_kg_json(kg_json_path)\n",
    "    kg_ttl, adjacency_list = load_kg_ttl(kg_ttl_path)\n",
    "\n",
    "    subreddit = \"News\"\n",
    "    topic = \"Democracy\"\n",
    "    user_query = \"Middle District Court of Louisiana incident results?\"\n",
    "\n",
    "    print(\"\\nüîç Retrieving Relevant Comments...\")\n",
    "    start_time = time.time()\n",
    "    context = retrieve_relevant_comments(index, kg_json, subreddit, topic)\n",
    "    retrieval_time = round(time.time() - start_time, 2)\n",
    "    print(f\"‚úÖ Retrieval Time: {retrieval_time} sec\")\n",
    "\n",
    "    print(\"\\nü§ñ Querying LLM...\")\n",
    "    response = chat_with_groq(context, user_query)\n",
    "\n",
    "    print(\"\\nüí° LLM Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Ranked Recommended Visualizations:\n",
      "word_cloud: 1.0\n",
      "bar_chart: 0.59\n",
      "line_chart: 0.55\n",
      "heatmap_chart: 0.35\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load NLP models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define visualization categories\n",
    "chart_types = {\n",
    "    \"area_chart\": \"Shows cumulative data trends with a filled area.\",\n",
    "    \"bar_chart\": \"Used for comparing categories or ranking values.\",\n",
    "    \"chord_diagram\": \"Best for visualizing relationships and interactions.\",\n",
    "    \"circle_packing\": \"Represents hierarchical relationships in a compact form.\",\n",
    "    \"connection_map\": \"Visualizes spatial relationships and geographic data.\",\n",
    "    \"DAG\": \"Shows directed relationships, commonly used for processes or networks.\",\n",
    "    \"donut_chart\": \"A variation of the pie chart, highlighting proportions.\",\n",
    "    \"heatmap_chart\": \"Displays intensity values in a matrix format.\",\n",
    "    \"line_chart\": \"Best for showing trends over time or sequential data.\",\n",
    "    \"mosaic_plot\": \"Used to show the relationship between categorical variables.\",\n",
    "    \"network_graph\": \"Illustrates complex relationships in networks.\",\n",
    "    \"polar_area\": \"Represents cyclic data with proportionally scaled segments.\",\n",
    "    \"small_multiples\": \"Facilitates comparisons across multiple categories.\",\n",
    "    \"stacked_area_chart\": \"Shows part-to-whole relationships over time.\",\n",
    "    \"sunburst_chart\": \"Depicts hierarchical data as concentric layers.\",\n",
    "    \"tree_diagram\": \"Illustrates hierarchical relationships in tree structure.\",\n",
    "    \"treemap_chart\": \"Depicts hierarchical structures using nested rectangles.\",\n",
    "    \"voronoi_map\": \"Divides spatial regions based on distance.\",\n",
    "    \"word_cloud\": \"Visualizes common words and keyword frequency in text-heavy data.\"\n",
    "}\n",
    "\n",
    "# Precompute chart category embeddings\n",
    "category_embeddings = {\n",
    "    chart: model.encode(description, normalize_embeddings=True)\n",
    "    for chart, description in chart_types.items()\n",
    "}\n",
    "\n",
    "# Extract features from query & response\n",
    "def extract_features(query, response):\n",
    "    \"\"\"Extracts key elements from the query-response pair.\"\"\"\n",
    "    combined_text = query + \" \" + response\n",
    "    doc = nlp(combined_text)\n",
    "\n",
    "    numbers = [token.text for token in doc if token.like_num]\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\"}]\n",
    "    \n",
    "    trend_keywords = {\"increase\", \"decline\", \"growth\", \"rise\", \"drop\", \"trend\", \"fall\", \"reduce\", \"expansion\"}\n",
    "    trends = [token.lemma_ for token in doc if token.lemma_ in trend_keywords]\n",
    "    \n",
    "    relationship_keywords = {\"prefer\", \"dominate\", \"compared\", \"versus\", \"majority\", \"minority\"}\n",
    "    relationships = [token.lemma_ for token in doc if token.lemma_ in relationship_keywords]\n",
    "    \n",
    "    ranking_keywords = {\"ranking\", \"top\", \"best\", \"percent\", \"favorite\", \"most popular\"}\n",
    "    rankings = [token.lemma_ for token in doc if token.lemma_ in ranking_keywords]\n",
    "    \n",
    "    geo_terms = {\"earthquake\", \"magnitude\", \"epicenter\", \"seismic\", \"hurricane\", \"flood\", \"storm\"}\n",
    "    geo_features = [token.lemma_ for token in doc if token.lemma_ in geo_terms]\n",
    "    \n",
    "    keywords = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Identify text-heavy responses\n",
    "    is_text_heavy = len(keywords) > 30 \n",
    "\n",
    "    return {\n",
    "        \"numbers\": numbers,\n",
    "        \"locations\": locations,\n",
    "        \"trends\": trends,\n",
    "        \"relationships\": relationships,\n",
    "        \"geo_features\": geo_features,\n",
    "        \"rankings\": rankings,\n",
    "        \"keywords\": keywords,\n",
    "        \"is_text_heavy\": is_text_heavy  # New feature\n",
    "    }\n",
    "\n",
    "# Adjust scores based on extracted features\n",
    "def boost_scores(scores, response_type, features):\n",
    "    \"\"\"Boosts scores dynamically based on detected features.\"\"\"\n",
    "    boosts = {\n",
    "        \"numeric\": {\"bar_chart\": 0.6, \"line_chart\": 0.5, \"heatmap_chart\": 0.4},\n",
    "        \"geographic\": {\"connection_map\": 0.7, \"voronoi_map\": 0.6, \"network_graph\": 0.5},\n",
    "        \"textual\": {\"word_cloud\": 0.7, \"chord_diagram\": 0.5}\n",
    "    }\n",
    "    \n",
    "    if response_type in boosts:\n",
    "        for chart, boost in boosts[response_type].items():\n",
    "            scores[chart] += boost\n",
    "    \n",
    "    if features[\"trends\"]:\n",
    "        scores[\"line_chart\"] += 0.6\n",
    "        scores[\"area_chart\"] += 0.5\n",
    "    \n",
    "    if features[\"relationships\"]:\n",
    "        scores[\"bar_chart\"] += 0.6\n",
    "        scores[\"chord_diagram\"] += 0.5\n",
    "    \n",
    "    if features[\"geo_features\"]:\n",
    "        scores[\"connection_map\"] += 0.7\n",
    "        scores[\"network_graph\"] += 0.6\n",
    "    \n",
    "    if features[\"rankings\"]:\n",
    "        scores[\"bar_chart\"] += 0.7\n",
    "        scores[\"mosaic_plot\"] += 0.5\n",
    "\n",
    "    # Ensure Word Cloud is boosted for text-heavy responses\n",
    "    if features[\"is_text_heavy\"]:\n",
    "        scores[\"word_cloud\"] += 1.0\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Function to recommend all visualizations\n",
    "def recommend_visualizations(query, response):\n",
    "    \"\"\"Recommends a ranked list of all visualizations.\"\"\"\n",
    "    features = extract_features(query, response)\n",
    "    response_embedding = model.encode(query + \" \" + response, normalize_embeddings=True)\n",
    "    \n",
    "    similarity_scores = {\n",
    "        chart: cosine_similarity([response_embedding], [embedding]).flatten()[0]\n",
    "        for chart, embedding in category_embeddings.items()\n",
    "    }\n",
    "    \n",
    "    # Apply feature-based score boosts\n",
    "    similarity_scores = boost_scores(similarity_scores, \"numeric\", features)\n",
    "\n",
    "    # Normalize scores for better ranking\n",
    "    min_score = min(similarity_scores.values())\n",
    "    max_score = max(similarity_scores.values())\n",
    "    if max_score - min_score > 0:  # Avoid division by zero\n",
    "        for chart in similarity_scores:\n",
    "            similarity_scores[chart] = (similarity_scores[chart] - min_score) / (max_score - min_score)\n",
    "\n",
    "    # Sort and return all charts\n",
    "    ranked_charts = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)[:4]\n",
    "    \n",
    "    return [(chart, round(score, 2)) for chart, score in ranked_charts]\n",
    "\n",
    "def getViz(user_query, response):\n",
    "    recommended_charts = recommend_visualizations(user_query, response)\n",
    "    print(\"\\n‚úÖ Ranked Recommended Visualizations:\")\n",
    "    for chart, score in recommended_charts:\n",
    "        print(f\"{chart}: {score}\")\n",
    "\n",
    "getViz(user_query, response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
