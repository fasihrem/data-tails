{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontologies to use\n",
    "- **Post-Subreddit:** A post belongs to a specific subreddit.\n",
    "- **Post-Author:** A post is created by an author.\n",
    "- **Post-Topics:** A post is related to one or more topics based on the topic modeling results.\n",
    "- **Post-Comments:** A post has a set of comments.\n",
    "- **Post-Keywords:** A post is associated with specific keywords derived from the topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading libraries to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvis in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (8.9.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.3.0)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources \n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "!pip install pyvis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import rdflib\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from gensim import corpora\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data from *MongoDB*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connect to MongoDB\n",
    "- Read the data from the collection\n",
    "- Convert the data into a pandas DataFrame\n",
    "- Print the first few rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  type  \\\n",
      "0  66e965e698330736e0d693d5  None   \n",
      "1  66e965e698330736e0d693d6  None   \n",
      "2  66e965e698330736e0d693d7  None   \n",
      "3  66e965e698330736e0d693d8  None   \n",
      "4  66e965e698330736e0d693d9  None   \n",
      "\n",
      "                                           postTitle postDesc  \\\n",
      "0  Adults(especially those over 30), how young do...      NaN   \n",
      "1  What is a thing that your parents consider nor...      NaN   \n",
      "2                 What is a smell that comforts you?      NaN   \n",
      "3  When in history was it the best time to be a w...      NaN   \n",
      "4    What's the worst way someone broke up with you?      NaN   \n",
      "\n",
      "              postTime            authorName noOfUpvotes isNSFW  \\\n",
      "0  2024-08-06 01:02:35  Excellent-Studio7257        4068  False   \n",
      "1  2024-08-06 01:47:22        Bigbumoffhappy        2073  False   \n",
      "2  2024-08-05 22:21:53         bloomsmittenn        2188  False   \n",
      "3  2024-08-06 03:32:59   More_food_please_77         778  False   \n",
      "4  2024-08-05 21:01:39    ImpressiveWrap7363        1989  False   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  [It's weird. I can have an adult/intellectual ...        5590.0   \n",
      "1  [Thinking you're supposed to raise your kids t...        1684.0   \n",
      "2  [The weather in the woods after a heavy rain, ...        4958.0   \n",
      "3  [It might depend on where you are but now with...        1046.0   \n",
      "4  [I have 2 bad ones.  First one, he had his mom...        1749.0   \n",
      "\n",
      "                                            imageUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                             postUrl  subReddit  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['DataTails']\n",
    "collection = db['Data']  \n",
    "data_cursor = collection.find({})\n",
    "DF = pd.DataFrame(list(data_cursor))\n",
    "print(DF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stopword Removal & Lemmatization:** \n",
    "    - Preprocessing() uses NLTK to remove stopwords and lemmatize the text.\n",
    "- **Handling Missing Data:**\n",
    "    - Missing postDesc fields are filled with an empty string.\n",
    "    - Missing noOfUpvotes is filled with 0.\n",
    "- **Datetime Conversion:** \n",
    "    - postTime is converted to a datetime object, and any errors are coerced.\n",
    "- **Handling Comments:** \n",
    "    - Comments are converted from list format to a string of concatenated comments.\n",
    "- **Final Clean Text:** \n",
    "    - Both postTitle and postDesc are cleaned using regular expressions and tokenization, and then passed through the NLTK-based text preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subReddit           2\n",
      "postTitle           0\n",
      "postDesc        15607\n",
      "postTime            0\n",
      "authorName        587\n",
      "noOfUpvotes         0\n",
      "comments            0\n",
      "noOfComments        2\n",
      "postUrl             2\n",
      "imageUrl            3\n",
      "isNSFW              1\n",
      "dtype: int64\n",
      "   subReddit                                          postTitle postDesc  \\\n",
      "0  AskReddit                   [adult, especially, young, seem]       []   \n",
      "1  AskReddit  [parent, consider, normal, consider, normal, a...       []   \n",
      "2  AskReddit                                   [smell, comfort]       []   \n",
      "3  AskReddit                       [history, best, time, woman]       []   \n",
      "4  AskReddit                       [worst, way, someone, broke]       []   \n",
      "\n",
      "             postTime            authorName noOfUpvotes  \\\n",
      "0 2024-08-06 01:02:35  Excellent-Studio7257        4068   \n",
      "1 2024-08-06 01:47:22        Bigbumoffhappy        2073   \n",
      "2 2024-08-05 22:21:53         bloomsmittenn        2188   \n",
      "3 2024-08-06 03:32:59   More_food_please_77         778   \n",
      "4 2024-08-05 21:01:39    ImpressiveWrap7363        1989   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  It's weird. I can have an adult/intellectual c...        5590.0   \n",
      "1  Thinking you're supposed to raise your kids th...        1684.0   \n",
      "2  The weather in the woods after a heavy rain Ne...        4958.0   \n",
      "3  It might depend on where you are but now with ...        1046.0   \n",
      "4  I have 2 bad ones.  First one, he had his mom ...        1749.0   \n",
      "\n",
      "                                             postUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                            imageUrl  isNSFW  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "subReddit       0\n",
      "postTitle       0\n",
      "postDesc        0\n",
      "postTime        1\n",
      "authorName      0\n",
      "noOfUpvotes     0\n",
      "comments        0\n",
      "noOfComments    0\n",
      "postUrl         0\n",
      "imageUrl        0\n",
      "isNSFW          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "StopWords = set(stopwords.words('english'))\n",
    "custom_stopwords = StopWords | {\"make\", \"thing\", \"know\", \"get\", \"want\", \"like\", \"would\", \"could\", \"you\", \"say\",\"also\",\"aita\",\"com\",\"www\",\"made\",\"ago\",\"day\",\"000\"}\n",
    "def Preprocessing(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in custom_stopwords and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "Cols = ['subReddit', 'postTitle', 'postDesc', 'postTime', 'authorName', 'noOfUpvotes', 'comments', 'noOfComments', 'postUrl','imageUrl','isNSFW']\n",
    "DF = DF[Cols]\n",
    "print(DF.isnull().sum())\n",
    "\n",
    "\n",
    "DF['subReddit'] = DF['subReddit'].fillna('Unknown_SubReddit')\n",
    "DF['authorName'] = DF['authorName'].fillna('Unknown_Author')\n",
    "DF['postTitle'] = DF['postTitle'].fillna('Untitled')\n",
    "DF['postUrl'] = DF['postUrl'].fillna('http://example.com/NOPOST.png')\n",
    "DF['imageUrl'] = DF['imageUrl'].fillna('http://example.com/NOImage.png')\n",
    "DF['isNSFW'] = DF['isNSFW'].fillna(False)\n",
    "DF['postDesc'].fillna('', inplace=True)\n",
    "DF['noOfUpvotes'].fillna(0, inplace=True)\n",
    "DF['noOfComments'].fillna(0, inplace=True)\n",
    "DF['postTime'] = pd.to_datetime(DF['postTime'], errors='coerce')\n",
    "DF['comments'] = DF['comments'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "DF['postTitle'] = DF['postTitle'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['postDesc'] = DF['postDesc'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['isNSFW'] = DF['isNSFW'].astype(bool)\n",
    "print(DF.head())\n",
    "print(DF.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import json\n",
    "\n",
    "# RDF graph initialization\n",
    "g = rdflib.Graph()\n",
    "SIOC = rdflib.Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = rdflib.Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = rdflib.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "REDDIT = rdflib.Namespace(\"http://reddit.com/ns#\")  # Custom namespace for Reddit-specific relationships\n",
    "g.bind(\"sioc\", SIOC)\n",
    "g.bind(\"dc\", DCMI)\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"reddit\", REDDIT)\n",
    "\n",
    "\n",
    "# Function to add post data to RDF graph\n",
    "def add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri):\n",
    "    post_uri = URIRef(f\"http://reddit.com/post{index}\")\n",
    "    subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{row['subReddit']}\")\n",
    "    author_uri = URIRef(f\"http://reddit.com/user/{row['authorName']}\")\n",
    "    \n",
    "    # Add post properties and link to topic\n",
    "    g.add((post_uri, rdflib.RDF.type, SIOC.Post))\n",
    "    g.add((post_uri, DCMI.title, Literal(' '.join(row['postTitle']))))\n",
    "    g.add((post_uri, DCMI.description, Literal(' '.join(row['postDesc']))))\n",
    "    g.add((post_uri, DCMI.date, Literal(row['postTime'])))\n",
    "    g.add((post_uri, SIOC.num_replies, Literal(row['noOfUpvotes'])))\n",
    "    g.add((post_uri, SIOC.link, URIRef(row['postUrl'])))\n",
    "    g.add((post_uri, SIOC.NSFW, Literal(row['isNSFW'])))\n",
    "    g.add((post_uri, SIOC.has_type, post_type_uri))\n",
    "    g.add((post_uri, SIOC.topic, topic_uri))\n",
    "\n",
    "    # Create the relationships based on the list of relationship types\n",
    "    # CreatedBy: Indicates that a post is created by an author.\n",
    "    g.add((post_uri, REDDIT.CreatedBy, author_uri))\n",
    "\n",
    "    # BelongsTo: Links a post to a subreddit.\n",
    "    g.add((post_uri, REDDIT.BelongsTo, subreddit_uri))\n",
    "\n",
    "    # PartOfPost: Indicates that a comment is part of a post (the comment belongs to the post).\n",
    "    comment_uri = URIRef(f\"http://reddit.com/comment/{index}\")\n",
    "    g.add((comment_uri, REDDIT.PartOfPost, post_uri))\n",
    "\n",
    "    # HasType: Links an entity (post, comment) to its specific type (text post, link post, etc.).\n",
    "    g.add((post_uri, REDDIT.HasType, post_type_uri))\n",
    "    g.add((comment_uri, REDDIT.HasType, comment_type_uri))\n",
    "\n",
    "    # TaggedIn: Indicates that a post or comment is tagged with a specific topic or keyword.\n",
    "    for tag in row.get('tags', []):  # Assuming tags are available as a list\n",
    "        tag_uri = URIRef(f\"http://reddit.com/tag/{tag}\")\n",
    "        g.add((post_uri, REDDIT.TaggedIn, tag_uri))\n",
    "        g.add((comment_uri, REDDIT.TaggedIn, tag_uri))\n",
    "\n",
    "    # UpvotedBy: Indicates that a user has upvoted a post or comment.\n",
    "    if row.get('upvotedBy'):\n",
    "        for upvoter in row['upvotedBy']:\n",
    "            upvoter_uri = URIRef(f\"http://reddit.com/user/{upvoter}\")\n",
    "            g.add((post_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "\n",
    "    # DownvotedBy: Indicates that a user has downvoted a post or comment.\n",
    "    if row.get('downvotedBy'):\n",
    "        for downvoter in row['downvotedBy']:\n",
    "            downvoter_uri = URIRef(f\"http://reddit.com/user/{downvoter}\")\n",
    "            g.add((post_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "\n",
    "    # Moderates: Indicates that a user is a moderator of a subreddit.\n",
    "    if row.get('moderators'):\n",
    "        for moderator in row['moderators']:\n",
    "            moderator_uri = URIRef(f\"http://reddit.com/user/{moderator}\")\n",
    "            g.add((subreddit_uri, REDDIT.Moderates, moderator_uri))\n",
    "\n",
    "    # HasFlair: Links a post or comment to a specific flair (e.g., tags like \"Important\", \"Question\", etc.).\n",
    "    if row.get('flair'):\n",
    "        flair_uri = URIRef(f\"http://reddit.com/flair/{row['flair']}\")\n",
    "        g.add((post_uri, REDDIT.HasFlair, flair_uri))\n",
    "        g.add((comment_uri, REDDIT.HasFlair, flair_uri))\n",
    "\n",
    "    # Link post to subreddit and type\n",
    "    g.add((subreddit_uri, rdflib.RDF.type, SIOC.Container))\n",
    "    g.add((subreddit_uri, SIOC.has_post, post_uri))\n",
    "    g.add((subreddit_uri, SIOC.has_type, subreddit_type_uri))\n",
    "    g.add((post_uri, SIOC.Container, subreddit_uri))\n",
    "\n",
    "    # Link to author and author type\n",
    "    g.add((author_uri, rdflib.RDF.type, FOAF.Person))\n",
    "    g.add((author_uri, FOAF.name, Literal(row['authorName'])))\n",
    "    g.add((post_uri, SIOC.has_creator, author_uri))\n",
    "    g.add((author_uri, SIOC.has_type, author_type_uri))\n",
    "\n",
    "    # Add comments and link to comment type\n",
    "    g.add((comment_uri, rdflib.RDF.type, SIOC.Comment))\n",
    "    g.add((comment_uri, DCMI.title, Literal(row['comments'])))\n",
    "    g.add((post_uri, SIOC.has_reply, comment_uri))\n",
    "    g.add((comment_uri, SIOC.has_type, comment_type_uri))\n",
    "\n",
    "# Process each subreddit and create LDA models\n",
    "GroupedData = DF.groupby('subReddit')\n",
    "all_topics = defaultdict(set)\n",
    "\n",
    "for subreddit, group in GroupedData:\n",
    "    group['Combined'] = group['postTitle'] + group['postDesc']\n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in custom_stopwords])\n",
    "\n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "    LDA = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, iterations=100, random_state=42)\n",
    "    \n",
    "    for idx, topic in LDA.print_topics(num_words=5):\n",
    "        topic_uri = URIRef(f\"http://reddit.com/topic/{subreddit}_{idx}\")\n",
    "        topic_words = topic.split(\" + \")\n",
    "        unique_words = {word.split(\"*\")[1].strip('\"') for word in topic_words}\n",
    "        all_topics[subreddit].update(unique_words)\n",
    "\n",
    "        # Define URIs for types\n",
    "        subreddit_type_uri = URIRef(f\"http://reddit.com/type/subreddit/{subreddit}\")\n",
    "        post_type_uri = URIRef(f\"http://reddit.com/type/post/{subreddit}\")\n",
    "        comment_type_uri = URIRef(f\"http://reddit.com/type/comment/{subreddit}\")\n",
    "        author_type_uri = URIRef(f\"http://reddit.com/type/author/{subreddit}\")\n",
    "\n",
    "        # Link subreddit to topic\n",
    "        subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{subreddit}\")\n",
    "        g.add((subreddit_uri, SIOC.has_topic, topic_uri))\n",
    "        g.add((topic_uri, rdflib.RDF.type, SIOC.Topic))\n",
    "\n",
    "        # Iterate through each row in the group\n",
    "        for index, row in group.iterrows():\n",
    "            add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri)\n",
    "\n",
    "# Save graph\n",
    "g.serialize('D:/FYP/Github/data-tails/Backend/Ontologies/KG.ttl', format='turtle')\n",
    "json_ld = g.serialize(format='json-ld', indent=4)\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/KG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting *KG.json, KG.ttl* to format of D3 input file to view graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import rdflib\n",
    "# from rdflib.namespace import RDF, SIOC, REDDIT\n",
    "\n",
    "# Initialize D3-compatible JSON structure\n",
    "d3_data = {\n",
    "    \"nodes\": [],\n",
    "    \"links\": []\n",
    "}\n",
    "\n",
    "def parse_label(url):\n",
    "    if \"subreddit\" in url:\n",
    "        # Extract subreddit name after /subreddit/\n",
    "        name = url.split(\"/subreddit/\")[1]\n",
    "        return f\"Subreddit({name})\"\n",
    "    elif \"topic\" in url:\n",
    "        # Extract topic name after /topic/\n",
    "        name = url.split(\"/topic/\")[1]\n",
    "        return f\"Topic({name})\"\n",
    "    elif \"post\" in url:\n",
    "        # Extract post ID after /post\n",
    "        name = url.split(\"/reddit.com/\")[1]\n",
    "        return f\"Post({name})\"\n",
    "    elif \"user\" in url:\n",
    "        # Extract username after /user/\n",
    "        name = url.split(\"/user/\")[1]\n",
    "        return f\"Author({name})\"\n",
    "    elif \"comment\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/comment/\")[1]\n",
    "        return f\"Comment({name})\"\n",
    "    elif \"tag\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/tag/\")[1]\n",
    "        return f\"Tag({name})\"\n",
    "    elif \"upvoter\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/upvoter/\")[1]\n",
    "        return f\"UpVote({name})\"\n",
    "    elif \"downvoter\" in url:\n",
    "        # Extract comment ID after /comment/\n",
    "        name = url.split(\"/downvoter/\")[1]\n",
    "        return f\"DownVote({name})\"\n",
    "    return url  # In case no specific type is found, return the URL itself\n",
    "\n",
    "# Create a mapping for nodes to avoid duplicates\n",
    "node_map = {}\n",
    "\n",
    "# Function to add a node if it doesn't already exist\n",
    "def add_node(label, node_type, group, description=\"\"):\n",
    "    if label not in node_map:\n",
    "        node_id = f\"{node_type}_{len(d3_data['nodes']) + 1}\"  # Create a unique node ID\n",
    "        node_map[label] = node_id\n",
    "        d3_data[\"nodes\"].append({\n",
    "            \"id\": node_id,\n",
    "            \"label\": label,\n",
    "            \"type\": node_type,\n",
    "            \"group\": group  # Group for categorization\n",
    "        })\n",
    "    return node_map[label]\n",
    "\n",
    "# Populate nodes and links based on the RDF graph structure\n",
    "for subreddit in g.subjects(rdflib.RDF.type, SIOC.Container):\n",
    "    subreddit_label = str(subreddit)\n",
    "    subreddit_label= parse_label(subreddit_label)\n",
    "    subreddit_id = add_node(subreddit_label, \"Subreddit\", group=0, description=\"Community for specific topics\")\n",
    "\n",
    "    # For each post in the subreddit\n",
    "    for post in g.objects(subreddit, SIOC.has_post):\n",
    "        post_label = str(post)\n",
    "        post_label= parse_label(post_label)\n",
    "        post_id = add_node(post_label, \"Post\", group=1, description=\"Individual posts in the subreddit\")\n",
    "\n",
    "        # Create a link from subreddit to post\n",
    "        d3_data[\"links\"].append({\n",
    "            \"source\": subreddit_id,\n",
    "            \"target\": post_id,\n",
    "            \"type\": \"Contains\",\n",
    "            \"weight\": 1  # Link weight can be adjusted based on relevance or count\n",
    "        })\n",
    "\n",
    "        # Add authors\n",
    "        author = g.value(post, SIOC.has_creator)\n",
    "        if author:\n",
    "            author_label = str(author)\n",
    "            author_label= parse_label(author_label)\n",
    "            author_id = add_node(author_label, \"Author\", group=2, description=\"User who created the post\")\n",
    "\n",
    "            # Create a link from post to author\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": author_id,\n",
    "                \"type\": \"CreatedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        # For each comment on the post\n",
    "        for comment in g.objects(post, SIOC.has_reply):\n",
    "            comment_label = str(comment)\n",
    "            comment_label= parse_label(comment_label)\n",
    "            comment_id = add_node(comment_label, \"Comment\", group=3, description=\"User comments on the post\")\n",
    "\n",
    "            # Create a link from post to comment\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": comment_id,\n",
    "                \"type\": \"HasReply\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "            # Link comment to author if available\n",
    "            if author:\n",
    "                d3_data[\"links\"].append({\n",
    "                    \"source\": author_id,\n",
    "                    \"target\": comment_id,\n",
    "                    \"type\": \"CommentedBy\",\n",
    "                    \"weight\": 1\n",
    "                })\n",
    "\n",
    "            # Add tags to comment if available\n",
    "            for tag in g.objects(post, REDDIT.TaggedIn):\n",
    "                tag_label = str(tag)\n",
    "                tag_label= parse_label(tag_label)\n",
    "                tag_id = add_node(tag_label, \"Tag\", group=4, description=\"Tags related to the post or comment\")\n",
    "                d3_data[\"links\"].append({\n",
    "                    \"source\": comment_id,\n",
    "                    \"target\": tag_id,\n",
    "                    \"type\": \"TaggedWith\",\n",
    "                    \"weight\": 1\n",
    "                })\n",
    "\n",
    "        # Add topics to post\n",
    "        for topic in g.objects(post, SIOC.topic):\n",
    "            topic_label = str(topic)\n",
    "            topic_label= parse_label(topic_label)\n",
    "            topic_id = add_node(topic_label, \"Topic\", group=5, description=\"Topic related to the post\")\n",
    "\n",
    "            # Create a link from post to topic\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": topic_id,\n",
    "                \"type\": \"RelatedTo\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        # Add votes (upvotes and downvotes)\n",
    "        for upvoter in g.objects(post, REDDIT.UpvotedBy):\n",
    "            upvoter_label = str(upvoter)\n",
    "            upvoter_label= parse_label(upvoter_label)\n",
    "            upvoter_id = add_node(upvoter_label, \"Upvoter\", group=6, description=\"User who upvoted the post\")\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": upvoter_id,\n",
    "                \"type\": \"UpvotedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "        for downvoter in g.objects(post, REDDIT.DownvotedBy):\n",
    "            downvoter_label = str(downvoter)\n",
    "            downvoter_label= parse_label(downvoter_label)\n",
    "            downvoter_id = add_node(downvoter_label, \"Downvoter\", group=7, description=\"User who downvoted the post\")\n",
    "            d3_data[\"links\"].append({\n",
    "                \"source\": post_id,\n",
    "                \"target\": downvoter_id,\n",
    "                \"type\": \"DownvotedBy\",\n",
    "                \"weight\": 1\n",
    "            })\n",
    "\n",
    "# Write the D3-compatible JSON data to a file\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/D3KG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(d3_data, f, indent=4, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import rdflib\n",
    "from collections import deque\n",
    "\n",
    "# Initialize D3-compatible JSON structure\n",
    "d3_data = {\n",
    "    \"nodes\": [],\n",
    "    \"links\": []\n",
    "}\n",
    "\n",
    "# Create a mapping for nodes to avoid duplicates\n",
    "node_map = {}\n",
    "\n",
    "# Function to add a node if it doesn't already exist\n",
    "def add_node(label, node_type, group, description):\n",
    "    if label not in node_map:\n",
    "        node_id = f\"{node_type}_{len(d3_data['nodes']) + 1}\"  # Create a unique node ID\n",
    "        node_map[label] = node_id\n",
    "        d3_data[\"nodes\"].append({\n",
    "            \"id\": node_id,\n",
    "            \"label\": label,\n",
    "            \"type\": node_type,\n",
    "            \"group\": group,\n",
    "            \"description\": description\n",
    "        })\n",
    "    return node_map[label]\n",
    "\n",
    "# Function to perform BFS and retrieve a subgraph\n",
    "def bfs(start_node_id, max_depth):\n",
    "    visited = set()\n",
    "    queue = deque([(start_node_id, 0)])  # (node_id, depth)\n",
    "    nodes_to_display = set([start_node_id])\n",
    "    links_to_display = []\n",
    "\n",
    "    # Mark the starting node to highlight it\n",
    "    start_node_found = False\n",
    "\n",
    "    while queue:\n",
    "        current_node_id, current_depth = queue.popleft()\n",
    "        if current_depth < max_depth:\n",
    "            # Add links of current node to the links_to_display\n",
    "            for link in d3_data[\"links\"]:\n",
    "                if link[\"source\"] == current_node_id and link[\"target\"] not in visited:\n",
    "                    links_to_display.append(link)\n",
    "                    queue.append((link[\"target\"], current_depth + 1))\n",
    "                elif link[\"target\"] == current_node_id and link[\"source\"] not in visited:\n",
    "                    links_to_display.append(link)\n",
    "                    queue.append((link[\"source\"], current_depth + 1))\n",
    "\n",
    "        visited.add(current_node_id)\n",
    "\n",
    "        # Highlight the start node\n",
    "        if current_node_id == start_node_id and not start_node_found:\n",
    "            start_node_found = True\n",
    "            for node in d3_data[\"nodes\"]:\n",
    "                if node[\"id\"] == start_node_id:\n",
    "                    node[\"highlighted\"] = \"Head\"\n",
    "\n",
    "    # Filter the nodes that are part of the display\n",
    "    for link in links_to_display:\n",
    "        nodes_to_display.add(link[\"source\"])\n",
    "        nodes_to_display.add(link[\"target\"])\n",
    "\n",
    "    # Filter out the nodes and links that should be displayed\n",
    "    filtered_nodes = [node for node in d3_data[\"nodes\"] if node[\"id\"] in nodes_to_display]\n",
    "    filtered_links = [link for link in links_to_display]\n",
    "\n",
    "    return filtered_nodes, filtered_links\n",
    "\n",
    "# Load the RDF data from the D3KG.json file\n",
    "with open(\"D3KG.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    d3_data = json.load(f)\n",
    "\n",
    "# Example of how to call BFS with a starting node and max depth\n",
    "start_node = \"Author_3\"\n",
    "max_depth = 2\n",
    "filtered_nodes, filtered_links = bfs(start_node, max_depth)\n",
    "\n",
    "# Write the filtered subgraph to a file\n",
    "with open(\"BFS.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"nodes\": filtered_nodes, \"links\": filtered_links}, f, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRAPH RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
