{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-igraph in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.6)\n",
      "Requirement already satisfied: igraph==0.11.6 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-igraph) (0.11.6)\n",
      "Requirement already satisfied: texttable>=1.6.2 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from igraph==0.11.6->python-igraph) (1.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvis in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (8.9.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.3.0)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install python-igraph\n",
    "!pip install pyvis\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import rdflib\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  type  \\\n",
      "0  66e965e698330736e0d693d5  None   \n",
      "1  66e965e698330736e0d693d6  None   \n",
      "2  66e965e698330736e0d693d7  None   \n",
      "3  66e965e698330736e0d693d8  None   \n",
      "4  66e965e698330736e0d693d9  None   \n",
      "\n",
      "                                           postTitle postDesc  \\\n",
      "0  Adults(especially those over 30), how young do...      NaN   \n",
      "1  What is a thing that your parents consider nor...      NaN   \n",
      "2                 What is a smell that comforts you?      NaN   \n",
      "3  When in history was it the best time to be a w...      NaN   \n",
      "4    What's the worst way someone broke up with you?      NaN   \n",
      "\n",
      "              postTime            authorName noOfUpvotes isNSFW  \\\n",
      "0  2024-08-06 01:02:35  Excellent-Studio7257        4068  False   \n",
      "1  2024-08-06 01:47:22        Bigbumoffhappy        2073  False   \n",
      "2  2024-08-05 22:21:53         bloomsmittenn        2188  False   \n",
      "3  2024-08-06 03:32:59   More_food_please_77         778  False   \n",
      "4  2024-08-05 21:01:39    ImpressiveWrap7363        1989  False   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  [It's weird. I can have an adult/intellectual ...        5590.0   \n",
      "1  [Thinking you're supposed to raise your kids t...        1684.0   \n",
      "2  [The weather in the woods after a heavy rain, ...        4958.0   \n",
      "3  [It might depend on where you are but now with...        1046.0   \n",
      "4  [I have 2 bad ones.  First one, he had his mom...        1749.0   \n",
      "\n",
      "                                            imageUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                             postUrl  subReddit  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['DataTails']\n",
    "collection = db['Data']  \n",
    "data_cursor = collection.find({})\n",
    "DF = pd.DataFrame(list(data_cursor))\n",
    "print(DF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "StopWords = set(stopwords.words('english'))\n",
    "def Preprocessing(text):\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in StopWords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "Cols = ['subReddit', 'postTitle', 'postDesc', 'postTime', 'authorName', 'noOfUpvotes', 'comments', 'noOfComments', 'postUrl']\n",
    "DF = DF[Cols]\n",
    "print(DF.isnull().sum())\n",
    "DF['postDesc'].fillna('', inplace=True)\n",
    "DF['noOfUpvotes'].fillna(0, inplace=True)\n",
    "DF['postTime'] = pd.to_datetime(DF['postTime'], errors='coerce')\n",
    "DF['comments'] = DF['comments'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "DF['postTitle'] = DF['postTitle'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['postDesc'] = DF['postDesc'].apply(lambda x: Preprocessing(str(x)))\n",
    "print(DF.head())\n",
    "print(DF.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedData = DF.groupby('subReddit')\n",
    "SubReddits = defaultdict(dict)\n",
    "for subreddit, group in GroupedData:\n",
    "    print(f\"Processing Subreddit: {subreddit}\")\n",
    "    group['Combined'] = group['postTitle'] + \" \" + group['postDesc']\n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: word_tokenize(x))\n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "    LDA = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "    SubReddits[subreddit]['lda_model'] = LDA\n",
    "    SubReddits[subreddit]['dictionary'] = dictionary\n",
    "    SubReddits[subreddit]['corpus'] = corpus\n",
    "    group['topics'] = [LDA.get_document_topics(bow) for bow in corpus]\n",
    "    DF.loc[group.index, 'topics'] = group['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF.shape)\n",
    "print(DF.describe())\n",
    "print(DF.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subReddit           2\n",
      "postTitle           0\n",
      "postDesc        15607\n",
      "postTime            0\n",
      "authorName        587\n",
      "noOfUpvotes         0\n",
      "comments            0\n",
      "noOfComments        2\n",
      "postUrl             2\n",
      "imageUrl            3\n",
      "isNSFW              1\n",
      "dtype: int64\n",
      "   subReddit                                          postTitle postDesc  \\\n",
      "0  AskReddit                   [adult, especially, young, seem]       []   \n",
      "1  AskReddit  [parent, consider, normal, consider, normal, a...       []   \n",
      "2  AskReddit                                   [smell, comfort]       []   \n",
      "3  AskReddit                       [history, best, time, woman]       []   \n",
      "4  AskReddit                       [worst, way, someone, broke]       []   \n",
      "\n",
      "             postTime            authorName noOfUpvotes  \\\n",
      "0 2024-08-06 01:02:35  Excellent-Studio7257        4068   \n",
      "1 2024-08-06 01:47:22        Bigbumoffhappy        2073   \n",
      "2 2024-08-05 22:21:53         bloomsmittenn        2188   \n",
      "3 2024-08-06 03:32:59   More_food_please_77         778   \n",
      "4 2024-08-05 21:01:39    ImpressiveWrap7363        1989   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  It's weird. I can have an adult/intellectual c...        5590.0   \n",
      "1  Thinking you're supposed to raise your kids th...        1684.0   \n",
      "2  The weather in the woods after a heavy rain Ne...        4958.0   \n",
      "3  It might depend on where you are but now with ...        1046.0   \n",
      "4  I have 2 bad ones.  First one, he had his mom ...        1749.0   \n",
      "\n",
      "                                             postUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                            imageUrl  isNSFW  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "subReddit       0\n",
      "postTitle       0\n",
      "postDesc        0\n",
      "postTime        1\n",
      "authorName      0\n",
      "noOfUpvotes     0\n",
      "comments        0\n",
      "noOfComments    0\n",
      "postUrl         0\n",
      "imageUrl        0\n",
      "isNSFW          0\n",
      "dtype: int64\n",
      "Processing Subreddit: Advice\n",
      "Processing Subreddit: AmItheAsshole\n",
      "Processing Subreddit: AskReddit\n",
      "Processing Subreddit: Damnthatsinteresting\n",
      "Processing Subreddit: Filmmakers\n",
      "Processing Subreddit: Jokes\n",
      "Processing Subreddit: Music\n",
      "Processing Subreddit: NoStupidQuestions\n",
      "Processing Subreddit: Showerthoughts\n",
      "Processing Subreddit: Unknown_SubReddit\n",
      "Processing Subreddit: askscience\n",
      "Processing Subreddit: aww\n",
      "Processing Subreddit: books\n",
      "Processing Subreddit: funny\n",
      "Processing Subreddit: gadgets\n",
      "Processing Subreddit: gaming\n",
      "Processing Subreddit: help\n",
      "Processing Subreddit: islamabad\n",
      "Processing Subreddit: memes\n",
      "Processing Subreddit: mildlyinteresting\n",
      "Processing Subreddit: movies\n",
      "Processing Subreddit: news\n",
      "Processing Subreddit: olympics\n",
      "Processing Subreddit: pakistan\n",
      "Processing Subreddit: pics\n",
      "Processing Subreddit: politics\n",
      "Processing Subreddit: programming\n",
      "Processing Subreddit: science\n",
      "Processing Subreddit: showerthoughts\n",
      "Processing Subreddit: socialmedia\n",
      "Processing Subreddit: sports\n",
      "Processing Subreddit: technology\n",
      "Processing Subreddit: todayilearned\n",
      "Processing Subreddit: videos\n",
      "Processing Subreddit: wallstreetbets\n",
      "Processing Subreddit: worldnews\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "StopWords = set(stopwords.words('english'))\n",
    "custom_stopwords = StopWords | {\"make\", \"thing\", \"know\", \"http\", \"get\", \"want\", \"like\", \"would\", \"could\", \"you\", \"say\",\"also\",\"aita\",\"com\",\"www\",\"made\",\"ago\",\"day\",\"000\"}\n",
    "def Preprocessing(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in custom_stopwords and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "Cols = ['subReddit', 'postTitle', 'postDesc', 'postTime', 'authorName', 'noOfUpvotes', 'comments', 'noOfComments', 'postUrl','imageUrl','isNSFW']\n",
    "DF = DF[Cols]\n",
    "print(DF.isnull().sum())\n",
    "\n",
    "\n",
    "DF['subReddit'] = DF['subReddit'].fillna('Unknown_SubReddit')\n",
    "DF['authorName'] = DF['authorName'].fillna('Unknown_Author')\n",
    "DF['postTitle'] = DF['postTitle'].fillna('Untitled')\n",
    "DF['postUrl'] = DF['postUrl'].fillna('http://example.com/NOPOST.png')\n",
    "DF['imageUrl'] = DF['imageUrl'].fillna('http://example.com/NOImage.png')\n",
    "DF['isNSFW'] = DF['isNSFW'].fillna(False)\n",
    "DF['postDesc'].fillna('', inplace=True)\n",
    "DF['noOfUpvotes'].fillna(0, inplace=True)\n",
    "DF['noOfComments'].fillna(0, inplace=True)\n",
    "DF['postTime'] = pd.to_datetime(DF['postTime'], errors='coerce')\n",
    "DF['comments'] = DF['comments'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "DF['postTitle'] = DF['postTitle'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['postDesc'] = DF['postDesc'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['isNSFW'] = DF['isNSFW'].astype(bool)\n",
    "print(DF.head())\n",
    "print(DF.isnull().sum())\n",
    "\n",
    "# Initialize graph and namespaces\n",
    "g = rdflib.Graph()\n",
    "SIOC = rdflib.Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = rdflib.Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = rdflib.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "g.bind(\"sioc\", SIOC)\n",
    "g.bind(\"dc\", DCMI)\n",
    "g.bind(\"foaf\", FOAF)\n",
    "\n",
    "# Function to add posts to the graph\n",
    "def add_post_to_graph(row, index, topic_uri):\n",
    "    post_uri = rdflib.URIRef(f\"http://reddit.com/post{index}\")\n",
    "    subreddit_uri = rdflib.URIRef(f\"http://reddit.com/subreddit/{row['subReddit']}\")\n",
    "    author_uri = rdflib.URIRef(f\"http://reddit.com/user/{row['authorName']}\")\n",
    "    \n",
    "    g.add((post_uri, rdflib.RDF.type, SIOC.Post))\n",
    "    g.add((post_uri, DCMI.title, rdflib.Literal(row['postTitle'])))\n",
    "    g.add((post_uri, DCMI.description, rdflib.Literal(row['postDesc'])))\n",
    "    g.add((post_uri, DCMI.date, rdflib.Literal(row['postTime'])))\n",
    "    g.add((post_uri, SIOC.num_replies, rdflib.Literal(row['noOfUpvotes'])))\n",
    "    g.add((post_uri, SIOC.link, rdflib.URIRef(row['postUrl'])))\n",
    "    \n",
    "    g.add((subreddit_uri, rdflib.RDF.type, SIOC.Container))\n",
    "    g.add((subreddit_uri, SIOC.has_post, post_uri))\n",
    "    g.add((post_uri, SIOC.Container, subreddit_uri))\n",
    "\n",
    "    g.add((author_uri, rdflib.RDF.type, FOAF.Person))\n",
    "    g.add((author_uri, FOAF.name, rdflib.Literal(row['authorName'])))\n",
    "    g.add((post_uri, SIOC.has_creator, author_uri))\n",
    "\n",
    "    comment_uri = rdflib.URIRef(f\"http://reddit.com/comment/{index}\")\n",
    "    g.add((comment_uri, rdflib.RDF.type, SIOC.Comment))\n",
    "    g.add((comment_uri, DCMI.title, rdflib.Literal(row['comments'])))\n",
    "    g.add((post_uri, SIOC.has_reply, comment_uri))\n",
    "\n",
    "    g.add((topic_uri, SIOC.has_post, post_uri))\n",
    "\n",
    "# Group by subreddit and process each one for topic modeling\n",
    "GroupedData = DF.groupby('subReddit')\n",
    "all_topics = defaultdict(set)\n",
    "for subreddit, group in GroupedData:\n",
    "    print(f\"Processing Subreddit: {subreddit}\")\n",
    "    group['Combined'] = group['postTitle'] + group['postDesc']\n",
    "    \n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in custom_stopwords])\n",
    "    \n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "\n",
    "    LDA = gensim.models.LdaModel(\n",
    "        corpus,\n",
    "        num_topics=5,\n",
    "        id2word=dictionary,\n",
    "        passes=15,\n",
    "        iterations=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    for idx, topic in LDA.print_topics(num_words=5):\n",
    "        topic_words = topic.split(\" + \")\n",
    "        unique_words = {word.split(\"*\")[1].strip('\"') for word in topic_words}\n",
    "        all_topics[subreddit].update(unique_words)\n",
    "\n",
    "# Save RDF graph in Turtle and JSON-LD formats\n",
    "g.serialize('D:/FYP/Github/data-tails/Backend/Ontologies/KG1.ttl', format='turtle')\n",
    "json_ld = g.serialize(format='json-ld', indent=4)\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/KG1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_ld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddits and topics have been saved to Topics.csv.\n",
      "Filtered topics saved to D:/FYP/Github/data-tails/Backend/Ontologies/FTopics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save subreddits and topics to Topics.csv\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/Topics.csv\", mode='w', newline='', encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"subreddit\", \"topics\"])\n",
    "    for subreddit, topics in all_topics.items():\n",
    "        writer.writerow([subreddit, \", \".join(sorted(topics))])\n",
    "\n",
    "print(\"Subreddits and topics have been saved to Topics.csv.\")\n",
    "\n",
    "\n",
    "\n",
    "# List of subreddits to include in the new CSV\n",
    "selected_subreddits = [\n",
    "    \"todayilearned\", \"news\", \"mildlyinteresting\", \"AskReddit\", \"Damnthatsinteresting\", \n",
    "    \"showerthoughts\", \"wallstreetbets\", \"NatureIsFuckingLit\", \"LifeProTips\", \"space\", \n",
    "    \"MovieDetails\", \"tifu\", \"futurology\", \"Music\", \"europe\", \"unpopularopinion\", \n",
    "    \"pakistan\", \"islamabad\", \"lahore\", \"karachi\"\n",
    "]\n",
    "\n",
    "# Read the existing topics CSV\n",
    "topics_df = pd.read_csv(\"D:/FYP/Github/data-tails/Backend/Ontologies/Topics.csv\")\n",
    "\n",
    "# Filter rows where 'subreddit' is in the selected subreddits list\n",
    "filtered_topics_df = topics_df[topics_df['subreddit'].isin(selected_subreddits)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_topics_csv_path = \"D:/FYP/Github/data-tails/Backend/Ontologies/FTopics.csv\"\n",
    "filtered_topics_df.to_csv(filtered_topics_csv_path, index=False)\n",
    "\n",
    "print(f\"Filtered topics saved to {filtered_topics_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating GRAPH STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import json\n",
    "\n",
    "# RDF graph initialization\n",
    "g = rdflib.Graph()\n",
    "SIOC = rdflib.Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = rdflib.Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = rdflib.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "REDDIT = rdflib.Namespace(\"http://reddit.com/ns#\")  # Custom namespace for Reddit-specific relationships\n",
    "g.bind(\"sioc\", SIOC)\n",
    "g.bind(\"dc\", DCMI)\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"reddit\", REDDIT)\n",
    "\n",
    "\n",
    "# Function to add post data to RDF graph\n",
    "def add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri):\n",
    "    post_uri = URIRef(f\"http://reddit.com/post{index}\")\n",
    "    subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{row['subReddit']}\")\n",
    "    author_uri = URIRef(f\"http://reddit.com/user/{row['authorName']}\")\n",
    "    \n",
    "    # Add post properties and link to topic\n",
    "    g.add((post_uri, rdflib.RDF.type, SIOC.Post))\n",
    "    g.add((post_uri, DCMI.title, Literal(' '.join(row['postTitle']))))\n",
    "    g.add((post_uri, DCMI.description, Literal(' '.join(row['postDesc']))))\n",
    "    g.add((post_uri, DCMI.date, Literal(row['postTime'])))\n",
    "    g.add((post_uri, SIOC.num_replies, Literal(row['noOfUpvotes'])))\n",
    "    g.add((post_uri, SIOC.link, URIRef(row['postUrl'])))\n",
    "    g.add((post_uri, SIOC.NSFW, Literal(row['isNSFW'])))\n",
    "    g.add((post_uri, SIOC.has_type, post_type_uri))\n",
    "    g.add((post_uri, SIOC.topic, topic_uri))\n",
    "\n",
    "    # Create the relationships based on the list of relationship types\n",
    "    # CreatedBy: Indicates that a post is created by an author.\n",
    "    g.add((post_uri, REDDIT.CreatedBy, author_uri))\n",
    "\n",
    "    # BelongsTo: Links a post to a subreddit.\n",
    "    g.add((post_uri, REDDIT.BelongsTo, subreddit_uri))\n",
    "\n",
    "    # PartOfPost: Indicates that a comment is part of a post (the comment belongs to the post).\n",
    "    comment_uri = URIRef(f\"http://reddit.com/comment/{index}\")\n",
    "    g.add((comment_uri, REDDIT.PartOfPost, post_uri))\n",
    "\n",
    "    # HasType: Links an entity (post, comment) to its specific type (text post, link post, etc.).\n",
    "    g.add((post_uri, REDDIT.HasType, post_type_uri))\n",
    "    g.add((comment_uri, REDDIT.HasType, comment_type_uri))\n",
    "\n",
    "    # TaggedIn: Indicates that a post or comment is tagged with a specific topic or keyword.\n",
    "    for tag in row.get('tags', []):  # Assuming tags are available as a list\n",
    "        tag_uri = URIRef(f\"http://reddit.com/tag/{tag}\")\n",
    "        g.add((post_uri, REDDIT.TaggedIn, tag_uri))\n",
    "        g.add((comment_uri, REDDIT.TaggedIn, tag_uri))\n",
    "\n",
    "    # UpvotedBy: Indicates that a user has upvoted a post or comment.\n",
    "    if row.get('upvotedBy'):\n",
    "        for upvoter in row['upvotedBy']:\n",
    "            upvoter_uri = URIRef(f\"http://reddit.com/user/{upvoter}\")\n",
    "            g.add((post_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "\n",
    "    # DownvotedBy: Indicates that a user has downvoted a post or comment.\n",
    "    if row.get('downvotedBy'):\n",
    "        for downvoter in row['downvotedBy']:\n",
    "            downvoter_uri = URIRef(f\"http://reddit.com/user/{downvoter}\")\n",
    "            g.add((post_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "\n",
    "    # Moderates: Indicates that a user is a moderator of a subreddit.\n",
    "    if row.get('moderators'):\n",
    "        for moderator in row['moderators']:\n",
    "            moderator_uri = URIRef(f\"http://reddit.com/user/{moderator}\")\n",
    "            g.add((subreddit_uri, REDDIT.Moderates, moderator_uri))\n",
    "\n",
    "    # HasFlair: Links a post or comment to a specific flair (e.g., tags like \"Important\", \"Question\", etc.).\n",
    "    if row.get('flair'):\n",
    "        flair_uri = URIRef(f\"http://reddit.com/flair/{row['flair']}\")\n",
    "        g.add((post_uri, REDDIT.HasFlair, flair_uri))\n",
    "        g.add((comment_uri, REDDIT.HasFlair, flair_uri))\n",
    "\n",
    "    # Link post to subreddit and type\n",
    "    g.add((subreddit_uri, rdflib.RDF.type, SIOC.Container))\n",
    "    g.add((subreddit_uri, SIOC.has_post, post_uri))\n",
    "    g.add((subreddit_uri, SIOC.has_type, subreddit_type_uri))\n",
    "    g.add((post_uri, SIOC.Container, subreddit_uri))\n",
    "\n",
    "    # Link to author and author type\n",
    "    g.add((author_uri, rdflib.RDF.type, FOAF.Person))\n",
    "    g.add((author_uri, FOAF.name, Literal(row['authorName'])))\n",
    "    g.add((post_uri, SIOC.has_creator, author_uri))\n",
    "    g.add((author_uri, SIOC.has_type, author_type_uri))\n",
    "\n",
    "    # Add comments and link to comment type\n",
    "    g.add((comment_uri, rdflib.RDF.type, SIOC.Comment))\n",
    "    g.add((comment_uri, DCMI.title, Literal(row['comments'])))\n",
    "    g.add((post_uri, SIOC.has_reply, comment_uri))\n",
    "    g.add((comment_uri, SIOC.has_type, comment_type_uri))\n",
    "\n",
    "# Process each subreddit and create LDA models\n",
    "GroupedData = DF.groupby('subReddit')\n",
    "all_topics = defaultdict(set)\n",
    "\n",
    "for subreddit, group in GroupedData:\n",
    "    group['Combined'] = group['postTitle'] + group['postDesc']\n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in custom_stopwords])\n",
    "\n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "    LDA = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, iterations=100, random_state=42)\n",
    "    \n",
    "    for idx, topic in LDA.print_topics(num_words=5):\n",
    "        topic_uri = URIRef(f\"http://reddit.com/topic/{subreddit}_{idx}\")\n",
    "        topic_words = topic.split(\" + \")\n",
    "        unique_words = {word.split(\"*\")[1].strip('\"') for word in topic_words}\n",
    "        all_topics[subreddit].update(unique_words)\n",
    "\n",
    "        # Define URIs for types\n",
    "        subreddit_type_uri = URIRef(f\"http://reddit.com/type/subreddit/{subreddit}\")\n",
    "        post_type_uri = URIRef(f\"http://reddit.com/type/post/{subreddit}\")\n",
    "        comment_type_uri = URIRef(f\"http://reddit.com/type/comment/{subreddit}\")\n",
    "        author_type_uri = URIRef(f\"http://reddit.com/type/author/{subreddit}\")\n",
    "\n",
    "        # Link subreddit to topic\n",
    "        subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{subreddit}\")\n",
    "        g.add((subreddit_uri, SIOC.has_topic, topic_uri))\n",
    "        g.add((topic_uri, rdflib.RDF.type, SIOC.Topic))\n",
    "\n",
    "        # Iterate through each row in the group\n",
    "        for index, row in group.iterrows():\n",
    "            add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri)\n",
    "\n",
    "# Save graph\n",
    "g.serialize('D:/FYP/Github/data-tails/Backend/Ontologies/KG.ttl', format='turtle')\n",
    "json_ld = g.serialize(format='json-ld', indent=4)\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/KG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Loading Knowledge Graphs...\n",
      "‚úÖ Loaded KG.json with 63763 entities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x0000025BC9088820>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\isodate\\isodatetime.py\", line 55, in parse_datetime\n",
      "    tmpdate = parse_date(datestring)\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\isodate\\isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: 'Na'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded KG.ttl with 648909 triples.\n",
      "\n",
      "üîç Retrieving Relevant Comments...\n",
      "\n",
      "ü§ñ Querying Groq...\n",
      "\n",
      "üí° Groq Response: OCD (Obsessive-Compulsive Disorder) is a complex condition that is not yet fully understood, but research has shed light on some of the possible causes and contributing factors. Here are some of the key theories:\n",
      "\n",
      "1. **Genetics**: OCD tends to run in families, suggesting a possible genetic component. Researchers have identified several genes that may contribute to an increased risk of developing OCD.\n",
      "2. **Neurochemistry**: OCD is often linked to imbalances in neurotransmitters, such as serotonin, dopamine, and glutamate, which play a crucial role in brain function and behavior. Abnormalities in these chemicals may contribute to the development of OCD symptoms.\n",
      "3. **Brain structure and function**: Studies have found differences in brain structure and function in individuals with OCD, including altered activity in areas responsible for emotional regulation, decision-making, and impulse control.\n",
      "4. **Environmental factors**: Experiencing traumatic or stressful events, such as childhood abuse or neglect, may increase the risk of developing OCD.\n",
      "5. **Brain development**: Some research suggests that OCD may be linked to disruptions in brain development during childhood and adolescence, particularly in areas such as the anterior cingulate cortex and the caudate nucleus.\n",
      "6. **Viral infections**: In some cases, OCD has been linked to viral infections, such as streptococcal throat infections (PANDAS) or other infections that trigger an autoimmune response.\n",
      "7. **Hormonal influences**: Hormonal fluctuations, particularly during puberty, may contribute to the development of OCD.\n",
      "8. **Psychological factors**: Learning and-conditioning theories suggest that OCD symptoms may be acquired through classical conditioning, where an individual learns to associate certain stimuli or situations with anxiety and develops compulsive behaviors as a way to cope with that anxiety.\n",
      "9. **Social and cultural factors**: Societal and cultural factors, such as exposure to OCD-related information or pressure to conform to certain standards of cleanliness or perfection, may contribute to the development of OCD symptoms.\n",
      "\n",
      "While these factors are thought to play a role in the development of OCD, it's important to note that OCD is a complex condition, and the exact causes and mechanisms are still not fully understood. Research continues to uncover new insights into the causes and treatments of OCD.\n"
     ]
    }
   ],
   "source": [
    "import rdflib\n",
    "from rdflib import Graph, Namespace, Literal\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import subprocess\n",
    "import re\n",
    "from isodate import parse_datetime, ISO8601Error\n",
    "\n",
    "# Namespaces\n",
    "SIOC = Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "reddit = Namespace(\"http://reddit.com/\")\n",
    "\n",
    "# Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "def clean_dates(graph):\n",
    "    for s, p, o in graph.triples((None, DCMI.date, None)):\n",
    "        if isinstance(o, Literal):\n",
    "            try:\n",
    "                parse_datetime(str(o))  # Validate\n",
    "            except ISO8601Error:\n",
    "                graph.remove((s, p, o))\n",
    "                graph.add((s, p, Literal(\"1970-01-01T00:00:00\", datatype=SIOC.date)))\n",
    "\n",
    "def retrieve_expanded_context(graph, user_query, depth=2):\n",
    "    sparql_query = f\"\"\"\n",
    "    SELECT ?subject ?predicate ?object\n",
    "    WHERE {{\n",
    "        ?subject ?predicate ?object .\n",
    "        OPTIONAL {{ ?subject <{DCMI.title}> ?title }}\n",
    "        OPTIONAL {{ ?subject <{DCMI.description}> ?description }}\n",
    "        FILTER(\n",
    "            CONTAINS(LCASE(STR(?title)), LCASE(\"{user_query}\")) || \n",
    "            CONTAINS(LCASE(STR(?description)), LCASE(\"{user_query}\"))\n",
    "        )\n",
    "    }}\n",
    "    LIMIT {depth * 10}\n",
    "    \"\"\"\n",
    "    results = graph.query(sparql_query)\n",
    "    return \"\\n\".join([f\"{s} --[{p}]--> {o}\" for s, p, o in results])\n",
    "\n",
    "def query_wizardlm2(context, user_query):\n",
    "    prompt = f\"Context:\\n{context}\\n\\nQuestion:\\n{user_query}\\n\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"wizardlm2\"],\n",
    "            input=prompt.encode(\"utf-8\"),  # Encode input\n",
    "            capture_output=True\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(result.stderr.decode(\"utf-8\"))\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error querying WizardLM2: {str(e)}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    kg_file = \"./KG.ttl\"\n",
    "    kg = Graph()\n",
    "    kg.parse(kg_file, format=\"turtle\")\n",
    "    clean_dates(kg)\n",
    "\n",
    "    user_query = \"What are the causes of OCD?\"\n",
    "    context = retrieve_expanded_context(kg, user_query, depth=3)\n",
    "    print(f\"Retrieved Context:\\n{context}\")\n",
    "\n",
    "    response = query_wizardlm2(context, user_query)\n",
    "    print(f\"\\nLLM Response:\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Recommedation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Query: What are the most common fears among people?\n",
      "üí¨ Response: A recent poll on r/AskReddit found that 42% of users fear failure, 35% fear public speaking, and 23% fear being alone. Spiders, heights, and drowning were also frequently mentioned.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('bar_chart', 0.56), ('line_chart', 0.49), ('heatmap', 0.29)]\n",
      "\n",
      "üí¨ Query: What are Reddit users‚Äô favorite fast-food chains?\n",
      "üí¨ Response: Based on a survey in r/AskReddit, 48% of users ranked Chick-fil-A as their favorite, 30% preferred McDonald's, and 22% chose Taco Bell. Reasons included food quality, affordability, and speed.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('bar_chart', 2.02), ('chord_diagram', 0.62), ('line_chart', 0.58)]\n",
      "\n",
      "üí¨ Query: What are the latest updates on the Ukraine-Russia conflict?\n",
      "üí¨ Response: Recent reports from r/worldnews indicate increased military activity along the eastern border. A UN spokesperson confirmed ongoing peace talks, but tensions remain high.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('line_chart', 1.32), ('area_chart', 0.67), ('bar_chart', 0.64)]\n",
      "\n",
      "üí¨ Query: Has there been a major natural disaster recently?\n",
      "üí¨ Response: According to r/worldnews, a Category 4 hurricane made landfall in Florida, causing severe flooding and power outages for over 1 million residents. Relief efforts are underway.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('maps', 0.85), ('connection_map', 0.73), ('line_chart', 0.58)]\n",
      "\n",
      "üí¨ Query: What are the latest advancements in artificial intelligence?\n",
      "üí¨ Response: A new AI model, GPT-5, was announced in r/technology. It boasts improved reasoning, coding capabilities, and multi-modal integration, making it one of the most advanced LLMs to date.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('bar_chart', 0.74), ('line_chart', 0.68), ('heatmap', 0.38)]\n",
      "\n",
      "üí¨ Query: What‚Äôs the most powerful consumer GPU available?\n",
      "üí¨ Response: A benchmark test shared in r/technology shows that the NVIDIA RTX 4090 outperforms all competitors, with a 15% lead over the AMD Radeon RX 7900 XTX in 4K gaming.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('bar_chart', 0.68), ('line_chart', 0.63), ('heatmap', 0.46)]\n",
      "\n",
      "üí¨ Query: What new medical breakthroughs have been announced?\n",
      "üí¨ Response: Researchers in r/science have developed a new cancer treatment that increases survival rates by 40%. The therapy targets aggressive tumor cells while minimizing side effects.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('line_chart', 1.25), ('bar_chart', 0.73), ('area_chart', 0.6)]\n",
      "\n",
      "üí¨ Query: Has any new planet been discovered?\n",
      "üí¨ Response: A post in r/science confirms that astronomers using the James Webb Telescope have discovered an Earth-like exoplanet with possible signs of liquid water.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('line_chart', 0.58), ('bar_chart', 0.56), ('heatmap', 0.4)]\n",
      "\n",
      "üí¨ Query: How are global leaders responding to climate change?\n",
      "üí¨ Response: In r/politics, reports indicate that several world leaders at the UN Climate Summit pledged to reduce carbon emissions by 30% by 2030, with mixed reactions from environmental groups.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('line_chart', 1.2), ('bar_chart', 0.66), ('area_chart', 0.58)]\n",
      "\n",
      "üí¨ Query: What are the latest updates on the U.S. elections?\n",
      "üí¨ Response: Recent discussions in r/politics suggest that the frontrunners in the U.S. election are gaining traction among young voters, with debates heating up over key policy issues.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('line_chart', 0.74), ('bar_chart', 0.68), ('heatmap', 0.41)]\n",
      "\n",
      "üí¨ Query: What are the most-played video games in 2024?\n",
      "üí¨ Response: According to a poll in r/gaming, the top 3 most-played games are 'Elden Ring DLC,' 'Baldur‚Äôs Gate 3,' and 'Call of Duty: Modern Warfare III.' Online multiplayer games remain dominant.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('bar_chart', 1.37), ('line_chart', 0.67), ('sorted_heatmap', 0.64)]\n",
      "\n",
      "üí¨ Query: Are microtransactions still a problem in gaming?\n",
      "üí¨ Response: A heated debate on r/gaming revealed that 65% of players feel microtransactions ruin the gaming experience, while 35% believe they are acceptable in free-to-play models.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('bar_chart', 0.6), ('line_chart', 0.53), ('heatmap', 0.32)]\n",
      "\n",
      "üí¨ Query: What‚Äôs the next big space mission?\n",
      "üí¨ Response: According to r/space, NASA's Artemis II mission is set to send astronauts around the Moon next year, marking a significant step toward future Mars exploration.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('bar_chart', 0.61), ('line_chart', 0.57), ('heatmap', 0.34)]\n",
      "\n",
      "üí¨ Query: Has there been any recent asteroid impact?\n",
      "üí¨ Response: Scientists in r/space confirmed that a small asteroid entered Earth‚Äôs atmosphere over the Pacific Ocean, burning up harmlessly before reaching the surface.\n",
      "‚úÖ Top 3 Recommended Visualizations: [('bar_chart', 0.56), ('line_chart', 0.52), ('heatmap', 0.32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load NLP models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define visualization categories conceptually\n",
    "chart_types = {\n",
    "    \"bar_chart\": \"Used for comparing categories or ranking values.\",\n",
    "    \"line_chart\": \"Best for showing trends over time or sequential data.\",\n",
    "    \"area_chart\": \"Shows cumulative data trends with a filled area.\",\n",
    "    \"scatterplot\": \"Visualizes relationships or correlations between two variables.\",\n",
    "    \"density_facet\": \"Represents distribution patterns across multiple facets.\",\n",
    "    \"gradient_encoding\": \"Indicates intensity or magnitude with color gradients.\",\n",
    "    \"candlestick_chart\": \"Commonly used for stock market trends and price movements.\",\n",
    "    \"stacked_normalized_area_chart\": \"Shows part-to-whole relationships over time.\",\n",
    "    \"circle_packing\": \"Represents hierarchical relationships in a compact form.\",\n",
    "    \"dendrogram\": \"Illustrates hierarchical clustering or tree structures.\",\n",
    "    \"DAG\": \"Shows directed relationships, commonly used for processes or networks.\",\n",
    "    \"treemap\": \"Depicts hierarchical structures using nested rectangles.\",\n",
    "    \"chord_diagram\": \"Best for visualizing relationships and interactions.\",\n",
    "    \"heatmap\": \"Displays intensity values in a matrix format.\",\n",
    "    \"connection_map\": \"Visualizes spatial relationships and geographic data.\",\n",
    "    \"maps\": \"Represents geographic data with location-based insights.\",\n",
    "    \"map_small_multiples\": \"Shows multiple maps for comparison.\",\n",
    "    \"hexbin_map\": \"Aggregates spatial data into hexagonal bins.\",\n",
    "    \"centerline_labelling\": \"Enhances map readability with labeled centerlines.\",\n",
    "    \"voronoi_map\": \"Divides spatial regions based on distance.\",\n",
    "    \"sorted_heatmap\": \"Ranks and sorts data in a matrix format.\"\n",
    "}\n",
    "\n",
    "# Precompute chart category embeddings\n",
    "category_embeddings = {\n",
    "    chart: model.encode(description, normalize_embeddings=True)\n",
    "    for chart, description in chart_types.items()\n",
    "}\n",
    "\n",
    "# Function to extract features from both the query and response\n",
    "def extract_features(query, response):\n",
    "    \"\"\"Extracts numerical data, locations, trends, and relationships from text.\"\"\"\n",
    "    combined_text = query + \" \" + response\n",
    "    doc = nlp(combined_text)\n",
    "\n",
    "    numbers = [token.text for token in doc if token.like_num]\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\"}]\n",
    "\n",
    "    trend_keywords = {\"increase\", \"decline\", \"growth\", \"rise\", \"drop\", \"trend\", \"fall\", \"reduce\", \"expansion\"}\n",
    "    trends = [token.lemma_ for token in doc if token.lemma_ in trend_keywords]\n",
    "\n",
    "    relationship_keywords = {\"prefer\", \"dominate\", \"compared\", \"versus\", \"majority\", \"minority\"}\n",
    "    relationships = [token.lemma_ for token in doc if token.lemma_ in relationship_keywords]\n",
    "\n",
    "    ranking_keywords = {\"ranking\", \"top\", \"best\", \"percent\", \"favorite\", \"most popular\"}\n",
    "    rankings = [token.lemma_ for token in doc if token.lemma_ in ranking_keywords]\n",
    "\n",
    "    geo_terms = {\"earthquake\", \"magnitude\", \"epicenter\", \"seismic\", \"hurricane\", \"flood\", \"storm\"}\n",
    "    geo_features = [token.lemma_ for token in doc if token.lemma_ in geo_terms]\n",
    "\n",
    "    keywords = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    return {\n",
    "        \"numbers\": numbers,\n",
    "        \"locations\": locations,\n",
    "        \"trends\": trends,\n",
    "        \"relationships\": relationships,\n",
    "        \"geo_features\": geo_features,\n",
    "        \"rankings\": rankings,\n",
    "        \"keywords\": keywords\n",
    "    }\n",
    "\n",
    "# Adjust similarity scores based on extracted features\n",
    "def boost_scores(scores, response_type, features):\n",
    "    \"\"\"Dynamically boosts scores based on response type and detected keywords.\"\"\"\n",
    "    boosts = {\n",
    "        \"numeric\": {\"bar_chart\": 0.6, \"line_chart\": 0.5, \"heatmap\": 0.4},\n",
    "        \"geographic\": {\"maps\": 0.7, \"connection_map\": 0.6, \"hexbin_map\": 0.5, \"voronoi_map\": 0.5},\n",
    "        \"textual\": {\"scatterplot\": 0.2, \"DAG\": 0.2, \"chord_diagram\": 0.2}\n",
    "    }\n",
    "\n",
    "    if response_type in boosts:\n",
    "        for chart, boost in boosts[response_type].items():\n",
    "            scores[chart] += boost\n",
    "\n",
    "    if features[\"trends\"]:\n",
    "        scores[\"line_chart\"] += 0.6\n",
    "        scores[\"area_chart\"] += 0.5\n",
    "\n",
    "    if features[\"relationships\"]:\n",
    "        scores[\"bar_chart\"] += 0.6\n",
    "        scores[\"chord_diagram\"] += 0.5\n",
    "\n",
    "    if features[\"geo_features\"]:\n",
    "        scores[\"maps\"] += 0.7\n",
    "        scores[\"connection_map\"] += 0.6\n",
    "        scores[\"hexbin_map\"] += 0.5\n",
    "\n",
    "    if features[\"rankings\"]:\n",
    "        scores[\"bar_chart\"] += 0.7\n",
    "        scores[\"sorted_heatmap\"] += 0.5\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Function to recommend visualizations\n",
    "def recommend_visualizations(query, response):\n",
    "    \"\"\"Recommends the most relevant visualizations based on query and response context.\"\"\"\n",
    "    features = extract_features(query, response)\n",
    "    response_embedding = model.encode(query + \" \" + response, normalize_embeddings=True)\n",
    "\n",
    "    similarity_scores = {\n",
    "        chart: cosine_similarity([response_embedding], [embedding]).flatten()[0]\n",
    "        for chart, embedding in category_embeddings.items()\n",
    "    }\n",
    "\n",
    "    similarity_scores = boost_scores(similarity_scores, \"numeric\", features)\n",
    "\n",
    "    top_recommendations = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    return [(chart, round(score, 2)) for chart, score in top_recommendations]\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "queries_responses = [\n",
    "    # r/AskReddit - Opinion-based and personal experiences\n",
    "    (\"What are the most common fears among people?\", \n",
    "     \"A recent poll on r/AskReddit found that 42% of users fear failure, 35% fear public speaking, and 23% fear being alone. Spiders, heights, and drowning were also frequently mentioned.\"),\n",
    "\n",
    "    (\"What are Reddit users‚Äô favorite fast-food chains?\", \n",
    "     \"Based on a survey in r/AskReddit, 48% of users ranked Chick-fil-A as their favorite, 30% preferred McDonald's, and 22% chose Taco Bell. Reasons included food quality, affordability, and speed.\"),\n",
    "\n",
    "    # r/worldnews - Global events & breaking news\n",
    "    (\"What are the latest updates on the Ukraine-Russia conflict?\", \n",
    "     \"Recent reports from r/worldnews indicate increased military activity along the eastern border. A UN spokesperson confirmed ongoing peace talks, but tensions remain high.\"),\n",
    "\n",
    "    (\"Has there been a major natural disaster recently?\", \n",
    "     \"According to r/worldnews, a Category 4 hurricane made landfall in Florida, causing severe flooding and power outages for over 1 million residents. Relief efforts are underway.\"),\n",
    "\n",
    "    # r/technology - Trends in AI, software, and hardware\n",
    "    (\"What are the latest advancements in artificial intelligence?\", \n",
    "     \"A new AI model, GPT-5, was announced in r/technology. It boasts improved reasoning, coding capabilities, and multi-modal integration, making it one of the most advanced LLMs to date.\"),\n",
    "\n",
    "    (\"What‚Äôs the most powerful consumer GPU available?\", \n",
    "     \"A benchmark test shared in r/technology shows that the NVIDIA RTX 4090 outperforms all competitors, with a 15% lead over the AMD Radeon RX 7900 XTX in 4K gaming.\"),\n",
    "\n",
    "    # r/science - Research & discoveries\n",
    "    (\"What new medical breakthroughs have been announced?\", \n",
    "     \"Researchers in r/science have developed a new cancer treatment that increases survival rates by 40%. The therapy targets aggressive tumor cells while minimizing side effects.\"),\n",
    "\n",
    "    (\"Has any new planet been discovered?\", \n",
    "     \"A post in r/science confirms that astronomers using the James Webb Telescope have discovered an Earth-like exoplanet with possible signs of liquid water.\"),\n",
    "\n",
    "    # r/politics - Government policies & elections\n",
    "    (\"How are global leaders responding to climate change?\", \n",
    "     \"In r/politics, reports indicate that several world leaders at the UN Climate Summit pledged to reduce carbon emissions by 30% by 2030, with mixed reactions from environmental groups.\"),\n",
    "\n",
    "    (\"What are the latest updates on the U.S. elections?\", \n",
    "     \"Recent discussions in r/politics suggest that the frontrunners in the U.S. election are gaining traction among young voters, with debates heating up over key policy issues.\"),\n",
    "\n",
    "    # r/gaming - Video game trends & preferences\n",
    "    (\"What are the most-played video games in 2024?\", \n",
    "     \"According to a poll in r/gaming, the top 3 most-played games are 'Elden Ring DLC,' 'Baldur‚Äôs Gate 3,' and 'Call of Duty: Modern Warfare III.' Online multiplayer games remain dominant.\"),\n",
    "\n",
    "    (\"Are microtransactions still a problem in gaming?\", \n",
    "     \"A heated debate on r/gaming revealed that 65% of players feel microtransactions ruin the gaming experience, while 35% believe they are acceptable in free-to-play models.\"),\n",
    "\n",
    "    # r/space - Astronomy & exploration\n",
    "    (\"What‚Äôs the next big space mission?\", \n",
    "     \"According to r/space, NASA's Artemis II mission is set to send astronauts around the Moon next year, marking a significant step toward future Mars exploration.\"),\n",
    "\n",
    "    (\"Has there been any recent asteroid impact?\", \n",
    "     \"Scientists in r/space confirmed that a small asteroid entered Earth‚Äôs atmosphere over the Pacific Ocean, burning up harmlessly before reaching the surface.\"),\n",
    "]\n",
    "\n",
    "\n",
    "for query, response in queries_responses:\n",
    "    recommended_charts = recommend_visualizations(query, response)\n",
    "    print(f\"\\nüí¨ Query: {query}\")\n",
    "    print(f\"üí¨ Response: {response}\")\n",
    "    print(f\"‚úÖ Top 3 Recommended Visualizations: {recommended_charts}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
