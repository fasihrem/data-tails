{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-igraph in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.6)\n",
      "Requirement already satisfied: igraph==0.11.6 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-igraph) (0.11.6)\n",
      "Requirement already satisfied: texttable>=1.6.2 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from igraph==0.11.6->python-igraph) (1.7.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvis in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (8.9.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.3.0)\n",
      "Requirement already satisfied: networkx>=1.11 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyvis) (3.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.30 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (5.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from ipython>=5.3.0->pyvis) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2>=2.9.6->pyvis) (2.1.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.30->ipython>=5.3.0->pyvis) (0.2.12)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\97156\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\97156\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install python-igraph\n",
    "!pip install pyvis\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import rdflib\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        _id  type  \\\n",
      "0  66e965e698330736e0d693d5  None   \n",
      "1  66e965e698330736e0d693d6  None   \n",
      "2  66e965e698330736e0d693d7  None   \n",
      "3  66e965e698330736e0d693d8  None   \n",
      "4  66e965e698330736e0d693d9  None   \n",
      "\n",
      "                                           postTitle postDesc  \\\n",
      "0  Adults(especially those over 30), how young do...      NaN   \n",
      "1  What is a thing that your parents consider nor...      NaN   \n",
      "2                 What is a smell that comforts you?      NaN   \n",
      "3  When in history was it the best time to be a w...      NaN   \n",
      "4    What's the worst way someone broke up with you?      NaN   \n",
      "\n",
      "              postTime            authorName noOfUpvotes isNSFW  \\\n",
      "0  2024-08-06 01:02:35  Excellent-Studio7257        4068  False   \n",
      "1  2024-08-06 01:47:22        Bigbumoffhappy        2073  False   \n",
      "2  2024-08-05 22:21:53         bloomsmittenn        2188  False   \n",
      "3  2024-08-06 03:32:59   More_food_please_77         778  False   \n",
      "4  2024-08-05 21:01:39    ImpressiveWrap7363        1989  False   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  [It's weird. I can have an adult/intellectual ...        5590.0   \n",
      "1  [Thinking you're supposed to raise your kids t...        1684.0   \n",
      "2  [The weather in the woods after a heavy rain, ...        4958.0   \n",
      "3  [It might depend on where you are but now with...        1046.0   \n",
      "4  [I have 2 bad ones.  First one, he had his mom...        1749.0   \n",
      "\n",
      "                                            imageUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                             postUrl  subReddit  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...  AskReddit  \n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client['DataTails']\n",
    "collection = db['Data']  \n",
    "data_cursor = collection.find({})\n",
    "DF = pd.DataFrame(list(data_cursor))\n",
    "print(DF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "StopWords = set(stopwords.words('english'))\n",
    "def Preprocessing(text):\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in StopWords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "Cols = ['subReddit', 'postTitle', 'postDesc', 'postTime', 'authorName', 'noOfUpvotes', 'comments', 'noOfComments', 'postUrl']\n",
    "DF = DF[Cols]\n",
    "print(DF.isnull().sum())\n",
    "DF['postDesc'].fillna('', inplace=True)\n",
    "DF['noOfUpvotes'].fillna(0, inplace=True)\n",
    "DF['postTime'] = pd.to_datetime(DF['postTime'], errors='coerce')\n",
    "DF['comments'] = DF['comments'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "DF['postTitle'] = DF['postTitle'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['postDesc'] = DF['postDesc'].apply(lambda x: Preprocessing(str(x)))\n",
    "print(DF.head())\n",
    "print(DF.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GroupedData = DF.groupby('subReddit')\n",
    "SubReddits = defaultdict(dict)\n",
    "for subreddit, group in GroupedData:\n",
    "    print(f\"Processing Subreddit: {subreddit}\")\n",
    "    group['Combined'] = group['postTitle'] + \" \" + group['postDesc']\n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: word_tokenize(x))\n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "    LDA = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "    SubReddits[subreddit]['lda_model'] = LDA\n",
    "    SubReddits[subreddit]['dictionary'] = dictionary\n",
    "    SubReddits[subreddit]['corpus'] = corpus\n",
    "    group['topics'] = [LDA.get_document_topics(bow) for bow in corpus]\n",
    "    DF.loc[group.index, 'topics'] = group['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF.shape)\n",
    "print(DF.describe())\n",
    "print(DF.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subReddit           2\n",
      "postTitle           0\n",
      "postDesc        15607\n",
      "postTime            0\n",
      "authorName        587\n",
      "noOfUpvotes         0\n",
      "comments            0\n",
      "noOfComments        2\n",
      "postUrl             2\n",
      "imageUrl            3\n",
      "isNSFW              1\n",
      "dtype: int64\n",
      "   subReddit                                          postTitle postDesc  \\\n",
      "0  AskReddit                   [adult, especially, young, seem]       []   \n",
      "1  AskReddit  [parent, consider, normal, consider, normal, a...       []   \n",
      "2  AskReddit                                   [smell, comfort]       []   \n",
      "3  AskReddit                       [history, best, time, woman]       []   \n",
      "4  AskReddit                       [worst, way, someone, broke]       []   \n",
      "\n",
      "             postTime            authorName noOfUpvotes  \\\n",
      "0 2024-08-06 01:02:35  Excellent-Studio7257        4068   \n",
      "1 2024-08-06 01:47:22        Bigbumoffhappy        2073   \n",
      "2 2024-08-05 22:21:53         bloomsmittenn        2188   \n",
      "3 2024-08-06 03:32:59   More_food_please_77         778   \n",
      "4 2024-08-05 21:01:39    ImpressiveWrap7363        1989   \n",
      "\n",
      "                                            comments  noOfComments  \\\n",
      "0  It's weird. I can have an adult/intellectual c...        5590.0   \n",
      "1  Thinking you're supposed to raise your kids th...        1684.0   \n",
      "2  The weather in the woods after a heavy rain Ne...        4958.0   \n",
      "3  It might depend on where you are but now with ...        1046.0   \n",
      "4  I have 2 bad ones.  First one, he had his mom ...        1749.0   \n",
      "\n",
      "                                             postUrl  \\\n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   \n",
      "\n",
      "                                            imageUrl  isNSFW  \n",
      "0  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "1  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "2  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "3  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "4  https://www.reddit.com/r/AskReddit/comments/1e...   False  \n",
      "subReddit       0\n",
      "postTitle       0\n",
      "postDesc        0\n",
      "postTime        1\n",
      "authorName      0\n",
      "noOfUpvotes     0\n",
      "comments        0\n",
      "noOfComments    0\n",
      "postUrl         0\n",
      "imageUrl        0\n",
      "isNSFW          0\n",
      "dtype: int64\n",
      "Processing Subreddit: Advice\n",
      "Processing Subreddit: AmItheAsshole\n",
      "Processing Subreddit: AskReddit\n",
      "Processing Subreddit: Damnthatsinteresting\n",
      "Processing Subreddit: Filmmakers\n",
      "Processing Subreddit: Jokes\n",
      "Processing Subreddit: Music\n",
      "Processing Subreddit: NoStupidQuestions\n",
      "Processing Subreddit: Showerthoughts\n",
      "Processing Subreddit: Unknown_SubReddit\n",
      "Processing Subreddit: askscience\n",
      "Processing Subreddit: aww\n",
      "Processing Subreddit: books\n",
      "Processing Subreddit: funny\n",
      "Processing Subreddit: gadgets\n",
      "Processing Subreddit: gaming\n",
      "Processing Subreddit: help\n",
      "Processing Subreddit: islamabad\n",
      "Processing Subreddit: memes\n",
      "Processing Subreddit: mildlyinteresting\n",
      "Processing Subreddit: movies\n",
      "Processing Subreddit: news\n",
      "Processing Subreddit: olympics\n",
      "Processing Subreddit: pakistan\n",
      "Processing Subreddit: pics\n",
      "Processing Subreddit: politics\n",
      "Processing Subreddit: programming\n",
      "Processing Subreddit: science\n",
      "Processing Subreddit: showerthoughts\n",
      "Processing Subreddit: socialmedia\n",
      "Processing Subreddit: sports\n",
      "Processing Subreddit: technology\n",
      "Processing Subreddit: todayilearned\n",
      "Processing Subreddit: videos\n",
      "Processing Subreddit: wallstreetbets\n",
      "Processing Subreddit: worldnews\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "StopWords = set(stopwords.words('english'))\n",
    "custom_stopwords = StopWords | {\"make\", \"thing\", \"know\", \"http\", \"get\", \"want\", \"like\", \"would\", \"could\", \"you\", \"say\",\"also\",\"aita\",\"com\",\"www\",\"made\",\"ago\",\"day\",\"000\"}\n",
    "def Preprocessing(text):\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'\\b\\w{1,2}\\b', '', text)  # Remove short words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in custom_stopwords and len(word) > 2]\n",
    "    return tokens\n",
    "\n",
    "Cols = ['subReddit', 'postTitle', 'postDesc', 'postTime', 'authorName', 'noOfUpvotes', 'comments', 'noOfComments', 'postUrl','imageUrl','isNSFW']\n",
    "DF = DF[Cols]\n",
    "print(DF.isnull().sum())\n",
    "\n",
    "\n",
    "DF['subReddit'] = DF['subReddit'].fillna('Unknown_SubReddit')\n",
    "DF['authorName'] = DF['authorName'].fillna('Unknown_Author')\n",
    "DF['postTitle'] = DF['postTitle'].fillna('Untitled')\n",
    "DF['postUrl'] = DF['postUrl'].fillna('http://example.com/NOPOST.png')\n",
    "DF['imageUrl'] = DF['imageUrl'].fillna('http://example.com/NOImage.png')\n",
    "DF['isNSFW'] = DF['isNSFW'].fillna(False)\n",
    "DF['postDesc'].fillna('', inplace=True)\n",
    "DF['noOfUpvotes'].fillna(0, inplace=True)\n",
    "DF['noOfComments'].fillna(0, inplace=True)\n",
    "DF['postTime'] = pd.to_datetime(DF['postTime'], errors='coerce')\n",
    "DF['comments'] = DF['comments'].apply(lambda x: ' '.join(x) if isinstance(x, list) else str(x))\n",
    "DF['postTitle'] = DF['postTitle'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['postDesc'] = DF['postDesc'].apply(lambda x: Preprocessing(str(x)))\n",
    "DF['isNSFW'] = DF['isNSFW'].astype(bool)\n",
    "print(DF.head())\n",
    "print(DF.isnull().sum())\n",
    "\n",
    "# Initialize graph and namespaces\n",
    "g = rdflib.Graph()\n",
    "SIOC = rdflib.Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = rdflib.Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = rdflib.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "g.bind(\"sioc\", SIOC)\n",
    "g.bind(\"dc\", DCMI)\n",
    "g.bind(\"foaf\", FOAF)\n",
    "\n",
    "# Function to add posts to the graph\n",
    "def add_post_to_graph(row, index, topic_uri):\n",
    "    post_uri = rdflib.URIRef(f\"http://reddit.com/post{index}\")\n",
    "    subreddit_uri = rdflib.URIRef(f\"http://reddit.com/subreddit/{row['subReddit']}\")\n",
    "    author_uri = rdflib.URIRef(f\"http://reddit.com/user/{row['authorName']}\")\n",
    "    \n",
    "    g.add((post_uri, rdflib.RDF.type, SIOC.Post))\n",
    "    g.add((post_uri, DCMI.title, rdflib.Literal(row['postTitle'])))\n",
    "    g.add((post_uri, DCMI.description, rdflib.Literal(row['postDesc'])))\n",
    "    g.add((post_uri, DCMI.date, rdflib.Literal(row['postTime'])))\n",
    "    g.add((post_uri, SIOC.num_replies, rdflib.Literal(row['noOfUpvotes'])))\n",
    "    g.add((post_uri, SIOC.link, rdflib.URIRef(row['postUrl'])))\n",
    "    \n",
    "    g.add((subreddit_uri, rdflib.RDF.type, SIOC.Container))\n",
    "    g.add((subreddit_uri, SIOC.has_post, post_uri))\n",
    "    g.add((post_uri, SIOC.Container, subreddit_uri))\n",
    "\n",
    "    g.add((author_uri, rdflib.RDF.type, FOAF.Person))\n",
    "    g.add((author_uri, FOAF.name, rdflib.Literal(row['authorName'])))\n",
    "    g.add((post_uri, SIOC.has_creator, author_uri))\n",
    "\n",
    "    comment_uri = rdflib.URIRef(f\"http://reddit.com/comment/{index}\")\n",
    "    g.add((comment_uri, rdflib.RDF.type, SIOC.Comment))\n",
    "    g.add((comment_uri, DCMI.title, rdflib.Literal(row['comments'])))\n",
    "    g.add((post_uri, SIOC.has_reply, comment_uri))\n",
    "\n",
    "    g.add((topic_uri, SIOC.has_post, post_uri))\n",
    "\n",
    "# Group by subreddit and process each one for topic modeling\n",
    "GroupedData = DF.groupby('subReddit')\n",
    "all_topics = defaultdict(set)\n",
    "for subreddit, group in GroupedData:\n",
    "    print(f\"Processing Subreddit: {subreddit}\")\n",
    "    group['Combined'] = group['postTitle'] + group['postDesc']\n",
    "    \n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in custom_stopwords])\n",
    "    \n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "\n",
    "    LDA = gensim.models.LdaModel(\n",
    "        corpus,\n",
    "        num_topics=5,\n",
    "        id2word=dictionary,\n",
    "        passes=15,\n",
    "        iterations=100,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    for idx, topic in LDA.print_topics(num_words=5):\n",
    "        topic_words = topic.split(\" + \")\n",
    "        unique_words = {word.split(\"*\")[1].strip('\"') for word in topic_words}\n",
    "        all_topics[subreddit].update(unique_words)\n",
    "\n",
    "# Save RDF graph in Turtle and JSON-LD formats\n",
    "g.serialize('D:/FYP/Github/data-tails/Backend/Ontologies/KG1.ttl', format='turtle')\n",
    "json_ld = g.serialize(format='json-ld', indent=4)\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/KG1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_ld)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subreddits and topics have been saved to Topics.csv.\n",
      "Filtered topics saved to D:/FYP/Github/data-tails/Backend/Ontologies/FTopics.csv\n"
     ]
    }
   ],
   "source": [
    "# Save subreddits and topics to Topics.csv\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/Topics.csv\", mode='w', newline='', encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"subreddit\", \"topics\"])\n",
    "    for subreddit, topics in all_topics.items():\n",
    "        writer.writerow([subreddit, \", \".join(sorted(topics))])\n",
    "\n",
    "print(\"Subreddits and topics have been saved to Topics.csv.\")\n",
    "\n",
    "\n",
    "\n",
    "# List of subreddits to include in the new CSV\n",
    "selected_subreddits = [\n",
    "    \"todayilearned\", \"news\", \"mildlyinteresting\", \"AskReddit\", \"Damnthatsinteresting\", \n",
    "    \"showerthoughts\", \"wallstreetbets\", \"NatureIsFuckingLit\", \"LifeProTips\", \"space\", \n",
    "    \"MovieDetails\", \"tifu\", \"futurology\", \"Music\", \"europe\", \"unpopularopinion\", \n",
    "    \"pakistan\", \"islamabad\", \"lahore\", \"karachi\"\n",
    "]\n",
    "\n",
    "# Read the existing topics CSV\n",
    "topics_df = pd.read_csv(\"D:/FYP/Github/data-tails/Backend/Ontologies/Topics.csv\")\n",
    "\n",
    "# Filter rows where 'subreddit' is in the selected subreddits list\n",
    "filtered_topics_df = topics_df[topics_df['subreddit'].isin(selected_subreddits)]\n",
    "\n",
    "# Save the filtered data to a new CSV file\n",
    "filtered_topics_csv_path = \"D:/FYP/Github/data-tails/Backend/Ontologies/FTopics.csv\"\n",
    "filtered_topics_df.to_csv(filtered_topics_csv_path, index=False)\n",
    "\n",
    "print(f\"Filtered topics saved to {filtered_topics_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating GRAPH STRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "from rdflib import URIRef, Literal\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import json\n",
    "\n",
    "# RDF graph initialization\n",
    "g = rdflib.Graph()\n",
    "SIOC = rdflib.Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = rdflib.Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = rdflib.Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "REDDIT = rdflib.Namespace(\"http://reddit.com/ns#\")  # Custom namespace for Reddit-specific relationships\n",
    "g.bind(\"sioc\", SIOC)\n",
    "g.bind(\"dc\", DCMI)\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"reddit\", REDDIT)\n",
    "\n",
    "\n",
    "# Function to add post data to RDF graph\n",
    "def add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri):\n",
    "    post_uri = URIRef(f\"http://reddit.com/post{index}\")\n",
    "    subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{row['subReddit']}\")\n",
    "    author_uri = URIRef(f\"http://reddit.com/user/{row['authorName']}\")\n",
    "    \n",
    "    # Add post properties and link to topic\n",
    "    g.add((post_uri, rdflib.RDF.type, SIOC.Post))\n",
    "    g.add((post_uri, DCMI.title, Literal(' '.join(row['postTitle']))))\n",
    "    g.add((post_uri, DCMI.description, Literal(' '.join(row['postDesc']))))\n",
    "    g.add((post_uri, DCMI.date, Literal(row['postTime'])))\n",
    "    g.add((post_uri, SIOC.num_replies, Literal(row['noOfUpvotes'])))\n",
    "    g.add((post_uri, SIOC.link, URIRef(row['postUrl'])))\n",
    "    g.add((post_uri, SIOC.NSFW, Literal(row['isNSFW'])))\n",
    "    g.add((post_uri, SIOC.has_type, post_type_uri))\n",
    "    g.add((post_uri, SIOC.topic, topic_uri))\n",
    "\n",
    "    # Create the relationships based on the list of relationship types\n",
    "    # CreatedBy: Indicates that a post is created by an author.\n",
    "    g.add((post_uri, REDDIT.CreatedBy, author_uri))\n",
    "\n",
    "    # BelongsTo: Links a post to a subreddit.\n",
    "    g.add((post_uri, REDDIT.BelongsTo, subreddit_uri))\n",
    "\n",
    "    # PartOfPost: Indicates that a comment is part of a post (the comment belongs to the post).\n",
    "    comment_uri = URIRef(f\"http://reddit.com/comment/{index}\")\n",
    "    g.add((comment_uri, REDDIT.PartOfPost, post_uri))\n",
    "\n",
    "    # HasType: Links an entity (post, comment) to its specific type (text post, link post, etc.).\n",
    "    g.add((post_uri, REDDIT.HasType, post_type_uri))\n",
    "    g.add((comment_uri, REDDIT.HasType, comment_type_uri))\n",
    "\n",
    "    # TaggedIn: Indicates that a post or comment is tagged with a specific topic or keyword.\n",
    "    for tag in row.get('tags', []):  # Assuming tags are available as a list\n",
    "        tag_uri = URIRef(f\"http://reddit.com/tag/{tag}\")\n",
    "        g.add((post_uri, REDDIT.TaggedIn, tag_uri))\n",
    "        g.add((comment_uri, REDDIT.TaggedIn, tag_uri))\n",
    "\n",
    "    # UpvotedBy: Indicates that a user has upvoted a post or comment.\n",
    "    if row.get('upvotedBy'):\n",
    "        for upvoter in row['upvotedBy']:\n",
    "            upvoter_uri = URIRef(f\"http://reddit.com/user/{upvoter}\")\n",
    "            g.add((post_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.UpvotedBy, upvoter_uri))\n",
    "\n",
    "    # DownvotedBy: Indicates that a user has downvoted a post or comment.\n",
    "    if row.get('downvotedBy'):\n",
    "        for downvoter in row['downvotedBy']:\n",
    "            downvoter_uri = URIRef(f\"http://reddit.com/user/{downvoter}\")\n",
    "            g.add((post_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "            g.add((comment_uri, REDDIT.DownvotedBy, downvoter_uri))\n",
    "\n",
    "    # Moderates: Indicates that a user is a moderator of a subreddit.\n",
    "    if row.get('moderators'):\n",
    "        for moderator in row['moderators']:\n",
    "            moderator_uri = URIRef(f\"http://reddit.com/user/{moderator}\")\n",
    "            g.add((subreddit_uri, REDDIT.Moderates, moderator_uri))\n",
    "\n",
    "    # HasFlair: Links a post or comment to a specific flair (e.g., tags like \"Important\", \"Question\", etc.).\n",
    "    if row.get('flair'):\n",
    "        flair_uri = URIRef(f\"http://reddit.com/flair/{row['flair']}\")\n",
    "        g.add((post_uri, REDDIT.HasFlair, flair_uri))\n",
    "        g.add((comment_uri, REDDIT.HasFlair, flair_uri))\n",
    "\n",
    "    # Link post to subreddit and type\n",
    "    g.add((subreddit_uri, rdflib.RDF.type, SIOC.Container))\n",
    "    g.add((subreddit_uri, SIOC.has_post, post_uri))\n",
    "    g.add((subreddit_uri, SIOC.has_type, subreddit_type_uri))\n",
    "    g.add((post_uri, SIOC.Container, subreddit_uri))\n",
    "\n",
    "    # Link to author and author type\n",
    "    g.add((author_uri, rdflib.RDF.type, FOAF.Person))\n",
    "    g.add((author_uri, FOAF.name, Literal(row['authorName'])))\n",
    "    g.add((post_uri, SIOC.has_creator, author_uri))\n",
    "    g.add((author_uri, SIOC.has_type, author_type_uri))\n",
    "\n",
    "    # Add comments and link to comment type\n",
    "    g.add((comment_uri, rdflib.RDF.type, SIOC.Comment))\n",
    "    g.add((comment_uri, DCMI.title, Literal(row['comments'])))\n",
    "    g.add((post_uri, SIOC.has_reply, comment_uri))\n",
    "    g.add((comment_uri, SIOC.has_type, comment_type_uri))\n",
    "\n",
    "# Process each subreddit and create LDA models\n",
    "GroupedData = DF.groupby('subReddit')\n",
    "all_topics = defaultdict(set)\n",
    "\n",
    "for subreddit, group in GroupedData:\n",
    "    group['Combined'] = group['postTitle'] + group['postDesc']\n",
    "    group['Tokens'] = group['Combined'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in custom_stopwords])\n",
    "\n",
    "    # Create dictionary and corpus\n",
    "    dictionary = corpora.Dictionary(group['Tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in group['Tokens']]\n",
    "    LDA = gensim.models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, iterations=100, random_state=42)\n",
    "    \n",
    "    for idx, topic in LDA.print_topics(num_words=5):\n",
    "        topic_uri = URIRef(f\"http://reddit.com/topic/{subreddit}_{idx}\")\n",
    "        topic_words = topic.split(\" + \")\n",
    "        unique_words = {word.split(\"*\")[1].strip('\"') for word in topic_words}\n",
    "        all_topics[subreddit].update(unique_words)\n",
    "\n",
    "        # Define URIs for types\n",
    "        subreddit_type_uri = URIRef(f\"http://reddit.com/type/subreddit/{subreddit}\")\n",
    "        post_type_uri = URIRef(f\"http://reddit.com/type/post/{subreddit}\")\n",
    "        comment_type_uri = URIRef(f\"http://reddit.com/type/comment/{subreddit}\")\n",
    "        author_type_uri = URIRef(f\"http://reddit.com/type/author/{subreddit}\")\n",
    "\n",
    "        # Link subreddit to topic\n",
    "        subreddit_uri = URIRef(f\"http://reddit.com/subreddit/{subreddit}\")\n",
    "        g.add((subreddit_uri, SIOC.has_topic, topic_uri))\n",
    "        g.add((topic_uri, rdflib.RDF.type, SIOC.Topic))\n",
    "\n",
    "        # Iterate through each row in the group\n",
    "        for index, row in group.iterrows():\n",
    "            add_post_to_graph(row, index, topic_uri, subreddit_type_uri, author_type_uri, post_type_uri, comment_type_uri)\n",
    "\n",
    "# Save graph\n",
    "g.serialize('D:/FYP/Github/data-tails/Backend/Ontologies/KG.ttl', format='turtle')\n",
    "json_ld = g.serialize(format='json-ld', indent=4)\n",
    "with open(\"D:/FYP/Github/data-tails/Backend/Ontologies/KG.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Loading Knowledge Graphs...\n",
      "‚úÖ Loaded KG.json with 63763 entities.\n",
      "‚ùå Error loading KG.json: 'list' object has no attribute 'items'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to convert Literal lexical form to value. Datatype=http://www.w3.org/2001/XMLSchema#dateTime, Converter=<function parse_datetime at 0x000001FD956EAE60>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\rdflib\\term.py\", line 2119, in _castLexicalToPython\n",
      "    return conv_func(lexical)  # type: ignore[arg-type]\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\isodate\\isodatetime.py\", line 55, in parse_datetime\n",
      "    tmpdate = parse_date(datestring)\n",
      "  File \"c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\isodate\\isodates.py\", line 203, in parse_date\n",
      "    raise ISO8601Error('Unrecognised ISO 8601 date format: %r' % datestring)\n",
      "isodate.isoerror.ISO8601Error: Unrecognised ISO 8601 date format: 'Na'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded KG.ttl with 648909 triples.\n",
      "\n",
      "üîç Retrieving Relevant Comments...\n",
      "‚úÖ Retrieval Time: 0.0 sec\n",
      "\n",
      "ü§ñ Querying LLM...\n",
      "\n",
      "üí° LLM Response: The Middle District of Louisiana is one of the four federal judicial districts in the state of Louisiana. The court has jurisdiction over the following parishes:\n",
      "\n",
      "* Avoyelles\n",
      "* Bossier\n",
      "* Catahoula\n",
      "* Concordia\n",
      "* East Carroll\n",
      "* Franklin\n",
      "* Grant\n",
      "* La Salle\n",
      "* Madison\n",
      "* Natchitoches\n",
      "* Ouachita\n",
      "* Richland\n",
      "* Ruston\n",
      "* Tensas\n",
      "* Union\n",
      "* Vermilion\n",
      "* Webster\n",
      "* West Carroll\n",
      "\n",
      "Since you're asking about the results of incidents in the Middle District of Louisiana, please specify the type of incident you're referring to (e.g. criminal, civil, defamation, etc.) and the approximate date of occurrence. I'll do my best to provide you with the relevant information.\n",
      "\n",
      "Note: As a general rule, court records are public information, but some documents may be sealed or subject to redactions. I'll do my best to provide you with accurate and available information.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import rdflib\n",
    "import subprocess\n",
    "import re\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from rdflib import Graph, Namespace, Literal, URIRef\n",
    "\n",
    "# ‚úÖ Define RDF Namespaces\n",
    "SIOC = Namespace(\"http://rdfs.org/sioc/ns#\")\n",
    "DCMI = Namespace(\"http://purl.org/dc/elements/1.1/\")\n",
    "FOAF = Namespace(\"http://xmlns.com/foaf/0.1/\")\n",
    "REDDIT = Namespace(\"http://reddit.com/ns#\")\n",
    "\n",
    "# ‚úÖ Load KG.json and Index Data for Fast Lookups\n",
    "def load_kg_json(file_path):\n",
    "    \"\"\"Loads KG.json and creates a fast index for lookup by subreddit & topic.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        print(f\"‚úÖ Loaded KG.json with {len(data)} entities.\")\n",
    "\n",
    "        # **Pre-index posts & comments for O(1) lookup**\n",
    "        index = defaultdict(lambda: defaultdict(list))\n",
    "        for entity_id, entity in data.items():\n",
    "            subreddit = entity.get(\"sioc:Container\", \"\").replace(\"http://reddit.com/subreddit/\", \"\")\n",
    "            topics = entity.get(\"sioc:topic\", [])\n",
    "            if isinstance(topics, str):\n",
    "                topics = [topics]\n",
    "\n",
    "            for topic in topics:\n",
    "                topic = topic.replace(\"http://reddit.com/topic/\", \"\")\n",
    "                index[subreddit][topic].append(entity_id)\n",
    "\n",
    "        return index, data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading KG.json: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ‚úÖ Load KG.ttl & Build Fast Lookup Table\n",
    "def load_kg_ttl(file_path):\n",
    "    \"\"\"Loads KG.ttl and builds an adjacency list for fast lookups.\"\"\"\n",
    "    try:\n",
    "        g = Graph()\n",
    "        g.parse(file_path, format=\"turtle\")\n",
    "        adjacency_list = defaultdict(list)\n",
    "\n",
    "        for s, p, o in g:\n",
    "            adjacency_list[str(s)].append((s, p, o))\n",
    "\n",
    "        print(f\"‚úÖ Loaded KG.ttl with {len(g)} triples.\")\n",
    "        return g, adjacency_list\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading KG.ttl: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# ‚úÖ **Optimized Comment Retrieval**\n",
    "def retrieve_relevant_comments(index, kg_json, subreddit, topic):\n",
    "    \"\"\"Retrieves only relevant comments based on subreddit & topic index.\"\"\"\n",
    "    if not index:\n",
    "        return \"‚ùå KG.json not loaded.\"\n",
    "\n",
    "    # **Step 1: Get relevant posts from pre-indexed data**\n",
    "    relevant_posts = index.get(subreddit, {}).get(topic, [])\n",
    "    if not relevant_posts:\n",
    "        return \"‚ùå No relevant posts found for the subreddit & topic.\"\n",
    "\n",
    "    # **Step 2: Fetch only comments from those posts**\n",
    "    matched_comments = []\n",
    "    for post in relevant_posts:\n",
    "        post_data = kg_json.get(post, {})\n",
    "        if \"sioc:has_reply\" in post_data:\n",
    "            replies = post_data[\"sioc:has_reply\"]\n",
    "            if isinstance(replies, str):\n",
    "                replies = [replies]\n",
    "            for comment in replies:\n",
    "                comment_text = kg_json.get(comment, {}).get(\"dc:title\", \"\")\n",
    "                if comment_text:\n",
    "                    matched_comments.append(comment_text)\n",
    "\n",
    "    return \"\\n\".join(matched_comments[:10]) if matched_comments else \"‚ùå No relevant comments found.\"\n",
    "\n",
    "# ‚úÖ Query WizardLM2 (Fast)\n",
    "def query_wizardlm2(context, user_query):\n",
    "    \"\"\"Queries WizardLM2 using Ollama API.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {user_query}\n",
    "\n",
    "    Provide a detailed answer based on the context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", \"wizardlm2\"],\n",
    "            input=prompt.encode(\"utf-8\"),  \n",
    "            capture_output=True\n",
    "        )\n",
    "        if result.returncode != 0:\n",
    "            raise Exception(result.stderr.decode(\"utf-8\"))\n",
    "        return result.stdout.decode(\"utf-8\").strip()\n",
    "    except Exception as e:\n",
    "        return f\"Error querying WizardLM2: {str(e)}\"\n",
    "\n",
    "# ‚úÖ **Run Main Program**\n",
    "if __name__ == \"__main__\":\n",
    "    kg_json_path = \"./KG.json\"\n",
    "    kg_ttl_path = \"./KG.ttl\"\n",
    "\n",
    "    print(\"\\nüîç Loading Knowledge Graphs...\")\n",
    "    index, kg_json = load_kg_json(kg_json_path)\n",
    "    kg_ttl, adjacency_list = load_kg_ttl(kg_ttl_path)\n",
    "\n",
    "    subreddit = \"News\"\n",
    "    topic = \"Democracy\"\n",
    "    user_query = \"Middle District Court of Louisiana incident results?\"\n",
    "\n",
    "    print(\"\\nüîç Retrieving Relevant Comments...\")\n",
    "    start_time = time.time()\n",
    "    context = retrieve_relevant_comments(index, kg_json, subreddit, topic)\n",
    "    retrieval_time = round(time.time() - start_time, 2)\n",
    "    print(f\"‚úÖ Retrieval Time: {retrieval_time} sec\")\n",
    "    print(f\"Retrieved Context:\\n{context}\")\n",
    "\n",
    "    print(\"\\nü§ñ Querying LLM...\")\n",
    "    response = query_wizardlm2(context, user_query)\n",
    "\n",
    "    print(\"\\nüí° LLM Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Recommedation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.8.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Top 3 Recommended Visualizations: [('bar_chart', 0.68), ('line_chart', 0.63), ('heatmap_chart', 0.46)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\97156\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load NLP models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Define visualization categories\n",
    "chart_types = {\n",
    "    \"area_chart\": \"Shows cumulative data trends with a filled area.\",\n",
    "    \"bar_chart\": \"Used for comparing categories or ranking values.\",\n",
    "    \"chord_diagram\": \"Best for visualizing relationships and interactions.\",\n",
    "    \"circle_packing\": \"Represents hierarchical relationships in a compact form.\",\n",
    "    \"connection_map\": \"Visualizes spatial relationships and geographic data.\",\n",
    "    \"DAG\": \"Shows directed relationships, commonly used for processes or networks.\",\n",
    "    \"donut_chart\": \"A variation of the pie chart, highlighting proportions.\",\n",
    "    \"heatmap_chart\": \"Displays intensity values in a matrix format.\",\n",
    "    \"line_chart\": \"Best for showing trends over time or sequential data.\",\n",
    "    \"mosaic_plot\": \"Used to show the relationship between categorical variables.\",\n",
    "    \"network_graph\": \"Illustrates complex relationships in networks.\",\n",
    "    \"polar_area\": \"Represents cyclic data with proportionally scaled segments.\",\n",
    "    \"small_multiples\": \"Facilitates comparisons across multiple categories.\",\n",
    "    \"stacked_area_chart\": \"Shows part-to-whole relationships over time.\",\n",
    "    \"sunburst_chart\": \"Depicts hierarchical data as concentric layers.\",\n",
    "    \"tree_diagram\": \"Illustrates hierarchical relationships in tree structure.\",\n",
    "    \"treemap_chart\": \"Depicts hierarchical structures using nested rectangles.\",\n",
    "    \"voronoi_map\": \"Divides spatial regions based on distance.\",\n",
    "    \"word_cloud\": \"Visualizes common words and keyword frequency in text-heavy data.\"\n",
    "}\n",
    "\n",
    "# Precompute chart category embeddings\n",
    "category_embeddings = {\n",
    "    chart: model.encode(description, normalize_embeddings=True)\n",
    "    for chart, description in chart_types.items()\n",
    "}\n",
    "\n",
    "# Extract features from query & response\n",
    "def extract_features(query, response):\n",
    "    \"\"\"Extracts key elements from the query-response pair.\"\"\"\n",
    "    combined_text = query + \" \" + response\n",
    "    doc = nlp(combined_text)\n",
    "\n",
    "    numbers = [token.text for token in doc if token.like_num]\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ in {\"GPE\", \"LOC\"}]\n",
    "    \n",
    "    trend_keywords = {\"increase\", \"decline\", \"growth\", \"rise\", \"drop\", \"trend\", \"fall\", \"reduce\", \"expansion\"}\n",
    "    trends = [token.lemma_ for token in doc if token.lemma_ in trend_keywords]\n",
    "    \n",
    "    relationship_keywords = {\"prefer\", \"dominate\", \"compared\", \"versus\", \"majority\", \"minority\"}\n",
    "    relationships = [token.lemma_ for token in doc if token.lemma_ in relationship_keywords]\n",
    "    \n",
    "    ranking_keywords = {\"ranking\", \"top\", \"best\", \"percent\", \"favorite\", \"most popular\"}\n",
    "    rankings = [token.lemma_ for token in doc if token.lemma_ in ranking_keywords]\n",
    "    \n",
    "    geo_terms = {\"earthquake\", \"magnitude\", \"epicenter\", \"seismic\", \"hurricane\", \"flood\", \"storm\"}\n",
    "    geo_features = [token.lemma_ for token in doc if token.lemma_ in geo_terms]\n",
    "    \n",
    "    keywords = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "\n",
    "    # Identify text-heavy responses\n",
    "    is_text_heavy = len(keywords) > 30 \n",
    "\n",
    "    return {\n",
    "        \"numbers\": numbers,\n",
    "        \"locations\": locations,\n",
    "        \"trends\": trends,\n",
    "        \"relationships\": relationships,\n",
    "        \"geo_features\": geo_features,\n",
    "        \"rankings\": rankings,\n",
    "        \"keywords\": keywords,\n",
    "        \"is_text_heavy\": is_text_heavy  # New feature\n",
    "    }\n",
    "\n",
    "# Adjust scores based on extracted features\n",
    "def boost_scores(scores, response_type, features):\n",
    "    \"\"\"Boosts scores dynamically based on detected features.\"\"\"\n",
    "    boosts = {\n",
    "        \"numeric\": {\"bar_chart\": 0.6, \"line_chart\": 0.5, \"heatmap_chart\": 0.4},\n",
    "        \"geographic\": {\"connection_map\": 0.7, \"voronoi_map\": 0.6, \"network_graph\": 0.5},\n",
    "        \"textual\": {\"word_cloud\": 0.7, \"chord_diagram\": 0.5}\n",
    "    }\n",
    "    \n",
    "    if response_type in boosts:\n",
    "        for chart, boost in boosts[response_type].items():\n",
    "            scores[chart] += boost\n",
    "    \n",
    "    if features[\"trends\"]:\n",
    "        scores[\"line_chart\"] += 0.6\n",
    "        scores[\"area_chart\"] += 0.5\n",
    "    \n",
    "    if features[\"relationships\"]:\n",
    "        scores[\"bar_chart\"] += 0.6\n",
    "        scores[\"chord_diagram\"] += 0.5\n",
    "    \n",
    "    if features[\"geo_features\"]:\n",
    "        scores[\"connection_map\"] += 0.7\n",
    "        scores[\"network_graph\"] += 0.6\n",
    "    \n",
    "    if features[\"rankings\"]:\n",
    "        scores[\"bar_chart\"] += 0.7\n",
    "        scores[\"mosaic_plot\"] += 0.5\n",
    "\n",
    "    # Ensure Word Cloud is boosted for text-heavy responses\n",
    "    if features[\"is_text_heavy\"]:\n",
    "        scores[\"word_cloud\"] += 1.0\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Function to recommend all visualizations\n",
    "def recommend_visualizations(query, response):\n",
    "    \"\"\"Recommends a ranked list of all visualizations.\"\"\"\n",
    "    features = extract_features(query, response)\n",
    "    response_embedding = model.encode(query + \" \" + response, normalize_embeddings=True)\n",
    "    \n",
    "    similarity_scores = {\n",
    "        chart: cosine_similarity([response_embedding], [embedding]).flatten()[0]\n",
    "        for chart, embedding in category_embeddings.items()\n",
    "    }\n",
    "    \n",
    "    # Apply feature-based score boosts\n",
    "    similarity_scores = boost_scores(similarity_scores, \"numeric\", features)\n",
    "\n",
    "    # Normalize scores for better ranking\n",
    "    min_score = min(similarity_scores.values())\n",
    "    max_score = max(similarity_scores.values())\n",
    "    if max_score - min_score > 0:  # Avoid division by zero\n",
    "        for chart in similarity_scores:\n",
    "            similarity_scores[chart] = (similarity_scores[chart] - min_score) / (max_score - min_score)\n",
    "\n",
    "    # Sort and return all charts\n",
    "    ranked_charts = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)[:4]\n",
    "    \n",
    "    return [(chart, round(score, 2)) for chart, score in ranked_charts]\n",
    "\n",
    "def getViz(user_query, response):\n",
    "    recommended_charts = recommend_visualizations(user_query, response)\n",
    "    print(\"\\n‚úÖ Ranked Recommended Visualizations:\")\n",
    "    for chart, score in recommended_charts:\n",
    "        print(f\"{chart}: {score}\")\n",
    "\n",
    "getViz(user_query, response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
